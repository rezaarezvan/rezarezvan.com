<!DOCTYPE html><html class="bg-background text-foreground" lang="en"> <head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"><meta name="generator" content="Astro v5.14.1"><meta name="robots" content="index, follow"><meta name="HandheldFriendly" content="True"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="format-detection" content="telephone=no,date=no,address=no,email=no,url=no"><meta name="theme-color" content="#121212" media="(prefers-color-scheme: dark)"><meta name="theme-color" content="#121212" media="(prefers-color-scheme: light)"><link rel="sitemap" href="/sitemap-index.xml"><link rel="manifest" href="/site.webmanifest"><link rel="alternate" type="application/rss+xml" title="rezarezvan.com" href="https://rezarezvan.com/rss.xml"><!-- PageFind --><link href="/pagefind/pagefind-ui.css" rel="stylesheet"><script src="/pagefind/pagefind-ui.js"></script><!-- KaTeX support --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><!-- inline KaTeX --><script>
    function renderKaTeX() {
      if (typeof renderMathInElement !== 'undefined') {
        renderMathInElement(document.body, {
          delimiters: [
            { left: '$$', right: '$$', display: true },
            { left: '$', right: '$', display: false },
          ],
        })
      }
    }

    document.addEventListener('DOMContentLoaded', renderKaTeX)
    document.addEventListener('astro:after-swap', renderKaTeX)
  </script><link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><link rel="shortcut icon" href="/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><meta name="apple-mobile-web-app-title" content="rezvan-blog"><link rel="manifest" href="/site.webmanifest"><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><script type="module" src="/_astro/ClientRouter.astro_astro_type_script_index_0_lang.B3vRBseb.js"></script><script>
    function init() {
      setGiscusTheme()
    }

    const setGiscusTheme = () => {
      const giscus = document.querySelector('.giscus-frame')

      const isDark = document.documentElement.classList.contains('dark')

      if (giscus) {
        const url = new URL(giscus.src)
        url.searchParams.set('theme', isDark ? 'dark' : 'light')
        giscus.src = url.toString()
      }
    }

    document.addEventListener('DOMContentLoaded', () => init())
    document.addEventListener('astro:after-swap', () => init())
  </script><title>Rezvan Explains: Classical Machine Learning | rezarezvan.com</title><meta name="title" content="Rezvan Explains: Classical Machine Learning | rezarezvan.com"><meta name="description" content="Personal website and course notes repository"><link rel="canonical" href="https://rezarezvan.com"><meta property="og:title" content="Rezvan Explains: Classical Machine Learning"><meta property="og:description" content="Personal website and course notes repository"><meta property="og:image" content="https://rezarezvan.com/_astro/re_header.B_XaRkMt.jpg"><meta property="og:image:alt" content="Rezvan Explains: Classical Machine Learning"><meta property="og:type" content="website"><meta property="og:locale" content="en"><meta property="og:site_name" content="rezarezvan.com"><meta property="og:url" content="https://rezarezvan.com/blog/rezvan_explains/"><meta name="twitter:title" content="Rezvan Explains: Classical Machine Learning"><meta name="twitter:description" content="Personal website and course notes repository"><meta property="twitter:image" content="https://rezarezvan.com/_astro/re_header.B_XaRkMt.jpg"><meta name="twitter:image:alt" content="Rezvan Explains: Classical Machine Learning"><meta name="twitter:card" content="summary_large_image"><link rel="stylesheet" href="/_astro/_slug_.CJYmjoM9.css"></head><body> <div class="flex h-fit min-h-screen flex-col gap-y-6 font-sans"> <div class="bg-background/50 sticky top-0 z-50 divide-y backdrop-blur-sm xl:divide-none"> <header data-astro-transition-persist="astro-l7r54iwe-1"> <div class="mx-auto flex max-w-3xl items-center justify-between gap-4 px-4 py-3"> <a href="/" target="_self" class="transition-colors duration-300 ease-in-out flex shrink-0 items-center justify-center gap-3">  <span class="hidden h-full text-lg font-medium min-[300px]:block">rezarezvan.com</span>  </a> <div class="flex items-center sm:gap-4"> <nav class="hidden items-center gap-4 text-sm sm:flex sm:gap-6"> <a href="/blog" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> blog<span>/</span>  </a><a href="/notes" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> notes<span>/</span>  </a><a href="/dump" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> dump<span>/</span>  </a><a href="/research" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> research<span>/</span>  </a> </nav> <button id="magnifying-glass" aria-label="Search" class="flex items-center px-2 text-sm transition-colors duration-300 ease-in-out hover:rounded hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg height="16" stroke-linejoin="round" viewBox="0 0 16 16" width="16" style="color: currentcolor;"> <path fill-rule="evenodd" clip-rule="evenodd" d="M3.5 7C3.5 5.067 5.067 3.5 7 3.5C8.933 3.5 10.5 5.067 10.5 7C10.5 7.88461 10.1718 8.69256 9.63058 9.30876L9.30876 9.63058C8.69256 10.1718 7.88461 10.5 7 10.5C5.067 10.5 3.5 8.933 3.5 7ZM9.96544 11.0261C9.13578 11.6382 8.11014 12 7 12C4.23858 12 2 9.76142 2 7C2 4.23858 4.23858 2 7 2C9.76142 2 12 4.23858 12 7C12 8.11014 11.6382 9.13578 11.0261 9.96544L14.0303 12.9697L14.5607 13.5L13.5 14.5607L12.9697 14.0303L9.96544 11.0261Z" fill="currentColor"></path> </svg>
&nbsp;Search
</button> <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).load=e;window.dispatchEvent(new Event("astro:load"));})();</script><script>(()=>{var A=Object.defineProperty;var g=(i,o,a)=>o in i?A(i,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):i[o]=a;var d=(i,o,a)=>g(i,typeof o!="symbol"?o+"":o,a);{let i={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t),11:t=>1/0*t},o=t=>{let[l,e]=t;return l in i?i[l](e):void 0},a=t=>t.map(o),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([l,e])=>[l,o(e)]));class y extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var b;if(!this.hydrator||!this.isConnected)return;let e=(b=this.parentElement)==null?void 0:b.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let c=this.querySelectorAll("astro-slot"),n={},h=this.querySelectorAll("template[data-astro-template]");for(let r of h){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(let r of c){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("name")||"default"]=r.innerHTML)}let p;try{p=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(r){let s=this.getAttribute("component-url")||"<unknown>",v=this.getAttribute("component-export");throw v&&(s+=` (export ${v})`),console.error(`[hydrate] Error parsing props for component ${s}`,this.getAttribute("props"),r),r}let u;await this.hydrator(this)(this.Component,p,n,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),c.disconnect(),this.childrenConnectedCallback()},c=new MutationObserver(()=>{var n;((n=this.lastChild)==null?void 0:n.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});c.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),c=this.getAttribute("client");if(Astro[c]===void 0){window.addEventListener(`astro:${c}`,()=>this.start(),{once:!0});return}try{await Astro[c](async()=>{let n=this.getAttribute("renderer-url"),[h,{default:p}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),u=this.getAttribute("component-export")||"default";if(!u.includes("."))this.Component=h[u];else{this.Component=h;for(let f of u.split("."))this.Component=this.Component[f]}return this.hydrator=p,this.hydrate},e,this)}catch(n){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,n)}}attributeChangedCallback(){this.hydrate()}}d(y,"observedAttributes",["props"]),customElements.get("astro-island")||customElements.define("astro-island",y)}})();</script><astro-island uid="Z2bDsyt" prefix="r14" component-url="/_astro/mobile-menu.doY6Gi9x.js" component-export="default" renderer-url="/_astro/client.CVI9NSBG.js" props="{&quot;data-astro-transition-persist&quot;:[0,&quot;astro-iq5tym4z-2&quot;]}" ssr client="load" opts="{&quot;name&quot;:&quot;MobileMenu&quot;,&quot;value&quot;:true}" data-astro-transition-persist="astro-iq5tym4z-2" await-children><button data-slot="dropdown-menu-trigger" class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 size-9 md:hidden" title="Menu" type="button" id="radix-:r14R0:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu h-5 w-5"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg><span class="sr-only">Toggle menu</span></button><!--astro:end--></astro-island> <button data-slot="button" class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 size-9" id="theme-toggle" title="Toggle theme"> <svg width="1em" height="1em" class="size-4 scale-100 rotate-0 transition-all dark:scale-0 dark:-rotate-90" data-icon="lucide:sun">   <symbol id="ai:lucide:sun" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><circle cx="12" cy="12" r="4"/><path d="M12 2v2m0 16v2M4.93 4.93l1.41 1.41m11.32 11.32l1.41 1.41M2 12h2m16 0h2M6.34 17.66l-1.41 1.41M19.07 4.93l-1.41 1.41"/></g></symbol><use href="#ai:lucide:sun"></use>  </svg> <svg width="1em" height="1em" class="absolute size-4 scale-0 rotate-90 transition-all dark:scale-100 dark:rotate-0" data-icon="lucide:moon">   <symbol id="ai:lucide:moon" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.985 12.486a9 9 0 1 1-9.473-9.472c.405-.022.617.46.402.803a6 6 0 0 0 8.268 8.268c.344-.215.825-.004.803.401"/></symbol><use href="#ai:lucide:moon"></use>  </svg> <span class="sr-only">Toggle theme</span> </button> <script data-astro-rerun>
  const theme = (() => {
    const localStorageTheme = localStorage?.getItem('theme') ?? ''
    if (['dark', 'light'].includes(localStorageTheme)) {
      return localStorageTheme
    }
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
      return 'dark'
    }
    return 'light'
  })()

  document.documentElement.setAttribute('data-theme', theme)
  document.documentElement.classList.add(
    theme === 'dark' ? 'scheme-dark' : 'scheme-light',
  )
  window.localStorage.setItem('theme', theme)
</script> <script type="module">function a(){const e=document.documentElement,n=e.getAttribute("data-theme")==="dark"?"light":"dark";e.classList.add("[&_*]:transition-none"),e.setAttribute("data-theme",n),e.classList.remove("scheme-dark","scheme-light"),e.classList.add(n==="dark"?"scheme-dark":"scheme-light"),window.getComputedStyle(e).getPropertyValue("opacity"),requestAnimationFrame(()=>{e.classList.remove("[&_*]:transition-none")}),localStorage.setItem("theme",n)}function s(){const e=document.getElementById("theme-toggle");e&&e.addEventListener("click",a)}s();document.addEventListener("astro:after-swap",()=>{const e=localStorage.getItem("theme")||"light",t=document.documentElement;t.classList.add("[&_*]:transition-none"),window.getComputedStyle(t).getPropertyValue("opacity"),t.setAttribute("data-theme",e),t.classList.remove("scheme-dark","scheme-light"),t.classList.add(e==="dark"?"scheme-dark":"scheme-light"),requestAnimationFrame(()=>{t.classList.remove("[&_*]:transition-none")}),s()});</script> </div> </div> </header> <div id="mobile-toc-container" class="w-full xl:hidden"><details class="group"><summary class="flex w-full cursor-pointer items-center justify-between"><div class="mx-auto flex w-full max-w-3xl items-center px-4 py-3"><div class="relative mr-2 size-4"><svg class="h-4 w-4" viewBox="0 0 24 24"><circle class="text-primary/20" cx="12" cy="12" r="10" fill="none" stroke="currentColor" stroke-width="2"></circle><circle id="mobile-toc-progress-circle" class="text-primary" cx="12" cy="12" r="10" fill="none" stroke="currentColor" stroke-width="2" stroke-dasharray="62.83" stroke-dashoffset="62.83" transform="rotate(-90 12 12)"></circle></svg></div><span id="mobile-toc-current-section" class="text-muted-foreground flex-grow truncate text-sm">
Overview
</span><span class="text-muted-foreground ml-2"><svg width="1em" height="1em" class="h-4 w-4 transition-transform duration-200 group-open:rotate-180" data-icon="lucide:chevron-down">   <symbol id="ai:lucide:chevron-down" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m6 9l6 6l6-6"/></symbol><use href="#ai:lucide:chevron-down"></use>  </svg></span></div></summary><astro-island uid="Z2kiO11" prefix="r17" component-url="/_astro/scroll-area.DpuOTeCe.js" component-export="ScrollArea" renderer-url="/_astro/client.CVI9NSBG.js" props="{&quot;className&quot;:[0,&quot;mx-auto max-w-3xl&quot;],&quot;data-toc-header-scroll&quot;:[0,true]}" ssr client="load" opts="{&quot;name&quot;:&quot;ScrollArea&quot;,&quot;value&quot;:true}" await-children><div dir="ltr" data-slot="scroll-area" class="relative mx-auto max-w-3xl" data-toc-header-scroll="true" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px"><style>[data-radix-scroll-area-viewport]{scrollbar-width:none;-ms-overflow-style:none;-webkit-overflow-scrolling:touch;}[data-radix-scroll-area-viewport]::-webkit-scrollbar{display:none}</style><div data-radix-scroll-area-viewport="" data-slot="scroll-area-viewport" class="ring-ring/10 dark:ring-ring/20 dark:outline-ring/40 outline-ring/50 size-full rounded-[inherit] transition-[color,box-shadow] focus-visible:ring-4 focus-visible:outline-1" style="overflow-x:hidden;overflow-y:hidden"><div style="min-width:100%;display:table"><astro-slot><div class="max-h-[30vh]"><ul class="flex list-none flex-col gap-y-2 px-4 pb-4" id="mobile-table-of-contents"><li class="px-4 text-sm text-foreground/60"><a href="#preqrequisites-and-notation" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="preqrequisites-and-notation">Preqrequisites and Notation</a></li><li class="px-4 text-sm text-foreground/60"><a href="#what-why-and-how" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="what-why-and-how">What, Why, and How?</a></li><li class="px-4 text-sm ml-4 text-foreground/60"><a href="#statistics-vs-machine-learning" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="statistics-vs-machine-learning">Statistics VS. Machine Learning</a></li><li class="px-4 text-sm ml-4 text-foreground/60"><a href="#humble-beginnings" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="humble-beginnings">Humble Beginnings</a></li><li class="px-4 text-sm text-foreground/60"><a href="#different-types-of-learning" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="different-types-of-learning">Different Types of Learning</a></li><li class="px-4 text-sm ml-4 text-foreground/60"><a href="#supervised-learning" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="supervised-learning">Supervised Learning</a></li><li class="px-4 text-sm ml-8 text-foreground/60"><a href="#the-classification-task" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="the-classification-task">The Classification Task</a></li><li class="px-4 text-sm ml-8 text-foreground/60"><a href="#the-classification-learning-problem" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="the-classification-learning-problem">The Classification Learning Problem</a></li><li class="px-4 text-sm ml-12 text-foreground/60"><a href="#metrics" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="metrics">Metrics</a></li><li class="px-4 text-sm ml-8 text-foreground/60"><a href="#different-approaches" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="different-approaches">Different Approaches</a></li><li class="px-4 text-sm ml-12 text-foreground/60"><a href="#generative-models" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="generative-models">Generative Models</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#bayes-optimal-classifier" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="bayes-optimal-classifier">Bayes Optimal Classifier</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#maximum-likelihood-estimate" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="maximum-likelihood-estimate">Maximum Likelihood Estimate</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#bayes-optimal-classifier-summary" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="bayes-optimal-classifier-summary">Bayes Optimal Classifier Summary</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#naive-bayes-classifier" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="naive-bayes-classifier">Naive Bayes Classifier</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#linear-discriminant-analysis-lda" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)</a></li><li class="px-4 text-sm ml-12 text-foreground/60"><a href="#summary-generative-models" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="summary-generative-models">Summary Generative Models</a></li><li class="px-4 text-sm ml-12 text-foreground/60"><a href="#discriminative-models" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="discriminative-models">Discriminative Models</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#logistic-regression" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="logistic-regression">Logistic Regression</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#support-vector-machines-svms" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="support-vector-machines-svms">Support Vector Machines (SVMs)</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#maximum-margin-principle" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="maximum-margin-principle">Maximum Margin Principle</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#computing-the-margin" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="computing-the-margin">Computing the Margin</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#why-is-maximize-the-margin-a-good-idea" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="why-is-maximize-the-margin-a-good-idea">Why is Maximize the Margin a Good Idea?</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#soft-margin" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="soft-margin">Soft-Margin</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#convex-optimization" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="convex-optimization">(Convex) Optimization</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#standard-form-of-convex-optimization" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="standard-form-of-convex-optimization">Standard Form of Convex Optimization</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#duality-and-kkt-conditions" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="duality-and-kkt-conditions">Duality and KKT Conditions</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#svms-continued" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="svms-continued">SVMs Continued</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#deriving-the-dual-problem" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="deriving-the-dual-problem">Deriving the Dual Problem</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#gradient-descent-and-variants" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="gradient-descent-and-variants">Gradient Descent and variants</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#k-nearest-neighbors" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="k-nearest-neighbors">$k$-Nearest Neighbors</a></li><li class="px-4 text-sm ml-8 text-foreground/60"><a href="#the-regression-task" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="the-regression-task">The Regression Task</a></li><li class="px-4 text-sm ml-8 text-foreground/60"><a href="#the-regression-learning-problem" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="the-regression-learning-problem">The Regression Learning Problem</a></li><li class="px-4 text-sm ml-12 text-foreground/60"><a href="#linear-regression" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="linear-regression">Linear Regression</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#ordinary-least-squares-ols" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="ordinary-least-squares-ols">Ordinary Least Squares (OLS)</a></li><li class="px-4 text-sm ml-16 text-foreground/60"><a href="#general-ols-derivation" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="general-ols-derivation">General OLS Derivation</a></li><li class="px-4 text-sm ml-4 text-foreground/60"><a href="#unsupervised-learning" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="unsupervised-learning">Unsupervised Learning</a></li><li class="px-4 text-sm text-foreground/60"><a href="#footnote-label" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="footnote-label">Footnotes</a></li></ul></div></astro-slot></div></div></div><!--astro:end--></astro-island></details></div><script type="module" src="/_astro/TOCHeader.astro_astro_type_script_index_0_lang.CKMLAwWj.js"></script>   </div> <main class="grow"> <div class="mx-auto flex grow flex-col gap-y-6 px-4">   <section class="grid grid-cols-[minmax(0px,1fr)_min(calc(var(--breakpoint-md)-2rem),100%)_minmax(0px,1fr)] gap-y-6"> <div class="col-start-2"> <nav aria-label="breadcrumb" data-slot="breadcrumb"> <ol data-slot="breadcrumb-list" class="text-muted-foreground flex flex-wrap items-center gap-1.5 text-sm break-words sm:gap-2.5"> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"> <a data-slot="breadcrumb-link" class="hover:text-foreground transition-colors" href="/"> <svg width="1em" height="1em" class="size-4 shrink-0" data-icon="lucide:home">   <symbol id="ai:lucide:home" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 21v-8a1 1 0 0 0-1-1h-4a1 1 0 0 0-1 1v8"/><path d="M3 10a2 2 0 0 1 .709-1.528l7-6a2 2 0 0 1 2.582 0l7 6A2 2 0 0 1 21 10v9a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"/></g></symbol><use href="#ai:lucide:home"></use>  </svg> </a> </li>  <li data-slot="breadcrumb-separator" role="presentation" aria-hidden="true" class="[&amp;&gt;svg]:size-3.5"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></li> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"><a data-slot="breadcrumb-link" class="hover:text-foreground transition-colors" href="/blog"> <span class="flex items-center gap-x-2"> <svg width="1em" height="1em" class="size-4" data-icon="lucide:library-big">   <symbol id="ai:lucide:library-big" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><rect width="8" height="18" x="3" y="3" rx="1"/><path d="M7 3v18m13.4-2.1c.2.5-.1 1.1-.6 1.3l-1.9.7c-.5.2-1.1-.1-1.3-.6L11.1 5.1c-.2-.5.1-1.1.6-1.3l1.9-.7c.5-.2 1.1.1 1.3.6Z"/></g></symbol><use href="#ai:lucide:library-big"></use>  </svg> Blog </span> </a></li>  <li data-slot="breadcrumb-separator" role="presentation" aria-hidden="true" class="[&amp;&gt;svg]:size-3.5"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></li> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"><span data-slot="breadcrumb-page" role="link" aria-disabled="true" aria-current="page" class="text-foreground font-normal"> <span class="flex items-center gap-x-2"> <svg width="1em" height="1em" class="size-4 shrink-0" data-icon="lucide:book-open-text">   <symbol id="ai:lucide:book-open-text" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 7v14m4-9h2m-2-4h2M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4a4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3a3 3 0 0 0-3-3zm3-6h2M6 8h2"/></symbol><use href="#ai:lucide:book-open-text"></use>  </svg> <span>Rezvan Explains: Classical Machine Learning</span> </span> </span></li> </ol> </nav> </div> <section class="relative col-start-2"> <img src="/_astro/re_header.B_XaRkMt_Z2vdXw6.webp" alt="Rezvan Explains: Classical Machine Learning" loading="lazy" aria-hidden="true" decoding="async" fetchpriority="auto" width="1200" height="630" class="absolute inset-0 -z-10 h-full w-full object-cover opacity-50 blur-sm filter"> <div class="relative z-10 flex flex-col gap-y-6 text-center"> <h1 class="mb-2 scroll-mt-31 py-2 text-3xl leading-tight font-medium text-pretty" id="post-title"> Rezvan Explains: Classical Machine Learning </h1> <div class="mb-4 flex flex-wrap items-center justify-center gap-2 text-sm"> <div class="flex items-center gap-2"> <span>Date: January 20, 2025</span>  <div data-orientation="vertical" role="none" data-slot="separator-root" class="bg-border red shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px h-4!"></div> <span>Last modified: May 27, 2025</span>  <div data-orientation="vertical" role="none" data-slot="separator-root" class="bg-border red shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px h-4!"></div> <span>50 min read</span> </div> </div> </div> </section> <nav class="col-start-2 grid grid-cols-1 gap-4 sm:grid-cols-2"> <a href="/blog/uncertainty#post-title" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-lg group flex items-center justify-start size-full" aria-disabled="false">  <svg width="1em" height="1em" class="mr-2 size-4 transition-transform group-hover:-translate-x-1" data-icon="lucide:arrow-left">   <symbol id="ai:lucide:arrow-left" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m12 19l-7-7l7-7m7 7H5"/></symbol><use href="#ai:lucide:arrow-left"></use>  </svg> <div class="flex flex-col items-start overflow-hidden text-wrap"> <span class="text-muted-foreground text-left text-xs"> Previous Post </span> <span class="w-full text-left text-sm text-balance text-ellipsis"> Uncertainty makes the soul </span> </div>  </a>  <a href="/blog/tools#post-title" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-lg group flex items-center justify-end size-full" aria-disabled="false">  <div class="flex flex-col items-end overflow-hidden text-wrap"> <span class="text-muted-foreground text-right text-xs"> Next Post </span> <span class="w-full text-right text-sm text-balance text-ellipsis"> Sharpen your axe </span> </div> <svg width="1em" height="1em" class="ml-2 size-4 transition-transform group-hover:translate-x-1" data-icon="lucide:arrow-right">   <symbol id="ai:lucide:arrow-right" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 12h14m-7-7l7 7l-7 7"/></symbol><use href="#ai:lucide:arrow-right"></use>  </svg>  </a> </nav> <div id="toc-sidebar-container" class="sticky top-20 col-start-1 row-span-1 mr-8 ml-auto hidden h-[calc(100vh-5rem)] max-w-md xl:block"><astro-island uid="K0QmV" prefix="r18" component-url="/_astro/scroll-area.DpuOTeCe.js" component-export="ScrollArea" renderer-url="/_astro/client.CVI9NSBG.js" props="{&quot;className&quot;:[0,&quot;flex max-h-[calc(100vh-8rem)] flex-col overflow-y-auto&quot;],&quot;type&quot;:[0,&quot;hover&quot;],&quot;data-toc-scroll-area&quot;:[0,true]}" ssr client="load" opts="{&quot;name&quot;:&quot;ScrollArea&quot;,&quot;value&quot;:true}" await-children><div dir="ltr" data-slot="scroll-area" class="relative flex max-h-[calc(100vh-8rem)] flex-col overflow-y-auto" data-toc-scroll-area="true" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px"><style>[data-radix-scroll-area-viewport]{scrollbar-width:none;-ms-overflow-style:none;-webkit-overflow-scrolling:touch;}[data-radix-scroll-area-viewport]::-webkit-scrollbar{display:none}</style><div data-radix-scroll-area-viewport="" data-slot="scroll-area-viewport" class="ring-ring/10 dark:ring-ring/20 dark:outline-ring/40 outline-ring/50 size-full rounded-[inherit] transition-[color,box-shadow] focus-visible:ring-4 focus-visible:outline-1" style="overflow-x:hidden;overflow-y:hidden"><div style="min-width:100%;display:table"><astro-slot><div class="flex flex-col gap-2 px-4"><span class="text-lg font-medium">Table of Contents</span><ul class="flex list-none flex-col gap-y-2"><li class="text-sm text-foreground/60"><a href="#preqrequisites-and-notation" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="preqrequisites-and-notation">Preqrequisites and Notation</a></li><li class="text-sm text-foreground/60"><a href="#what-why-and-how" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="what-why-and-how">What, Why, and How?</a></li><li class="text-sm ml-4 text-foreground/60"><a href="#statistics-vs-machine-learning" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="statistics-vs-machine-learning">Statistics VS. Machine Learning</a></li><li class="text-sm ml-4 text-foreground/60"><a href="#humble-beginnings" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="humble-beginnings">Humble Beginnings</a></li><li class="text-sm text-foreground/60"><a href="#different-types-of-learning" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="different-types-of-learning">Different Types of Learning</a></li><li class="text-sm ml-4 text-foreground/60"><a href="#supervised-learning" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="supervised-learning">Supervised Learning</a></li><li class="text-sm ml-8 text-foreground/60"><a href="#the-classification-task" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="the-classification-task">The Classification Task</a></li><li class="text-sm ml-8 text-foreground/60"><a href="#the-classification-learning-problem" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="the-classification-learning-problem">The Classification Learning Problem</a></li><li class="text-sm ml-12 text-foreground/60"><a href="#metrics" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="metrics">Metrics</a></li><li class="text-sm ml-8 text-foreground/60"><a href="#different-approaches" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="different-approaches">Different Approaches</a></li><li class="text-sm ml-12 text-foreground/60"><a href="#generative-models" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="generative-models">Generative Models</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#bayes-optimal-classifier" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="bayes-optimal-classifier">Bayes Optimal Classifier</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#maximum-likelihood-estimate" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="maximum-likelihood-estimate">Maximum Likelihood Estimate</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#bayes-optimal-classifier-summary" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="bayes-optimal-classifier-summary">Bayes Optimal Classifier Summary</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#naive-bayes-classifier" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="naive-bayes-classifier">Naive Bayes Classifier</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#linear-discriminant-analysis-lda" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)</a></li><li class="text-sm ml-12 text-foreground/60"><a href="#summary-generative-models" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="summary-generative-models">Summary Generative Models</a></li><li class="text-sm ml-12 text-foreground/60"><a href="#discriminative-models" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="discriminative-models">Discriminative Models</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#logistic-regression" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="logistic-regression">Logistic Regression</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#support-vector-machines-svms" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="support-vector-machines-svms">Support Vector Machines (SVMs)</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#maximum-margin-principle" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="maximum-margin-principle">Maximum Margin Principle</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#computing-the-margin" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="computing-the-margin">Computing the Margin</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#why-is-maximize-the-margin-a-good-idea" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="why-is-maximize-the-margin-a-good-idea">Why is Maximize the Margin a Good Idea?</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#soft-margin" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="soft-margin">Soft-Margin</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#convex-optimization" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="convex-optimization">(Convex) Optimization</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#standard-form-of-convex-optimization" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="standard-form-of-convex-optimization">Standard Form of Convex Optimization</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#duality-and-kkt-conditions" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="duality-and-kkt-conditions">Duality and KKT Conditions</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#svms-continued" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="svms-continued">SVMs Continued</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#deriving-the-dual-problem" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="deriving-the-dual-problem">Deriving the Dual Problem</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#gradient-descent-and-variants" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="gradient-descent-and-variants">Gradient Descent and variants</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#k-nearest-neighbors" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="k-nearest-neighbors">$k$-Nearest Neighbors</a></li><li class="text-sm ml-8 text-foreground/60"><a href="#the-regression-task" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="the-regression-task">The Regression Task</a></li><li class="text-sm ml-8 text-foreground/60"><a href="#the-regression-learning-problem" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="the-regression-learning-problem">The Regression Learning Problem</a></li><li class="text-sm ml-12 text-foreground/60"><a href="#linear-regression" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="linear-regression">Linear Regression</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#ordinary-least-squares-ols" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="ordinary-least-squares-ols">Ordinary Least Squares (OLS)</a></li><li class="text-sm ml-16 text-foreground/60"><a href="#general-ols-derivation" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="general-ols-derivation">General OLS Derivation</a></li><li class="text-sm ml-4 text-foreground/60"><a href="#unsupervised-learning" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="unsupervised-learning">Unsupervised Learning</a></li><li class="text-sm text-foreground/60"><a href="#footnote-label" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="footnote-label">Footnotes</a></li></ul></div></astro-slot></div></div></div><!--astro:end--></astro-island></div><script type="module">class f{links=document.querySelectorAll("[data-heading-link]");activeIds=[];headings=[];regions=[];scrollArea=null;tocScrollArea=null;reset(){this.links=document.querySelectorAll("#toc-sidebar-container [data-heading-link]"),this.activeIds=[],this.headings=[],this.regions=[];const t=document.getElementById("toc-sidebar-container");this.scrollArea=t?.querySelector("[data-radix-scroll-area-viewport]")||null,this.tocScrollArea=t?.querySelector("[data-toc-scroll-area]")||null}}const e=new f;class c{static build(){if(e.headings=Array.from(document.querySelectorAll(".prose h2, .prose h3, .prose h4, .prose h5, .prose h6")),e.headings.length===0){e.regions=[];return}e.regions=e.headings.map((t,o)=>{const i=e.headings[o+1];return{id:t.id,start:t.offsetTop,end:i?i.offsetTop:document.body.scrollHeight}})}static getVisibleIds(){if(e.headings.length===0)return[];const t=window.scrollY+80,o=window.scrollY+window.innerHeight,i=new Set,l=(s,r)=>s>=t&&s<=o||r>=t&&r<=o||s<=t&&r>=o;return e.headings.forEach(s=>{const r=s.offsetTop+s.offsetHeight;l(s.offsetTop,r)&&i.add(s.id)}),e.regions.forEach(s=>{if(s.start<=o&&s.end>=t){const r=document.getElementById(s.id);if(r){const a=r.offsetTop+r.offsetHeight;s.end>a&&(a<o||t<s.end)&&i.add(s.id)}}}),Array.from(i)}}class h{static update(){if(!e.scrollArea||!e.tocScrollArea)return;const{scrollTop:t,scrollHeight:o,clientHeight:i}=e.scrollArea,l=5,s=t<=l,r=t>=o-i-l;e.tocScrollArea.classList.toggle("mask-t-from-90%",!s),e.tocScrollArea.classList.toggle("mask-b-from-90%",!r)}}class g{static update(t){e.links.forEach(o=>{o.classList.remove("text-foreground")}),t.forEach(o=>{if(o){const i=document.querySelector(`#toc-sidebar-container [data-heading-link="${o}"]`);i&&i.classList.add("text-foreground")}}),this.scrollToActive(t)}static scrollToActive(t){if(!e.scrollArea||!t.length)return;const o=document.querySelector(`#toc-sidebar-container [data-heading-link="${t[0]}"]`);if(!o)return;const{top:i,height:l}=e.scrollArea.getBoundingClientRect(),{top:s,height:r}=o.getBoundingClientRect(),a=s-i+e.scrollArea.scrollTop,u=Math.max(0,Math.min(a-(l-r)/2,e.scrollArea.scrollHeight-e.scrollArea.clientHeight));Math.abs(u-e.scrollArea.scrollTop)>5&&(e.scrollArea.scrollTop=u)}}class d{static handleScroll(){const t=c.getVisibleIds();JSON.stringify(t)!==JSON.stringify(e.activeIds)&&(e.activeIds=t,g.update(e.activeIds))}static handleTOCScroll=()=>h.update();static handleResize(){c.build();const t=c.getVisibleIds();JSON.stringify(t)!==JSON.stringify(e.activeIds)&&(e.activeIds=t,g.update(e.activeIds)),h.update()}static init(){if(e.reset(),c.build(),e.headings.length===0){g.update([]);return}this.handleScroll(),setTimeout(h.update,100);const t={passive:!0};window.addEventListener("scroll",this.handleScroll,t),window.addEventListener("resize",this.handleResize,t),e.scrollArea?.addEventListener("scroll",this.handleTOCScroll,t)}static cleanup(){window.removeEventListener("scroll",this.handleScroll),window.removeEventListener("resize",this.handleResize),e.scrollArea?.removeEventListener("scroll",this.handleTOCScroll),Object.assign(e,{activeIds:[],headings:[],regions:[],scrollArea:null,tocScrollArea:null})}}document.addEventListener("astro:page-load",()=>d.init());document.addEventListener("astro:after-swap",()=>{d.cleanup(),d.init()});document.addEventListener("astro:before-swap",()=>d.cleanup());</script> <article class="prose col-start-2 max-w-none"> <!doctype html><html lang="en"><head></head><body>


<meta charset="utf-8">
<title>index</title>
<meta content="width=device-width, initial-scale=1" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" rel="stylesheet">

<svg xmlns="http://www.w3.org/2000/svg" style="display:none"><defs>
        <symbol id="info" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="10"></circle><path d="M12 16v-4"></path><path d="M12 8h.01"></path>
        </symbol>
        <symbol id="lightbulb" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M15 14c.2-1 .7-1.7 1.5-2.5 1-.9 1.5-2.2 1.5-3.5A6 6 0 0 0 6 8c0 1 .2 2.2 1.5 3.5.7.7 1.3 1.5 1.5 2.5"></path><path d="M9 18h6"></path><path d="M10 22h4"></path>
        </symbol>
        <symbol id="alert-triangle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="m21.73 18-8-14a2 2 0 0 0-3.48 0l-8 14A2 2 0 0 0 4 21h16a2 2 0 0 0 1.73-3"></path><path d="M12 9v4"></path><path d="m12 17h.01"></path>
        </symbol>
        <symbol id="shield-alert" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M20 13c0 5-3.5 7.5-7.66 8.95a1 1 0 0 1-.67-.01C7.5 20.5 4 18 4 13V6a1 1 0 0 1 1-1c2 0 4.5-1.2 6.24-2.72a1.17 1.17 0 0 1 1.52 0C14.51 3.81 17 5 19 5a1 1 0 0 1 1 1z"></path><path d="M12 8v4"></path><path d="M12 16h.01"></path>
        </symbol>
        <symbol id="message-square-warning" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M22 17a2 2 0 0 1-2 2H6.828a2 2 0 0 0-1.414.586l-2.202 2.202A.71.71 0 0 1 2 21.286V5a2 2 0 0 1 2-2h16a2 2 0 0 1 2 2z"></path><path d="M12 15h.01"></path><path d="m12 17v4"></path>
        </symbol>
        <symbol id="book-open" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 7v14"></path><path d="M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z"></path>
        </symbol>
        <symbol id="anchor" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 22V8"></path><path d="M5 12H2a10 10 0 0 0 20 0h-3"></path><circle cx="12" cy="5" r="3"></circle>
        </symbol>
        <symbol id="pen-tool" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M15.707 21.293a1 1 0 0 1-1.414 0l-1.586-1.586a1 1 0 0 1 0-1.414l5.586-5.586a1 1 0 0 1 1.414 0l1.586 1.586a1 1 0 0 1 0 1.414z"></path><path d="m18 13-1.375-6.874a1 1 0 0 0-.746-.776L3.235 2.028a1 1 0 0 0-1.207 1.207L5.35 15.879a1 1 0 0 0 .776.746L13 18"></path><path d="m2.3 2.3 7.286 7.286"></path><circle cx="11" cy="11" r="2"></circle>
        </symbol>
        <symbol id="check-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="10"></circle><path d="m9 12 2 2 4-4"></path>
        </symbol>
        <symbol id="puzzle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M15.39 4.39a1 1 0 0 0 1.68-.474 2.5 2.5 0 1 1 3.014 3.015 1 1 0 0 0-.474 1.68l1.683 1.682a2.414 2.414 0 0 1 0 3.414L19.61 15.39a1 1 0 0 1-1.68-.474 2.5 2.5 0 1 0-3.014 3.015 1 1 0 0 1 .474 1.68l-1.683 1.682a2.414 2.414 0 0 1-3.414 0L8.61 19.61a1 1 0 0 0-1.68.474 2.5 2.5 0 1 1-3.014-3.015 1 1 0 0 0 .474-1.68l-1.683-1.682a2.414 2.414 0 0 1 0-3.414L4.39 8.61a1 1 0 0 1 1.68.474 2.5 2.5 0 1 0 3.014-3.015 1 1 0 0 1-.474-1.68l1.683-1.682a2.414 2.414 0 0 1 3.414 0z"></path>
        </symbol>
        <symbol id="git-branch" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <line x1="6" x2="6" y1="3" y2="15"></line><circle cx="18" cy="6" r="3"></circle><circle cx="6" cy="18" r="3"></circle><path d="M18 9a9 9 0 0 1-9 9"></path>
        </symbol>
        <symbol id="file-text" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path>
        </symbol>
        <symbol id="help-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="10"></circle><path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3"></path><path d="M12 17h.01"></path>
        </symbol>
        <symbol id="check-square" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <rect width="18" height="18" x="3" y="3" rx="2"></rect><path d="m9 12 2 2 4-4"></path>
        </symbol>
        <symbol id="message-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M2.992 16.342a2 2 0 0 1 .094 1.167l-1.065 3.29a1 1 0 0 0 1.236 1.168l3.413-.998a2 2 0 0 1 1.099.092 10 10 0 1 0-4.777-4.719"></path>
        </symbol>
        <symbol id="rotate-ccw" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M3 12a9 9 0 1 0 9-9 9.75 9.75 0 0 0-6.74 2.74L3 8"></path><path d="M3 3v5h5"></path>
        </symbol>
        <symbol id="code" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="m16 18 6-6-6-6"></path><path d="m8 6-6 6 6 6"></path>
        </symbol>
        <symbol id="dumbbell" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M17.596 12.768a2 2 0 1 0 2.829-2.829l-1.768-1.767a2 2 0 0 0 2.828-2.829l-2.828-2.828a2 2 0 0 0-2.829 2.828l-1.767-1.768a2 2 0 1 0-2.829 2.829z"></path><path d="m2.5 21.5 1.4-1.4"></path><path d="m20.1 3.9 1.4-1.4"></path><path d="M5.343 21.485a2 2 0 1 0 2.829-2.828l1.767 1.768a2 2 0 1 0 2.829-2.829l-6.364-6.364a2 2 0 1 0-2.829 2.829l1.768 1.767a2 2 0 0 0-2.828 2.829z"></path><path d="m9.6 14.4 4.8-4.8"></path>
        </symbol>
        <symbol id="alert-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="10"></circle><line x1="12" x2="12" y1="8" y2="12"></line><line x1="12" x2="12.01" y1="16" y2="16"></line>
        </symbol>
        <symbol id="check" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M20 6 9 17l-5-5"></path>
        </symbol>
        <symbol id="check-circle-2" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="m9 12 2 2 4-4"></path><circle cx="12" cy="12" r="9"></circle>
        </symbol>
        <symbol id="list" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M3 5h.01"></path><path d="M3 12h.01"></path><path d="M3 19h.01"></path><path d="M8 5h13"></path><path d="M8 12h13"></path><path d="M8 19h13"></path>
        </symbol>
        <symbol id="chevron-down" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="m6 9 6 6 6-6"></path>
        </symbol>
        <symbol id="cpu" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 20v2"></path><path d="M12 2v2"></path><path d="M17 20v2"></path><path d="M17 2v2"></path><path d="M2 12h2"></path><path d="M2 17h2"></path><path d="M2 7h2"></path><path d="M20 12h2"></path><path d="M20 17h2"></path><path d="M20 7h2"></path><path d="M7 20v2"></path><path d="M7 2v2"></path><rect x="4" y="4" width="16" height="16" rx="2"></rect><rect x="8" y="8" width="8" height="8" rx="1"></rect>
        </symbol></defs></svg>
<p>As of writing this, last night as I was going to sleep, I was lying in bed and thought about the future.
After some rambling I came up with this (hopefully) series, <strong>Rezvan Explains</strong>. I took a course called <a href="https://www.cityu.edu.hk/catalogue/ug/current/course/CS4487.htm" rel="nofollow noreferrer noopener" target="_blank">machine learning</a> during my exchange semester @ CityUHK taught by <a href="https://kedema.org" rel="nofollow noreferrer noopener" target="_blank">Kede Ma</a> (wonderful professor and I highly recommend taking the course).
This post is very inspired by his course but, I want to add my touch on the topic.</p>
<p>My goal for this part is to build your <strong>intuition</strong> when it comes to machine learning.
I believe most people getting started with AI nowadays don’t want to understand the foundations modern deep learning is built upon.</p>
<h2 id="preqrequisites-and-notation">Preqrequisites and Notation</h2>
<p>This post will assume that you know about:</p>
<ul>
<li>Linear Algebra</li>
<li>Multivariable and Matrix Calculus</li>
<li>Probability and Statistics</li>
</ul>
<p>If you don’t feel comfortable with these topics, read up and come back when ready.</p>
<p>These are the notations that we will use, there is no need to go through them the first time you’re reading this, it’s meant as a <strong>reference</strong>.</p>





















































<table><thead><tr><th><strong>Symbol</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td>$M$</td><td>Number of Examples in Dataset</td></tr><tr><td>$N$</td><td>Number of Features in Training Sample</td></tr><tr><td>$\mathcal{D}$</td><td>Dataset of $M$ Examples</td></tr><tr><td>$\mathbf{x}$</td><td>$N$-dimensional Feature Vector</td></tr><tr><td>$y$</td><td>Class Label</td></tr><tr><td>$\mathbf{x}^{(i)}$</td><td>$i$-th Feature Vector in Dataset in $\mathcal{D}$</td></tr><tr><td>$\mathbf{x}^{(i)}_j$</td><td>$j$-th Feature in $i$-th Feature Vector in $\mathcal{D}$</td></tr><tr><td>$y^{(i)}$</td><td>$i$-th Class Label in Dataset in $\mathcal{D}$ corresponding to $\mathbf{x}^{(i)}$</td></tr><tr><td>$\mathbf{X}$</td><td>Input Space</td></tr><tr><td>$\mathbf{Y}$</td><td>Output Space</td></tr><tr><td>$| \mathbf{Y} |$</td><td>Number of Classes in Output Space</td></tr></tbody></table>
<h2 id="what-why-and-how">What, Why, and How?</h2>
<p>There is one fundamental flaw I’ve seen with <strong>every</strong> machine learning course I’ve taken.
Machine learning is a study of <strong>statistical algorithms</strong>, i.e., statistics and machine learning are <em>very</em> closely related fields.
But the distinct difference is in their principal goal.</p>
<h3 id="statistics-vs-machine-learning">Statistics VS. Machine Learning</h3>
<p>Generally in machine learning we want to find <strong>generalizable predictive patterns.</strong> <sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>
While this might not be exclusive to machine learning (a large part of statistics is also about prediction), in machine learning, it is the <strong>main goal</strong>.</p>
<p>I’m not going into the history of machine learning as a field and its evolution, but if it is one thing I want you to take away from this post, it is that machine learning <strong>fundamentally relies on data</strong>.
I can’t stress this enough, a lot of my fellow peers never got this intuition at the start of our courses, and therefore everything during the semester seemed like black magic.
But in the end <strong>everything</strong> comes down to <strong>data</strong>.</p>
<h3 id="humble-beginnings">Humble Beginnings</h3>
<p>Okay I lied, a bit of history won’t hurt.
The name <em>machine learning</em> implies that a machine is learning (duh).
I think the <em>machine</em> part is quite obvious (a computer), but learning is a bit more abstract.</p>
<p>A lot of people have defined <em>learning</em> in varying ways and for different domains, let’s take a look:</p>
<ul>
<li>Behaviorism (Skinner)
<ul>
<li>
<blockquote>
<p>Learning is a long-term change in behavior due to experience. <sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup></p>
</blockquote>
</li>
</ul>
</li>
<li>Cognitivism (Gestalt School)
<ul>
<li>
<blockquote>
<p>Learning is an internal mental process that integrates new information into established mental frameworks and updates those frameworks over time. <sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup></p>
</blockquote>
</li>
</ul>
</li>
<li>Connectionism (Hebbian Learning)
<ul>
<li>
<blockquote>
<p>Learning is a physical process in which neurons join by developing the synapses between them. <sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup></p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p>We can see that in an abstract sense, learning is taking some input and changing some internal state based on that input.
The last one is an outlier, but remember this for later.</p>
<h2 id="different-types-of-learning">Different Types of Learning</h2>
<p>Depending on the <strong>type of data</strong> we have but also <strong>what we want to achieve</strong>, the learning process <em>needs</em> to be different.</p>
<p>In this post, we will focus on <strong>supervised learning</strong> and <strong>unsupervised learning</strong>.</p>
<p>Let’s define supervised learning in mathematical terms.</p>
<h3 id="supervised-learning">Supervised Learning</h3>
<p>In supervised learning, we have a dataset consisting of well-defined input-output pairs.
The task is to find a (good) <strong>generalized mapping</strong> from an arbitrary input to an output.</p>
<p>Mathematically we define this as,</p>
<p>$$
\begin{equation}
f : \mathbf{X} \mapsto \mathbf{Y},
\end{equation}
$$</p>
<p>where $\mathbf{X}$ is the input space and $\mathbf{Y}$ is the output space, we want to find the <em>best</em> function $f$ that maps $\mathbf{X}$ to $\mathbf{Y}$.</p>
<h4 id="the-classification-task">The Classification Task</h4>
<p>The best way to understand supervised learning is through the <strong>classification task</strong> and <em>Fisher’s Iris dataset</em>. <sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup></p>
<p>In Fisher’s Iris dataset we have 150 samples of input-output pairs.
The input is a vector with four <strong>features</strong> — sepal length, sepal width, petal length, petal width — the output is the <strong>species</strong> of the iris flower.</p>
<p>As described above, our input and output spaces are,</p>
<p>$$
\begin{equation}
\begin{aligned}
\mathbf{X} &#x26; = \{ \text{Sepal Length}, \text{Sepal Width}, \text{Petal Length}, \text{Petal Width} \} \newline
\mathbf{Y} &#x26; = \{ \text{Setosa}, \text{Versicolor}, \text{Virginica} \}
\end{aligned}
\end{equation}
$$</p>
<p>So given a vector with four real numbers (where each number describes a feature of the iris flower), we want to predict a number that corresponds to the species of the iris flower.</p>
<p>Thus, the proper mathematical definition of the classification task is,</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-teal-600 dark:bg-teal-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-teal-700 dark:text-teal-400" data-lucide="book-open" viewBox="0 0 24 24"><use href="#book-open"></use></svg>Definition: Definition: The Classification Task<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-teal-700 dark:text-teal-400" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Given a feature vector $\mathbf{x} \in \mathbf{X} = \mathbb{R}^N$ that describes an object that belongs to one of $C$ classes from the set $\mathbf{Y} = \{ 1, 2, \ldots, C \}$, predict the class label $y \in \mathbf{Y}$.</p></div>
</details>
<h4 id="the-classification-learning-problem">The Classification Learning Problem</h4>
<p>Now that we have defined the classification task, let’s define the classification learning problem,</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-teal-600 dark:bg-teal-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-teal-700 dark:text-teal-400" data-lucide="book-open" viewBox="0 0 24 24"><use href="#book-open"></use></svg>Definition: Definition: The Classification Learning Problem<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-teal-700 dark:text-teal-400" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Given a dataset of example pairs $\mathcal{D} = \{ (\mathbf{x}^{(i)}, y^{(i)}), i = 1, \ldots, M \}$ where $\mathbf{x}^{(i)} \in \mathbf{X} = \mathbb{R}^N$ is a feature vector and $y^{(i)} \in \mathbf{Y} = \{1, \ldots, C\}$ is the class label, learn a function,
$$
f: \mathbb{R}^N \mapsto \mathbf{Y}
$$
that accurately predicts the class label $y$ for any feature vector $\mathbf{x}$.</p></div>
</details>
<h5 id="metrics">Metrics</h5>
<p>To determine how well our function $f$ is doing, we need to define some (primal) metrics.</p>
<p>Firstly, let’s define the <strong>indicator function</strong>,</p>
<p>$$
\begin{equation}
\mathbb{I}[A] = \begin{cases}
1, &#x26; \text{if } A \text{ is true}, \newline
0, &#x26; \text{otherwise}.
\end{cases}
\end{equation}
$$</p>
<p>The <strong>classification error</strong> is then defined as,</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-teal-600 dark:bg-teal-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-teal-700 dark:text-teal-400" data-lucide="book-open" viewBox="0 0 24 24"><use href="#book-open"></use></svg>Definition: Definition: The Classification Error<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-teal-700 dark:text-teal-400" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Given a dataset of example pairs $\mathcal{D} = \{ (\mathbf{x}^{(i)}, y^{(i)}), i = 1, \ldots, M \}$ and a function $f: \mathbb{R}^N \mapsto \mathbf{Y}$, the classification error of $f$ on $\mathcal{D}$ is defined as,
$$
\begin{equation}
\text{Error}(f, \mathcal{D}) = \frac{1}{M} \sum_{i=1}^{M} \mathbb{I}[f(\mathbf{x}^{(i)}) \neq y^{(i)}].
\end{equation}
$$</p></div>
</details>
<p>I.e., the classification error is the fraction of examples in the dataset that are misclassified by the function $f$.</p>
<p>The <strong>classification accuracy</strong> is then defined as,</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-teal-600 dark:bg-teal-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-teal-700 dark:text-teal-400" data-lucide="book-open" viewBox="0 0 24 24"><use href="#book-open"></use></svg>Definition: Definition: The Classification Accuracy<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-teal-700 dark:text-teal-400" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Given a dataset of example pairs $\mathcal{D} = \{ (\mathbf{x}^{(i)}, y^{(i)}), i = 1, \ldots, M \}$ and a function $f: \mathbb{R}^N \mapsto \mathbf{Y}$, the classification accuracy of $f$ on $\mathcal{D}$ is defined as,
$$
\begin{equation}
\text{Accuracy}(f, \mathcal{D}) = \frac{1}{M} \sum_{i=1}^{M} \mathbb{I}[f(\mathbf{x}^{(i)}) = y^{(i)}] = 1 - \text{Error}(f, \mathcal{D}).
\end{equation}
$$</p></div>
</details>
<h4 id="different-approaches">Different Approaches</h4>
<p>But how do we solve the classification learning problem?
We will go through different ways of approaching this problem.</p>
<p>But I want to start of with the <strong>probabilistic approach</strong> first.</p>
<h5 id="generative-models">Generative Models</h5>
<p>We know that our dataset $\mathcal{D}$ consists of input-output pairs $(\mathbf{x}^{(i)}, y^{(i)})$.
In the real dataset, we have a <strong>joint distribution</strong> $p(\mathbf{x}, y)$, this describes the probability of observing the pair $(\mathbf{x}, y)$.</p>
<p>We can learn this joint distribution from the dataset.
We know from <strong>Bayes’ Rule</strong> that,</p>
<p>$$
\begin{equation}
p(\mathbf{x}, y) = p(y) p(\mathbf{x} | y).
\end{equation}
$$</p>
<p>These two terms are very important, let’s define them properly.</p>
<p>The first term $p(y)$ is often called the <strong>prior distribution</strong> of the classes.
It describes how frequently each class occurs in nature (or in the dataset).</p>
<p>The second term $p(\mathbf{x} | y)$ is called the <strong>class-conditional distribution</strong>.
This distribution describes how the features $\mathbf{x}$ are distributed for different classes $y$.</p>
<p>To remind you, the <strong>classification task</strong> is to predict the <strong>class label $y$ given a feature vector $\mathbf{x}$</strong>.</p>
<p>So again, by Bayes’s Rule we know that,</p>
<p>$$
\begin{equation}
p(y | \mathbf{x}) = \frac{p(\mathbf{x}, y)}{p(\mathbf{x})} = \frac{p(y) p(\mathbf{x} | y)}{p(\mathbf{x})}.
\end{equation}
$$</p>
<p>So for the classification task, we want to choose the <strong>best prediction</strong>.</p>
<p>Let’s clarify the difference between the <strong>$\max$</strong> and <strong>$\arg \max$</strong> operators.
The $\max$ returns the maximum <strong>value</strong> of a function, while the $\arg \max$ returns the <strong>argument</strong> that gives the maximum value.</p>
<p>Or in mathematical terms,</p>
<p>$$
\begin{equation}
\max_{\mathbf{x}} f(\mathbf{x}) = \{f(\mathbf{x}) | \mathbf{x} \in \mathcal{D} \land \forall \mathbf{y} \in \mathcal{D}, f(\mathbf{y}) \leq f(\mathbf{x})\}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\underset{\mathbf{x}}{\arg\max} f(\mathbf{x}) = \{\mathbf{x} | \mathbf{x} \in \mathcal{D} \land \forall \mathbf{y} \in \mathcal{D}, f(\mathbf{y}) \leq f(\mathbf{x})\}
\end{equation}
$$</p>
<p>Therefore, the best prediction for the classification task is,</p>
<p>$$
\begin{equation}
\hat{y} = \arg \max_{y \in \mathbf{Y}} p(y | \mathbf{x}) = \arg \max_{y \in \mathbf{Y}} \frac{p(y) p(\mathbf{x} | y)}{p(\mathbf{x})}.
\end{equation}
$$</p>
<p>If we are looking to <strong>maximize the fraction</strong>, we can <strong>ignore the denominator</strong> $p(\mathbf{x})$ since we know that it is <strong>non-negative and class-independent</strong>.</p>
<p>$$
\begin{equation}
\hat{y} = \arg \max_{y \in \mathbf{Y}} p(y) p(\mathbf{x} | y).
\end{equation}
$$</p>
<p>So if we can estimate the <strong>prior distribution</strong> $p(y)$ and the <strong>class-conditional distribution</strong> $p(\mathbf{x} | y)$, we can predict the class label $y$ for <strong>any feature vector</strong> $\mathbf{x}$.</p>
<p>Let’s start with the <strong>prior distribution</strong> $p(y)$, since it is the easiest to learn.
From our dataset $\mathcal{D} = \{ (\mathbf{x}^{(i)}, y^{(i)}), i = 1, \ldots, M \}$, with well-defined input-output pairs we simply count how often each class label $y$ appears,</p>
<p>$$
\begin{equation}
p(y = c) \approx \frac{\sum_{i = 1}^M \mathbb{I}[y^{(i)} = c]}{M}.
\end{equation}
$$</p>
<p><strong>Note</strong>, that this is the simplest case if $y$ only captures one class, it is possible that for a different task that $y$ is a vector of classes (e.g., multi-label classification).</p>
<p>The class-conditional distribution $p(\mathbf{x} | y)$ is a bit more tricky. How do we model this distribution?</p>
<figure><img alt="Histogram of the petal length of all three species of iris flowers." loading="lazy" decoding="async" fetchpriority="auto" width="540" height="447" src="/_astro/iris_histogram.C8DSL9I6_2jI51p.svg" ><figcaption>Histogram of the petal length of all three species of iris flowers.</figcaption></figure>
<p>By looking at Figure 1, we see that it is a bit noisy, but we can see <em>some</em> Gaussian structure.
Let’s assume a probability model for the class conditional $p(\mathbf{x} | y)$, in our case <strong>Gaussian</strong>.</p>
<h6 id="bayes-optimal-classifier">Bayes Optimal Classifier</h6>
<p>So, each class is modeled as a <strong>separate Gaussian</strong> distribution <strong>of the feature values</strong>, or in other words,</p>
<p>$$
\begin{equation}
p(\mathbf{x} | y) = \frac{1}{\sqrt{2 \pi \sigma_{c}^2}} e^{-\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2}
\end{equation}
$$</p>
<p>Each class has its own mean and variance parameters ($\mu_{c}$ and $\sigma_{c}^2$).
But what’s considered the “best” estimates, and how do we find them?</p>
<h6 id="maximum-likelihood-estimate">Maximum Likelihood Estimate</h6>
<p>We want our model to be as close to the true distribution as possible.
Meaning that <strong>our (already) observed data should be the most probable</strong> under our model.</p>
<p>The <strong>likelihood function</strong> measures how well a model explains the observed data given the model parameters <sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup>.</p>
<p><strong>Maximum Likelihood Estimate (MLE)</strong> is a method for estimating said parameters by maximizing the likelihood function.</p>
<p><strong>Note</strong>, there are more sophisticated methods (e.g., Bayesian estimation, Maximum A Posteriori (MAP) estimation, etc.), but MLE has very nice properties and is good enough for most cases.</p>
<p>In our case, the likelihood function is defined as,</p>
<p>$$
\begin{equation}
L(\mu_c, \sigma_c^2) = \prod_{i=1}^{M_c} \frac{1}{\sqrt{2 \pi \sigma_{c}^2}} e^{-\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2},
\end{equation}
$$</p>
<p>where <strong>$M_c$ is the number of samples in class $c$</strong>, since the parameters are different for each class.</p>
<p>Thus, our MLE estimates are,</p>
<p>$$
\begin{equation}
(\hat{\mu_c}, \hat{\sigma_c^2}) = \underset{\mu_c, \sigma_c^2}{\arg\max} \prod_{i=1}^{M_c} \frac{1}{\sqrt{2 \pi \sigma_{c}^2}} e^{-\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2}.
\end{equation}
$$</p>
<p>Let’s use a little trick to avoid the product notation.
The <strong>logarithm</strong> is a <strong>monotonic function</strong>, meaning that the <strong>order of the values doesn’t change</strong>.
In other words, the <strong>maximum value before applying the logarithm is still the maximum value</strong> after applying the logarithm (just the scale changes).</p>
<p>Thus, taking the <strong>MLE of the log-likelihood function is equivalent</strong> to taking the MLE of the likelihood function.</p>
<p>$$
\begin{equation}
(\hat{\mu_c}, \hat{\sigma_c^2}) = \underset{\mu_c, \sigma_c^2}{\arg\max} \log \left[\prod_{i=1}^{M_c} \frac{1}{\sqrt{2 \pi \sigma_{c}^2}} e^{-\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2}\right].
\end{equation}
$$</p>
<p>Now, the product becomes a sum (since the logarithm of a product is the sum of the logarithms).</p>
<p>$$
\begin{equation}
(\hat{\mu_c}, \hat{\sigma_c^2}) = \underset{\mu_c, \sigma_c^2}{\arg\max} \sum_{i=1}^{M_c} \log \left[\frac{1}{\sqrt{2 \pi \sigma_{c}^2}} e^{-\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2}\right].
\end{equation}
$$</p>
<p>Now, I am going to simplify the expression step-by-step, at some steps I will have a $\big\rvert \cdot$ to indicate what operation we perform at that step.</p>
<p>$$
\begin{aligned}
\log \left[\frac{1}{\sqrt{2 \pi \sigma_{c}^2}} e^{-\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2}\right] &#x26;= \log \left[\frac{1}{\sqrt{2 \pi \sigma_{c}^2}}\right] + \log \left[e^{-\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2}\right] \ \biggr\rvert \log(ab) = \log(a) + \log(b) \newline
&#x26;= \log(1) - \log(\sqrt{2 \pi \sigma_{c}^2}) + \log \left[e^{-\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2}\right] \ \biggr\rvert \log(a/b) = \log(a) - \log(b) \newline
&#x26;= -\log(\sqrt{2 \pi \sigma_{c}^2}) + \log \left[e^{-\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2}\right] \ \biggr\rvert \log(1) = 0 \newline
&#x26;= -\log(\sqrt{2 \pi \sigma_{c}^2}) -\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2 \ \biggr\rvert \log(e^a) = a \newline
\end{aligned}
$$</p>
<p>So, our MLE estimates are,</p>
<p>$$
\begin{equation}
(\hat{\mu_c}, \hat{\sigma_c^2}) = \underset{\mu_c, \sigma_c^2}{\arg\max} \sum_{i=1}^{M_c} \left[-\log(\sqrt{2 \pi \sigma_{c}^2}) -\frac{1}{2 \sigma_{c}^2} \left(x^{(i)} - \mu_{c}\right)^2\right].
\end{equation}
$$</p>
<p>To find the maximum, we need to take the derivative of the expression with respect to $\mu_c$ and $\sigma_c^2$ and set them to zero.</p>
<p>Let’s start with $\mu_c$ since it’s easier.</p>
<p>$$
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \mu_c} \sum \left[-\log(\sqrt{2 \pi \sigma_c^2}) -\frac{1}{2 \sigma_c^2} \left(x^{(i)} - \mu_c\right)^2\right] &#x26;= 0 \newline
\sum \frac{\partial}{\partial \mu_c} \left[\underbrace{-\log(\sqrt{2 \pi \sigma_c^2})}_0 -\frac{1}{2 \sigma_c^2} \left(x^{(i)} - \mu_c\right)^2\right] &#x26;= 0 \newline
\sum \frac{1}{\sigma_c^2} \left(x^{(i)} - \mu_c\right) &#x26;= 0 \ \biggr\rvert \text{Chain Rule} \newline
\sum \left(x^{(i)} - \mu_c\right) &#x26;= 0 \ \biggr\rvert \cdot \sigma_c^2 \newline
\sum x^{(i)} - \sum \mu_c &#x26;= 0 \newline
\sum x^{(i)} - M_c \mu_c &#x26;= 0 \ \biggr\rvert \text{ Since $\mu_c$ doesn’t depend on $i$} \newline
\hat{\mu_c} &#x26;= \boxed{\frac{1}{M_c} \sum x^{(i)}}
\end{aligned}
\end{equation}
$$</p>
<p>Now, let’s find $\sigma_c^2$.</p>
<p>$$
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \sigma_c^2} \sum \left[-\log(\sqrt{2 \pi \sigma_c^2}) -\frac{1}{2 \sigma_c^2} \left(x^{(i)} - \mu_c\right)^2\right] &#x26;= 0 \newline
\sum \frac{\partial}{\partial \sigma_c^2} \left[-\log(\sqrt{2 \pi \sigma_c^2}) -\frac{1}{2 \sigma_c^2} \left(x^{(i)} - \mu_c\right)^2\right] &#x26;= 0 \newline
\sum \frac{\partial}{\partial \sigma_c^2} \left[-\log(\left(2 \pi \sigma_c^2\right)^{1/2}) -\frac{1}{2 \sigma_c^2} \left(x^{(i)} - \mu_c\right)^2\right] &#x26;= 0 \ \biggr\rvert \sqrt{a} = a^{1/2} \newline
\sum \frac{\partial}{\partial \sigma_c^2} \left[-\frac{1}{2} \log(2 \pi \sigma_c^2) -\frac{1}{2 \sigma_c^2} \left(x^{(i)} - \mu_c\right)^2\right] &#x26;= 0 \ \biggr\rvert \log(a^b) = b \log(a) \newline
\sum \left[-\frac{1}{2} \frac{1}{2 \pi \sigma_c^2} \cdot 2 \pi + \frac{1}{2 \sigma_c^4} \left(x^{(i)} - \mu_c\right)^2\right] &#x26;= 0 \newline
\sum \left[-\frac{1}{2 \sigma_c^2} + \frac{1}{2 \sigma_c^4} \left(x^{(i)} - \mu_c\right)^2\right] &#x26;= 0 \newline
\sum \left[-1 + \frac{1}{\sigma_c^2} \left(x^{(i)} - \mu_c\right)^2\right] &#x26;= 0 \ \biggr\rvert \cdot 2\sigma_c^2 \newline
-M_c + \sum \frac{1}{\sigma_c^2} \left(x^{(i)} - \mu_c\right)^2 &#x26;= 0 \newline
\sum \frac{1}{\sigma_c^2} \left(x^{(i)} - \mu_c\right)^2 &#x26;= M_c \newline
\frac{1}{\sigma_c^2} \sum \left(x^{(i)} - \mu_c\right)^2 &#x26;= M_c \ \biggr\rvert \text{Since $\sigma_c^2$ doesn’t depend on $i$} \newline
\hat{\sigma_c^2} &#x26;= \boxed{\frac{1}{M_c} \sum \left(x^{(i)} - \hat{\mu_c}\right)^2}
\end{aligned}
\end{equation}
$$</p>
<p>In our example from Figure 1, if we plot the Gaussian estimates, we get the following plot,</p>
<figure><img alt="Histogram of the petal length of all three species of iris flowers and their corresponding Gaussian estimates." loading="lazy" decoding="async" fetchpriority="auto" width="544" height="447" src="/_astro/iris_gaussian.DEcDliqS_2jYlMa.svg" ><figcaption>Histogram of the petal length of all three species of iris flowers and their corresponding Gaussian estimates.</figcaption></figure>
<p>We can see that Figure 2 indeed captures these distributions quite nicely.</p>
<h6 id="bayes-optimal-classifier-summary">Bayes Optimal Classifier Summary</h6>
<p>Let’s summarize what we have learned about Bayes Optimal Classifier and how we can use the Bayesian Decision Rule to classify new data points.</p>
<p>Given an observation $\mathbf{x}$, we want to pick the class $c$ with the <strong>highest posterior probability</strong> $p(y = c | \mathbf{x})$,</p>
<p>$$
\begin{equation}
f_B(\mathbf{x}) = \underset{c \in \mathbf{Y}}{\arg\max} \ p(y = c | \mathbf{x})
\end{equation}
$$</p>
<p>But in most real-world scenarios we don’t have $p(y | \mathbf{x})$ or any reliable way to estimate it, we only $p(y)$ and $p(\mathbf{x} | y)$.</p>
<p>So by using Bayes’ rule, the posterior probability can be expressed as,</p>
<p>$$
\begin{equation}
p(y = c | \mathbf{x}) = \frac{p(\mathbf{x} | y = c) \ p(y = c)}{p(\mathbf{x})}
\end{equation}
$$</p>
<figure><img alt="The class conditional densities and posterior probabilities for the Iris dataset." loading="lazy" decoding="async" fetchpriority="auto" width="755" height="369" src="/_astro/iris_probabilities.BqAlrlPo_Zr0J8n.svg" ><figcaption>The class conditional densities and posterior probabilities for the Iris dataset.</figcaption></figure>
<p>We can see that we have a <strong>decision boundary</strong>, or where the posterior probabilities meet.
We’ll talk more about decision boundaries later.</p>
<p>So, in summary for the Bayes Optimal Classifier,</p>
<ul>
<li><strong>Training</strong>
<ul>
<li>Estimate the class conditional densities $p(x | y = c)$ <strong>for each class $c$</strong> using <strong>MLE</strong> where distribution is assumed to be <strong>Gaussian</strong>.</li>
<li>Estimate the prior probabilities $p(y = c)$ using <strong>MLE</strong>.</li>
</ul>
</li>
<li><strong>Classification</strong>
<ul>
<li>Given a new sample $\mathbf{x}^{\star}$, calculate the probability $p(\mathbf{x}^{\star} | y = c)$ for each class $c$.</li>
<li>Pick the class $c$ with the <strong>largest posterior probability</strong> p(y = c | $\mathbf{x}^{\star}$).
<ul>
<li>Equivalently use $p(\mathbf{x}^{\star} | y = c) p(y = c)$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The Bayes Optimal Classifier is the best classifier in theory, but it is often not practical because it is hard estimating $p(\mathbf{x} | y = c)$.</p>
<h6 id="naive-bayes-classifier">Naive Bayes Classifier</h6>
<p><strong>Technically</strong>, the entire previous section is <strong>wrong in notation</strong>.
I’ve used $\mathbf{x}$ to denote the feature vector, but the method proposed is for a <strong>single feature</strong>.</p>
<p>If we have multiple features, the joint probability $p(\mathbf{x} | y = c)$ can’t be modeled as a single Gaussian distribution.
So let’s make a <strong>naive assumption</strong> that the features are <strong>statistically independent</strong> given the class label, e.g., in 2D,</p>
<p>$$
\begin{equation}
p(x_1, x_2 | y = c) = p(x_1 | y = c) p(x_2 | y = c)
\end{equation}
$$</p>
<p>The general form for classification is,</p>
<p>$$
\begin{equation}
f_{NB}(\mathbf{x}) = \underset{c}{\arg\max} \ p(y = c) \prod_{j=1}^{N} p(x_j | y = c),
\end{equation}
$$</p>
<p>where $N$ is the number of features.</p>
<p><strong>I will consider the 2D case for visualization and building your intuition</strong>, but everything we discuss can be generalized to $N$ dimensions.</p>
<p>The only difference from the previous section is that we now have parameters $(\mu_{j | c}, \sigma_{j | c}^2)$ instead of $(\mu_{c}, \sigma_{c}^2)$.</p>
<p><strong>TODO</strong></p>
<h6 id="linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)</h6>
<p>So far we have only considered <strong>univariate Gaussian distributions</strong>.
Why not extend this to <strong>multivariate Gaussian distributions</strong>?</p>
<p>However, LDA makes one assumption, that the <strong>covariance matrix is the same for all classes</strong>,</p>
<p>$$
\begin{equation}
p(\mathbf{x} | y = c) = \frac{1}{|(2\pi)^N \Sigma|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x} - \mu_c)^T \Sigma^{-1} (\mathbf{x} - \mu_c) \right),
\end{equation}
$$</p>
<p>where $\Sigma$ is the covariance matrix.</p>
<p>As with (Naive) Bayes Classifier, these parameters are learned from the data using MLE.</p>
<p>For now, let’s focus on class conditional densities $p(\mathbf{x} | y = c)$.
Let’s look at the log-likelihood function.</p>
<p>$$
\begin{aligned}
(\hat{\mu_c}, \hat{\Sigma}) &#x26; = \underset{\mu_c, \Sigma}{\arg\max} \log \left[\prod \frac{1}{(2\pi)^{N/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right) \right] \newline
&#x26; = \underset{\mu_c, \Sigma}{\arg\max} \sum \log \left[ \frac{1}{(2\pi)^{N/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right) \right] \newline
&#x26; = \underset{\mu_c, \Sigma}{\arg\max} \sum \left[ \log \left( \frac{1}{(2\pi)^{N/2} |\Sigma|^{1/2}} \right) + \log \left( \exp \left( -\frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right) \right) \right] \ \biggr\rvert \log(ab) = \log(a) + \log(b) \newline
&#x26; = \underset{\mu_c, \Sigma}{\arg\max} \sum \left[ \underbrace{\log(1)}_0 - \log \left( (2\pi)^{N/2} |\Sigma|^{1/2} \right) + \log \left( \exp \left( -\frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right) \right) \right] \ \biggr\rvert \log(a/b) = \log(a) - \log(b) \newline
&#x26; = \underset{\mu_c, \Sigma}{\arg\max} \sum \left[ -\log \left( (2\pi)^{N/2} |\Sigma|^{1/2} \right) - \frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right] \ \biggr\rvert \log(e^a) = a \newline
&#x26; = \underset{\mu_c, \Sigma}{\arg\max} \sum \left[ -\left(\log\left((2\pi)^{N/2}\right) + \log\left(|\Sigma|^{1/2}\right) \right) - \frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right] \ \biggr\rvert \log(ab) = \log(a) + \log(b) \newline
&#x26; = \underset{\mu_c, \Sigma}{\arg\max} \sum \left[ -\frac{N}{2} \log(2\pi) - \frac{1}{2} \log\left(|\Sigma|\right) - \frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right] \ \biggr\rvert \log(a^b) = b \log(a) \newline
\end{aligned}
$$</p>
<p>Then to find the MLE estimates, we differentiate the log-likelihood function with respect to $\mu_c$ and $\Sigma$ and set them to zero.</p>
<p>Let’s start with $\mu_c$.</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \mu_c} \sum \left[ -\frac{N}{2} \log(2\pi) - \frac{1}{2} \log\left(|\Sigma|\right) - \frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right] &#x26; = 0 \newline
\sum \frac{\partial}{\partial \mu_c} \left[ \underbrace{-\frac{N}{2} \log(2\pi)}_0 - \underbrace{\frac{1}{2} \log\left(|\Sigma|\right)}_0 - \frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right] &#x26; = 0 \newline
\sum \frac{\partial}{\partial \mu_c} \left[ -\frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right] &#x26; = 0.
\end{aligned}
$$</p>
<p>Now we need to use the identity $\frac{\partial}{\partial \mathbf{v}} \mathbf{v}^T A \mathbf{v} = 2 A \mathbf{v}$, by letting $\mathbf{v} = \mathbf{x}^{(i)} - \mu_c$ and using the chain rule — where the inner product is just $-\mathbf{I}$, we get,</p>
<p>$$
\begin{aligned}
\sum \frac{\partial}{\partial \mu_c} \left[ -\frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right] &#x26; = 0 \newline
\sum \frac{1}{2} 2 \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) &#x26; = 0 \newline
\sum \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) &#x26; = 0 \newline
\Sigma^{-1} \sum (\mathbf{x}^{(i)} - \mu_c) &#x26; = 0 \newline
\sum (\mathbf{x}^{(i)} - \mu_c) &#x26; = 0 \ \biggr\rvert \Sigma^{-1} \text{ is invertible} \newline
\sum \mathbf{x}^{(i)} - M_c \mu_c &#x26; = 0 \ \biggr\rvert \text{doesn’t depend on sum} \newline
\hat{\mu_c} &#x26; = \boxed{\frac{1}{M_c} \sum \mathbf{x}^{(i)}}.
\end{aligned}
$$</p>
<p>The mean MLE is the same as the MLE for the univariate Gaussian distribution.</p>
<p>Now for the covariance matrix $\Sigma$.</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \Sigma} \sum \left[ -\frac{N}{2} \log(2\pi) - \frac{1}{2} \log\left(|\Sigma|\right) - \frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right] &#x26; = 0 \newline
\sum \frac{\partial}{\partial \Sigma} \left[ \underbrace{-\frac{N}{2} \log(2\pi)}_0 - \frac{1}{2} \log\left(|\Sigma|\right) - \frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right] &#x26; = 0,
\end{aligned}
$$</p>
<p>From here we need to use two identities — $\frac{\partial}{\partial A} \log|A| = (A^{-1})^T$ and $\frac{\partial}{\partial A} \mathbf{a}^T A^{-1} \mathbf{b} = -A^{-T} \mathbf{a} \mathbf{b}^T A^{-T}$ — where the former holds if $A$ is symmetric.</p>
<p>Thus,
$$
\begin{aligned}
\sum \frac{\partial}{\partial \Sigma} \left[ -\frac{1}{2} \log\left(|\Sigma|\right) - \frac{1}{2} (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) \right] &#x26; = 0 \newline
\sum -\frac{1}{2} \Sigma^{-1} + \frac{1}{2} \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} &#x26; = 0 \newline
\sum -\Sigma^{-1} + \Sigma^{-1} (\mathbf{x}^{(i)} - \mu_c) (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} &#x26; = 0 \ \biggr\rvert \cdot 2 \newline
\sum \Sigma^{-1} \left(-\mathbf{I} + (\mathbf{x}^{(i)} - \mu_c) (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1}\right) &#x26; = 0 \ \biggr\rvert \textbf{left } \text{factorize } \Sigma^{-1} \newline
\Sigma^{-1} \sum \left(-\mathbf{I} + (\mathbf{x}^{(i)} - \mu_c) (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1}\right) &#x26; = 0 \ \biggr\rvert \text{doesn’t depend on sum} \newline
\sum \left(-\mathbf{I} + (\mathbf{x}^{(i)} - \mu_c) (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1}\right) &#x26; = 0 \ \biggr\rvert \Sigma^{-1} \text{ is invertible} \newline
-M_c \mathbf{I} + \sum (\mathbf{x}^{(i)} - \mu_c) (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} &#x26; = 0 \ \biggr\rvert \mathbf{I} \text{ doesn’t depend on sum} \newline
\sum (\mathbf{x}^{(i)} - \mu_c) (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} &#x26; = M_c \mathbf{I} \newline
\sum (\mathbf{x}^{(i)} - \mu_c) (\mathbf{x}^{(i)} - \mu_c)^T \Sigma^{-1} \Sigma &#x26; = M_c \mathbf{I} \Sigma \ \biggr\rvert \textbf{right } \text{multiply by } \Sigma \newline
\sum (\mathbf{x}^{(i)} - \mu_c) (\mathbf{x}^{(i)} - \mu_c)^T &#x26; = M_c \Sigma \newline
\Sigma &#x26; = \boxed{\frac{1}{M_c} \sum (\mathbf{x}^{(i)} - \mu_c) (\mathbf{x}^{(i)} - \mu_c)^T}.
\end{aligned}
$$</p>
<h5 id="summary-generative-models">Summary Generative Models</h5>
<p><strong>TODO</strong></p>
<p>I want to emphasize that we’ve taken a more <strong>frequentist approach (MLE)</strong> to the problem, but <strong>one can also take a Bayesian approach</strong> <sup><a href="#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup>.</p>
<h5 id="discriminative-models">Discriminative Models</h5>
<p>We’ve seen that generative models model the joint distribution $p(\mathbf{x}, y)$ — more specifically the class-conditional densities $p(\mathbf{x} | y)$ and the prior probabilities $p(y)$ — and then use Bayes’ rule to calculate the posterior probabilities $p(y | \mathbf{x})$.</p>
<p>But density estimation is hard and an ill-posed problem (to some extent), let’s take a <strong>discriminative</strong> approach instead, let’s solve for $p(y | \mathbf{x})$ directly.</p>
<h6 id="logistic-regression">Logistic Regression</h6>
<p>Let’s start with the simplest classifier, a <strong>binary linear classifier</strong> from a logistic regression perspective.</p>
<p>So we have the setup that our observation/feature vector $\mathbf{x} \in \mathbb{R}^N$, and we want to predict our class label $y \in \{-1, +1\}$.
<strong>Note</strong>, one can use a zero-indexed class label but using $-1$ and $+1$ yields a nice property that we will see later on.</p>
<p>Our <strong>goal</strong> is to have a linear function depends on $\mathbf{x}$ that,</p>
<p>$$
\begin{equation}
f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b = \sum_{j = 1}^N w_j x_j + b,
\end{equation}
$$</p>
<p>where $\mathbf{w} \in \mathbb{R}^N$ is the weight vector used to multiply each feature value and then sum.</p>
<p>From this we decide on a decision rule that is, <strong>if $f(\mathbf{x})$ then predict class $y = 1$</strong>, if $f(\mathbf{x}) &#x3C; 0$ then predict class $y = - 1$.
Equivalently, the decision rule is, $y = \text{sign}(f(\mathbf{x}))$.</p>
<p>This is why we have $-1$ and $+1$ as class labels.</p>
<p>Our linear classifier will separate the feature space into 2 half-spaces</p>
<figure><img alt="Linear Halfspace Example" loading="lazy" decoding="async" fetchpriority="auto" width="419" height="445" src="/_astro/linear_halfspace.CE___Smg_1hBxVw.svg" ><figcaption>Linear Halfspace Example</figcaption></figure>
<p>In a $N$-dimensional feature space $\mathbf{x} \in \mathbb{R}^N$ our parameters are $\mathbf{w} \in \mathbb{R}^N$.</p>
<p>Our equation — $\mathbf{w}^T \mathbf{x} + b = 0$ defines an $N - 1$-dimensional (linear) surface.
In general, we call this the <strong>hyperplane</strong>.</p>
<p>But how do we find the parameters $(\mathbf{w}, b)$?</p>
<p>Logistic regression takes a <strong>probabilistic</strong> approach to classification.
But there’s a problem, to take a probabilistic approach we need a function to map the value of $f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b$ to a probability value (between 0 and 1).</p>
<figure><img alt="Sigmoid function" loading="lazy" decoding="async" fetchpriority="auto" width="555" height="445" src="/_astro/sigmoid.DalRtMGm_2sHP4O.svg" ><figcaption>Sigmoid function</figcaption></figure>
<p>Luckily, the <strong>sigmoid function</strong> maps any real-value to $[0, 1]$. It is defined as,</p>
<p>$$
\begin{equation}
\sigma(z) = \frac{1}{1 + e^{-z}}, \quad z \in \mathbb{R}
\end{equation}
$$</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-600 dark:bg-pink-950/5" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-700 dark:text-pink-300" data-lucide="alert-circle" viewBox="0 0 24 24"><use href="#alert-circle"></use></svg>Problem: Exercise: What is the derivative of the sigmoid function?<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-700 dark:text-pink-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-300 dark:bg-pink-950/5">
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-500 dark:text-pink-200" data-lucide="check" viewBox="0 0 24 24"><use href="#check"></use></svg>Answer<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-500 dark:text-pink-200" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>$$
\begin{aligned}
\sigma(z) &#x26;= \frac{1}{1 + e^{-z}} \newline
\frac{d}{dz} \sigma(z) &#x26;= \frac{d}{dz} \left(1 + e^{-z}\right)^{-1} \newline
&#x26;= -1 \cdot \left(1 + e^{-z}\right)^{-2} \cdot -e^{-z} \biggr\rvert \text{ Chain rule} \newline
&#x26;= \frac{e^{-z}}{\left(1 + e^{-z}\right)^2} \newline
&#x26;= \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} \newline
&#x26;= \sigma(z) \cdot \left(1 - \sigma(z)\right)
\end{aligned}
$$</p></div>
</details></div>
</details>
<p>Thus, given a feature vector $\mathbf{x}$ the probability of the classes are,</p>
<p>$$
\begin{equation}
\begin{aligned}
p(y = +1 | \mathbf{x}) &#x26;= \sigma(f(\mathbf{x})) \newline
p(y = -1 | \mathbf{x}) &#x26;= 1 - \sigma(f(\mathbf{x})) = \sigma(-f(\mathbf{x}))
\end{aligned}
\end{equation}
$$</p>
<p>Equivalently, we can write this as,</p>
<p>$$
\begin{equation}
p(y | \mathbf{x}) = \sigma(yf(\mathbf{x}))
\end{equation}
$$</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-600 dark:bg-pink-950/5" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-700 dark:text-pink-300" data-lucide="alert-circle" viewBox="0 0 24 24"><use href="#alert-circle"></use></svg>Problem: Exercise: Can you prove that $1 - \sigma(z) = \sigma(-z)$?<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-700 dark:text-pink-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-300 dark:bg-pink-950/5">
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-500 dark:text-pink-200" data-lucide="check" viewBox="0 0 24 24"><use href="#check"></use></svg>Answer<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-500 dark:text-pink-200" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>$$
\begin{equation}
\begin{aligned}
1 - \sigma(z) &#x26;= 1 - \frac{1}{1 + e^{-z}} \ \biggr\rvert \cdot 1 + e^{-z} \newline
&#x26;= \frac{1 + e^{-z} - 1}{1 + e^{-z}} \newline
&#x26;= \frac{e^{-z}}{1 + e^{-z}} \ \biggr\rvert \cdot \frac{1}{e^{-z}} \newline
&#x26;= \frac{1}{1 + e^{z}} \newline
&#x26;= \sigma(-z)
\end{aligned}
\end{equation}
$$</p></div>
</details></div>
</details>
<p>Thus, we are <strong>directly modeling the class posterior probability</strong> $p(y | \mathbf{x})$.</p>
<p>Just like Naive Bayes and LDA we learn the parameters from the data.</p>
<p>By maximizing the (log) likelihood,</p>
<p>$$
\begin{equation}
\begin{aligned}
(\mathbf{w}^{\star}, b^{\star}) &#x26;= \underset{\mathbf{w}, b}{\arg\max} \prod_{i=1}^{N} p(y^{(i)} | \mathbf{x}^{(i)}; \mathbf{w}, b) \newline
&#x26; = \underset{\mathbf{w}, b}{\arg\max} \prod_{i=1}^{N} \sigma\left(y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b)\right) \newline
&#x26; = \underset{\mathbf{w}, b}{\arg\max} \sum_{i=1}^{N} \log \sigma\left(y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b)\right) \ \biggr\rvert \log(\cdot) \newline
\end{aligned}
\end{equation}
$$</p>
<p><strong>However</strong>, this does not have a closed-form solution like our previous models.
For now, we will leave it at this, but we will come back and see how we eventually can solve for these parameters.</p>
<h6 id="support-vector-machines-svms">Support Vector Machines (SVMs)</h6>
<p>So far we’ve only looked at <strong>probabilistic models</strong>, they all use the same probabilistic framework (MLE) to learn how to classify the data.</p>
<p>But when you think about it, a <strong>purely geometric approach</strong> to classification should be possible and easy to understand.
This is what the <strong>Support Vector Machine (SVM)</strong> does, a <strong>purely geometric approach</strong> to classification.</p>
<p>For now, let’s assume that our data is <strong>linearly separable</strong>.</p>
<figure><img alt="Linearly separable data" loading="lazy" decoding="async" fetchpriority="auto" width="550" height="463" src="/_astro/iris_linear_separation.QSTpwmxe_ZLrVBj.svg" ><figcaption>Linearly separable data</figcaption></figure>
<p>If our data is linearly separable, we will have many possible solutions for a hyperplane.
We need somekind of <strong>criterion</strong> to choose the best hyperplane.</p>
<h6 id="maximum-margin-principle">Maximum Margin Principle</h6>
<p>Let us define a naive criterion, the <strong>maximum margin principle</strong>.</p>
<figure><img alt="Maximum margin principle" loading="lazy" decoding="async" fetchpriority="auto" width="419" height="445" src="/_astro/margin_principle.CADAnE8L_1hBxVw.svg" ><figcaption>Maximum margin principle</figcaption></figure>
<p>We define the <strong>distance between the separating line and the closest point</strong> as the <strong>margin</strong>.
Intuitively, we think of this space as the amount of “wiggle room” for any potential errors in estimating $\mathbf{w}$.</p>
<p>Thus, we say that the <strong>best line is the one that maximizes the margin</strong> (i.e., has the most distance between the closest point(s) and the hyperplane).</p>
<p>By symmetry, there <strong>should be at least one margin point on each side of the hyperplane</strong> (assuming the data is linearly separable).</p>
<p>These points on the margin(s) are called the <strong>support vectors</strong> (these points define the margin).</p>
<h6 id="computing-the-margin">Computing the Margin</h6>
<p>How do we compute the margin from $\mathbf{w}^T \mathbf{x} + b = 0$ to a point $(\mathbf{x}^{(i)}, y^{(i)})$?</p>
<p>We compute the <strong>geometric distance</strong> from the point to the hyperplane,</p>
<p>$$
\begin{equation}
d^{(i)} = \frac{y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b)}{\Vert \mathbf{w} \Vert_2}
\end{equation}
$$</p>
<p>Then we simply compute this distance and take the minimum of all the distances,</p>
<p>$$
\begin{equation}
d = \min \{d^{(i)}\}_{i=1}^{M} = \underset{(\mathbf{x}, y) \in \mathcal{D}}{\min} \frac{y(\mathbf{w}^T \mathbf{x} + b)}{\Vert \mathbf{w} \Vert_2}
\end{equation}
$$</p>
<p>Thus, the maximum margin is found by solving,</p>
<p>$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{w}, b}{\max} \left(\underset{(\mathbf{x}, y) \in \mathcal{D}}{\min} \frac{y(\mathbf{w}^T \mathbf{x} + b)}{\Vert \mathbf{w} \Vert_2}\right) \newline
\underset{\mathbf{w}, b}{\max} \left(\frac{1}{\Vert \mathbf{w} \Vert_2} \underset{(\mathbf{x}, y) \in \mathcal{D}}{\min} y(\mathbf{w}^T \mathbf{x} + b)\right) \newline
\end{aligned}
\end{equation}
$$</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-600 dark:bg-pink-950/5" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-700 dark:text-pink-300" data-lucide="alert-circle" viewBox="0 0 24 24"><use href="#alert-circle"></use></svg>Problem: Exercise: If we rescale $(\mathbf{w}, b)$ does our objective function change?<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-700 dark:text-pink-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-300 dark:bg-pink-950/5">
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-500 dark:text-pink-200" data-lucide="check" viewBox="0 0 24 24"><use href="#check"></use></svg>Answer<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-500 dark:text-pink-200" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>No, even if we rescale $\mathbf{w} \to \gamma \mathbf{w}$ and $b \to \gamma b$ the objective function remains the same.</p></div>
</details></div>
</details>
<h6 id="why-is-maximize-the-margin-a-good-idea">Why is Maximize the Margin a Good Idea?</h6>
<p>Let’s take a step back and think about why maximizing the margin is a good idea.</p>
<p>Firstly, the <strong>true $\mathbf{w}$ is uncertain</strong> (we only have a finite amount of data), maximizing the margin allows the most uncertainty (wiggle room) for $\mathbf{w}$, while keeping all the points on the correct side of the hyperplane.
Also, the <strong>data is uncertain</strong>, maximizing the margin allows the most uncertainty (wiggle room) for the data, while keeping all the points on the correct side of the hyperplane.</p>
<p>Thus, maximizing the margin is a good idea because it allows the most uncertainty for both the data and the true $\mathbf{w}$.</p>
<h6 id="soft-margin">Soft-Margin</h6>
<p>So far we have assumed that the data is <strong>completely linearly separable</strong>, but this is rarely the case in practice.</p>
<figure><img alt="Soft-margin" loading="lazy" decoding="async" fetchpriority="auto" width="419" height="463" src="/_astro/soft_margin_principle.CF1Wufyo_Z1WdRMa.svg" ><figcaption>Soft-margin</figcaption></figure>
<p>Let’s now imagine that the data is <em>almost</em> linearly separable, but there are a few points that are on the wrong side of the hyperplane.</p>
<p>How should we proceed? We allow <em>some</em> samples to <strong>violate the margin</strong>.</p>
<p>We define the <strong>slack variable</strong> $\xi_i \geq 0$ for each sample $(\mathbf{x}^{(i)}, y^{(i)})$, where $\xi_i = 0$ means that our sample is outside of margin area (no slack) and $\xi_i > 0$ means our sample is inside the margin area (slack).</p>
<h6 id="convex-optimization">(Convex) Optimization</h6>
<p>Before we continue, we need to take a detour and talk about <strong>(convex) optimization</strong>.</p>
<p>A lot of machine learning problems can not be solved analytically and we need to rely on numerical optimization methods to solve them.</p>
<p>But what does this mean? Much of machine learning can be written as an optimization problem,</p>
<p>$$
\begin{equation}
\underset{\mathbf{\theta}}{\min} \ \ell(\mathcal{D}; \mathbf{\theta}) = \underset{\mathbf{\theta}}{\min} \frac{1}{M} \sum_{i = 1}^M \ell(\mathbf{x}^{(i)}, y^{(i)}; \mathbf{\theta})
\end{equation}
$$</p>
<p>where $\mathbf{\theta}$ is our parameter vector to be optimized, $\ell(\cdot)$ is our <strong>loss function</strong>, and $\mathcal{D}$ is our dataset.</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-600 dark:bg-pink-950/5" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-700 dark:text-pink-300" data-lucide="alert-circle" viewBox="0 0 24 24"><use href="#alert-circle"></use></svg>Problem: Exercise: What is the loss function for logistic regression?<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-700 dark:text-pink-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-300 dark:bg-pink-950/5">
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-500 dark:text-pink-200" data-lucide="check" viewBox="0 0 24 24"><use href="#check"></use></svg>Answer<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-500 dark:text-pink-200" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>$$
\begin{aligned}
\ell(\mathbf{x}^{(i)}, y^{(i)}; \mathbf{w}, b) &#x26; = \log \sigma(y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b)) \newline
&#x26; = \log \left(\frac{1}{1 + \exp(-y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b))}\right) \newline
&#x26; = \log(1) - \log(1 + \exp(-y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b))) \biggr\rvert \log(a/b) = \log(a) - \log(b) \newline
&#x26; = -\log(1 + \exp(-y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b))) \biggr\rvert \log(1) = 0 \newline
\end{aligned}
$$</p></div>
</details></div>
</details>
<p>When solving optimization problems, it is important to know which <em>type</em> of problem we have.</p>
<p><strong>Convex</strong> optimization is the easy case <strong>since we can guarantee to find the optimal solution</strong> (we will prove this later).
<strong>Non-convex</strong> optimization is the hard case <strong>since we can not guarantee to find the optimal solution</strong>.</p>
<p>Understanding (convex) optimization is a huge part of (modern) machine learning.
But I feel like there a lot of good sources on these … so I will give the brief introduction and explanation, if it is not sufficient find a good source and read about it.</p>
<p>To understand convex optimization, we first need to understand what a <strong>convex set</strong> is.</p>
<p>Recall that a <strong>line segment</strong> between two points $\mathbf{x}^{(1)}$ and $\mathbf{x}^{(2)}$ is defined as,</p>
<p>$$
\begin{equation}
\mathbf{x} = \alpha \mathbf{x}^{(1)} + (1 - \alpha) \mathbf{x}^{(2)}, \quad 0 \leq \alpha \leq 1.
\end{equation}
$$</p>
<p>A <strong>convex set</strong> is a set which contains all line segments between any two points in the set,</p>
<p>$$
\begin{equation}
\mathbf{x}^{(1)}, \mathbf{x}^{(2)} \in \chi, 0 \leq \alpha \leq 1 \Rightarrow \mathbf{x} = \alpha \mathbf{x}^{(1)} + (1 - \alpha) \mathbf{x}^{(2)} \in \chi,
\end{equation}
$$</p>
<p>where $\chi$ is our convex set.</p>
<figure><img alt="Convex set and line segment." loading="lazy" decoding="async" fetchpriority="auto" width="495" height="323" src="/_astro/convex_sets.Cb0o6xuL_Z1vzbms.svg" ><figcaption>Convex set and line segment.</figcaption></figure>
<p>A function, $f : \mathbb{R}^N \mapsto \mathbb{R}$ is <strong>convex</strong> if $\text{dom}(f)$ is a <em>convex set</em> <strong>and</strong>,</p>
<p>$$
\begin{equation}
f(\alpha \mathbf{x}^{(1)} + (1 - \alpha) \mathbf{x}^{(2)}) \leq \alpha f(\mathbf{x}^{(1)}) + (1 - \alpha) f(\mathbf{x}^{(2)})
\end{equation}
$$</p>
<figure><img alt="Convex function." loading="lazy" decoding="async" fetchpriority="auto" width="926" height="349" src="/_astro/convex_functions.KMf_aZ23_ZaHpp0.svg" ><figcaption>Convex function.</figcaption></figure>
<p>The most simple way one can understand convex functions is, <strong>any line segment between two points lies above the curve</strong>.</p>
<p>As we mentioned earlier, we can guarantee to find the optimal solution for convex optimization problems.
Let’s prove this now, I will use a proof of contradiction.</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-amber-500 dark:bg-amber-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-amber-700 dark:text-amber-300" data-lucide="check-square" viewBox="0 0 24 24"><use href="#check-square"></use></svg>Proof: Optimal Solution for Convex Optimization<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-amber-700 dark:text-amber-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Suppose $x$ is a local optimum and $y$ is a global optimum with $f(y) &#x3C; f(x)$.
The local optimum $x$ implies that there is a radius $R > 0$ such that,
$$
z \in \text{dom}(f), \Vert z - x \Vert_2 \leq R \Rightarrow f(x) \leq f(z).
$$
Now consider $z = \theta y + (1 - \theta)x$ with $\theta = \frac{R}{2 \Vert y - x \Vert_2}$.
First, we note that $\Vert y - x \Vert_2 > R$, due to our assumptions.
Therefore, we have $0 \leq \theta \leq \frac{1}{2}$, which implies that $z$ is a convex combination of $x$ and $y$ and is in the domain of $f$.
Note also that,
$$
\Vert z - x \Vert_2 = \Vert \theta y + (1 - \theta)x - x \Vert_2 = \Vert \theta (y - x) \Vert_2 = \theta \Vert y - x \Vert_2 = \frac{R}{2},
$$
which implies that $f(x) \leq f(z)$.
As $f$ is convex, we have,
$$
f(z) = f(\theta y + (1 - \theta)x) \leq \theta f(y) + (1 - \theta) f(x) &#x3C; \theta f(x) + (1 - \theta) f(x) = f(x),
$$
which is a contradiction, thus any local optimum must be a global optimum.</p></div>
</details>
<h6 id="standard-form-of-convex-optimization">Standard Form of Convex Optimization</h6>
<p>Convex optimization problems can be written in the following standard form,</p>
<p>$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{x}}{\min} &#x26; \quad f_0(\mathbf{x}) \newline
\text{subject to} &#x26; \quad f_i(\mathbf{x}) \leq 0, \quad i = 1, \ldots, r \newline
&#x26; \quad h_i(\mathbf{x}) = 0, \quad i = 1, \ldots, s
\end{aligned}
\end{equation}
$$</p>
<p>The optimal value is,</p>
<p>$$
\begin{equation}
p^{\star} = \min\{f_0(\mathbf{x}) | f_i(\mathbf{x}) \leq 0, i = 1, \ldots, r, h_i(\mathbf{x}) = 0, i = 1, \ldots,s \}
\end{equation}
$$</p>
<h6 id="duality-and-kkt-conditions">Duality and KKT Conditions</h6>
<p>But, how do we solve these optimization problems?</p>
<p>Let’s first introduce the <strong>Lagrangian</strong> <sup><a href="#user-content-fn-8" id="user-content-fnref-8" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup>.</p>
<p>We have our standard form optimization problem (not necessarily convex),</p>
<p>$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{x}}{\min} &#x26; \quad f_0(\mathbf{x}) \newline
\text{subject to} &#x26; \quad f_i(\mathbf{x}) \leq 0, \quad i = 1, \ldots, r \newline
&#x26; \quad h_i(\mathbf{x}) = 0, \quad i = 1, \ldots, s
\end{aligned}
\end{equation}
$$</p>
<p>With our variable $\mathbf{x} \in \mathbb{R}^N$ and its domain $\chi$ and the optimal value $p^{\star}$.</p>
<p>The <strong>Lagrangian</strong> is defined as,</p>
<p>$$
\begin{equation}
L(\mathbf{x}, \mathbf{\lambda}, \mathbf{\nu}) = f_0(\mathbf{x}) + \sum_{i=1}^{r} \lambda_i f_i(\mathbf{x}) + \sum_{i=1}^{s} \nu_i h_i(\mathbf{x}).
\end{equation}
$$</p>
<p>The domain $\text{dom}(L) = \chi \times \mathbb{R}^r \times \mathbb{R}^s$.
The lagrangian is a weighted sum of the objective function and the constraints.
The coefficients $\lambda_i$ and $\nu_i$ are called the <strong>Lagrange multipliers</strong>.</p>
<p>The <strong>lagrangian dual function</strong> $g : \mathbb{R}^r \times \mathbb{R}^s \mapsto \mathbb{R}$ is defined as,</p>
<p>$$
\begin{equation}
g(\mathbf{\lambda}, \mathbf{\nu}) = \underset{\mathbf{x} \in \chi}{\inf} \ L(\mathbf{x}, \mathbf{\lambda}, \mathbf{\nu}).
\end{equation}
$$</p>
<p>With this we can prove the <strong>lower bound property</strong>,</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-amber-500 dark:bg-amber-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-amber-700 dark:text-amber-300" data-lucide="check-square" viewBox="0 0 24 24"><use href="#check-square"></use></svg>Proof: Lower Bound Property<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-amber-700 dark:text-amber-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>If $\mathbf{\lambda} \geq 0$, then $g(\mathbf{\lambda}, \mathbf{\nu}) \leq p^{\star}$.
<strong>Proof</strong>: If $\tilde{x}$ is feasible and $\mathbf{\lambda} \geq 0$, then,
$$
f_0(\tilde{x}) \geq L(\tilde{x}, \mathbf{\lambda}, \mathbf{\nu}) \geq \underset{\mathbf{x} \in \chi}{\inf} L(\mathbf{x}, \mathbf{\lambda}, \mathbf{\nu}) = g(\mathbf{\lambda}, \mathbf{\nu}).
$$
Minimizing over all feasible $\tilde{x}$ gives $p^{\star} \geq g(\mathbf{\lambda}, \mathbf{\nu})$.</p></div>
</details>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-600 dark:bg-pink-950/5" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-700 dark:text-pink-300" data-lucide="alert-circle" viewBox="0 0 24 24"><use href="#alert-circle"></use></svg>Problem: Exercise: What is the lower-bound for the following optimization problem?<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-700 dark:text-pink-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{x}}{\min} &#x26; \quad \mathbf{x}^T \mathbf{x} \newline
\text{subject to} &#x26; \quad \mathbf{A} \mathbf{x} = \mathbf{b}
\end{aligned}
\end{equation}
$$</p><details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-300 dark:bg-pink-950/5">
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-500 dark:text-pink-200" data-lucide="check" viewBox="0 0 24 24"><use href="#check"></use></svg>Answer<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-500 dark:text-pink-200" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Firstly, rewrite it to standard form,
$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{x}}{\min} &#x26; \quad \mathbf{x}^T \mathbf{x} \newline
\text{subject to} &#x26; \quad \mathbf{A} \mathbf{x} - \mathbf{b} = 0
\end{aligned}
\end{equation}
$$
Formulate the Lagrangian $L(\mathbf{x}, \mathbf{\nu}) = \mathbf{x}^T \mathbf{x} + \mathbf{\nu}^T (\mathbf{A} \mathbf{x} - \mathbf{b})$
We take gradient of the Lagrangian and set it equal to zero to, $\nabla_{\mathbf{x}} L(\mathbf{x}, \mathbf{\nu}) = 2 \mathbf{x} + \mathbf{A}^T \mathbf{\nu} = 0$.
Solve for $\mathbf{x}$ to get $\mathbf{x} = -\frac{1}{2} \mathbf{A}^T \mathbf{\nu}$.
Substitute this into the dual function to get $g(\mathbf{\nu}) = -\frac{1}{4} \mathbf{\nu}^T \mathbf{A} \mathbf{A}^T \mathbf{\nu} - \mathbf{b}^T \mathbf{\nu}$.
Thus, this is the lower bound for all $\mathbf{\nu}$.</p></div>
</details></div>
</details>
<p>Notice how our original problem — which we will call the <strong>primal problem</strong> — is a minimization problem, while the dual problem is a maximization problem,</p>
<p>$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{\lambda}, \mathbf{\nu}}{\max} &#x26; \quad g(\mathbf{\lambda}, \mathbf{\nu}) \newline
\text{subject to} &#x26; \quad \mathbf{\lambda} \succeq 0
\end{aligned}
\end{equation}
$$</p>
<p>$\mathbf{\lambda}, \mathbf{\nu}$ are (dual) feasible if $\mathbf{\lambda} \succeq 0$ (here $\succeq 0$ means that every $\lambda_i \geq 0$) and $(\mathbf{\lambda}, \mathbf{\nu}) \in \text{dom}(g)$.</p>
<p><strong>The dual problem is always a convex optimization problem, and the optimal value of the dual problem is always a lower bound on the optimal value of the primal problem</strong>.
Yes everything above is in bold because it is important. Really take time to understand why this is important (and powerful).</p>
<p>We denote the optimal value of the dual problem as $d^{\star}$.
We say that we have <strong>weak duality</strong> if $d^{\star} \leq p^{\star}$. This <strong>always holds</strong>, both for convex and non-convex problems.</p>
<p>The interesting case is <strong>strong duality</strong>, or $d^{\star} = p^{\star}$, this usually does not hold in general but <em>usually</em> holds for convex problems (e.g., SVMs, which we will derive later).</p>
<p>Assume that strong duality holds such that $\mathbf{x}^{\star}$ is primal optimal and $(\mathbf{\lambda}^{\star}, \mathbf{\nu}^{\star})$ is dual optimal, then,</p>
<p>$$
\begin{equation}
\begin{aligned}
f(\mathbf{x}^{\star}) = g(\mathbf{\lambda}^{\star}, \mathbf{\nu}^{\star}) &#x26; = \underset{\mathbf{x} \in \chi}{\inf} \left(f_0(\mathbf{x}) + \sum_{i=1}^{r} \lambda_i^{\star} f_i(\mathbf{x}) + \sum_{i=1}^{s} \nu_i^{\star} h_i(\mathbf{x})\right) \newline
&#x26; \leq f_0(\mathbf{x}^{\star}) + \sum_{i=1}^{r} \lambda_i^{\star} f_i(\mathbf{x}^{\star}) + \sum_{i=1}^{s} \nu_i^{\star} h_i(\mathbf{x}^{\star}) \newline
\end{aligned}
\end{equation}
$$</p>
<p>If we let $\lambda_i^{\star} f_i(\mathbf{x}^{\star}) = 0$ for $i = 1, \ldots, r$ — which we will call <strong>complementary slackness</strong>) — then we have,</p>
<p>$$
\begin{equation}
\begin{aligned}
f(\mathbf{x}^{\star}) = g(\mathbf{\lambda}^{\star}, \mathbf{\nu}^{\star}) &#x26; = \underset{\mathbf{x} \in \chi}{\inf} \left(f_0(\mathbf{x}) + \sum_{i=1}^{r} \lambda_i^{\star} f_i(\mathbf{x}) + \sum_{i=1}^{s} \nu_i^{\star} h_i(\mathbf{x})\right) \newline
&#x26; \leq f_0(\mathbf{x}^{\star}) + \sum_{i=1}^{r} \lambda_i^{\star} f_i(\mathbf{x}^{\star}) + \sum_{i=1}^{s} \nu_i^{\star} h_i(\mathbf{x}^{\star}) \newline
&#x26; \leq f_0(\mathbf{x}^{\star})
\end{aligned}
\end{equation}
$$</p>
<p><strong>The two inequalities hold with equality</strong>. <strong>$\mathbf{x}^{\star}$ not only minimizes $f_0(\mathbf{x})$, but also minimizes $L(\mathbf{x}, \mathbf{\lambda}^{\star}, \mathbf{\nu}^{\star})$.</strong></p>
<p>We’ve <strong>finally</strong> covered the four KKT (Karush-Kuhn-Tucker) conditions!</p>
<ol>
<li><strong>Primal constraints</strong>: $f_i(\mathbf{x}^{\star}) \leq 0$ for $i = 1, \ldots, r, h_i(\mathbf{x}^{\star}) = 0$ for $i = 1, \ldots, s$.</li>
<li><strong>Dual constraints</strong>: $\mathbf{\lambda}^{\star} \succeq 0$.</li>
<li><strong>Complementary slackness</strong>: $\lambda_i^{\star} f_i(\mathbf{x}^{\star}) = 0$ for $i = 1, \ldots, r$.</li>
<li><strong>Gradient of Lagrangian with respect to $\mathbf{x}$ vanishes</strong>,
$$
\begin{equation}
\nabla_{\mathbf{x}} L(\mathbf{x}^{\star}, \mathbf{\lambda}^{\star}, \mathbf{\nu}^{\star}) = 0
\end{equation}
$$</li>
</ol>
<p>If strong duality holds and $\mathbf{x}^{\star}$ are optimal, then they must satisfy the KKT conditions.</p>
<h6 id="svms-continued">SVMs Continued</h6>
<p>Let’s now write our hard-margin SVM as an optimization problem,</p>
<p>$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{w}, b}{\min} &#x26; \quad \frac{1}{2} \Vert \mathbf{w} \Vert_2^2 \newline
\text{subject to} &#x26; \quad y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq 1, \quad i = 1, \ldots, M
\end{aligned}
\end{equation}
$$</p>
<p><strong>Note</strong> that our constraint is from the fact that rescaling $(\mathbf{w}, b)$ does not change the objective function.
Therefore, we set the margin to be $1$,</p>
<p>$$
\begin{equation}
\underset{(\mathbf{x}, y) \in \mathcal{D}}{\min} y(\mathbf{w}^T \mathbf{x} + b) = 1
\end{equation}
$$</p>
<p>For the soft-margin SVM, we introduced the slack variable $\xi_i \geq 0$ for each sample $(\mathbf{x}^{(i)}, y^{(i)})$,</p>
<p>$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{w}, b}{\min} &#x26; \quad \frac{1}{2} \Vert \mathbf{w} \Vert_2^2 + C \sum_{i=1}^{M} \xi_i \newline
\text{subject to} &#x26; \quad y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq 1 - \xi_i, \quad i = 1, \ldots, M \newline
&#x26; \quad \xi_i \geq 0, \quad i = 1, \ldots, M
\end{aligned}
\end{equation}
$$</p>
<p>We also introduce a (hyper)parameter $C$ as a penalty for violating the margin (too much slack).
Now that we have both the hard-margin and soft-margin SVMs, let’s derive their respective dual problems.</p>
<h6 id="deriving-the-dual-problem">Deriving the Dual Problem</h6>
<p>Let’s now derive the dual problem for SVMs, since they are so alike, I will derive both “at the same time”.
The unique part(s) of the soft-margin SVM will be highlighted in $\colorbox{purple}{\text{purple}}$.</p>
<p>We have the following optimization problem,</p>
<p>$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{w}, b}{\min} &#x26; \quad \frac{1}{2} \Vert \mathbf{w} \Vert_2^2 + \colorbox{purple}{$C \sum_{i=1}^{M} \xi_i$} \newline
\text{subject to} &#x26; \quad y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq 1 \colorbox{purple}{$- \xi_i$}, \quad i = 1, \ldots, M \newline
&#x26; \quad \colorbox{purple}{$\xi_i \geq 0 \quad i = 1, \ldots, M$}
\end{aligned}
\end{equation}
$$</p>
<p>Firstly, we write the problem into standard form,</p>
<p>$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{w}, b}{\min} &#x26; \quad \frac{1}{2} \Vert \mathbf{w} \Vert_2^2 + \colorbox{purple}{$C \sum_{i=1}^{M} \xi_i$} \newline
\text{subject to} &#x26; \quad 1 - y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b) \colorbox{purple}{$+ \xi_i$} \leq 0, \quad i = 1, \ldots, M \newline
&#x26; \quad \colorbox{purple}{$-\xi_i \leq 0 \quad i = 1, \ldots, M$}
\end{aligned}
\end{equation}
$$</p>
<p>Then, we form the Lagrangian,</p>
<p>$$
\begin{equation}
\begin{aligned}
L(\mathbf{w}, b, \mathbf{\lambda}, \colorbox{purple}{$\mathbf{\mu}$}) &#x26; = \frac{1}{2} \Vert \mathbf{w} \Vert_2^2 + \colorbox{purple}{$C \sum_{i=1}^{M} \xi_i$} + \sum_{i = 1}^M \lambda_i (1 - y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b) \colorbox{purple}{$+ \xi_i$}) - \colorbox{purple}{$\sum_{i=1}^{M} \mu_i \xi_i$} \newline
&#x26; = \frac{1}{2} \Vert \mathbf{w} \Vert_2^2 + \sum_{i = 1}^M \lambda_i - \lambda_i y^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) + \colorbox{purple}{$\sum_{i=1}^{M} \xi_i (C - \lambda_i - \mu_i)$} \newline
&#x26; = \frac{1}{2} \Vert \mathbf{w} \Vert_2^2 + \sum_{i = 1}^M \lambda_i - \lambda_i y^{(i)} \mathbf{w}^T \mathbf{x}^{(i)} + \lambda_i y^{(i)} b + \colorbox{purple}{$\sum_{i=1}^{M} \xi_i (C - \lambda_i - \mu_i)$} \newline
&#x26; = \frac{1}{2} \Vert \mathbf{w} \Vert_2^2 - \sum_{i = 1}^M \lambda_i y^{(i)} \mathbf{w}^T \mathbf{x}^{(i)} + \sum_{i = 1}^M \lambda_i y^{(i)} b + \sum_{i = 1}^M \lambda_i + \colorbox{purple}{$\sum_{i=1}^{M} \xi_i (C - \lambda_i - \mu_i)$}.
\end{aligned}
\end{equation}
$$</p>
<p>The dual function is defined as,</p>
<p>$$
g(\mathbf{\lambda}, \colorbox{purple}{$\mathbf{\mu}$}) = \underset{\mathbf{w}, b}{\max} \ L(\mathbf{w}, b, \mathbf{\lambda}, \colorbox{purple}{$\mathbf{\mu}$)}.
$$</p>
<p>To maximize $L$ with respect to $\mathbf{w}$, $b$ $\colorbox{purple}{$\xi_i$}$, we take the partial derivative of $\mathbf{w}$, $b$ and $\colorbox{purple}{$\xi_i$}$ of $L$ and set the expression to zero, respectively.</p>
<p>$$
\begin{equation}
\begin{aligned}
\frac{\partial L(\mathbf{w}, b, \mathbf{\lambda}, \colorbox{purple}{$\xi_i$})}{\partial \mathbf{w}} &#x26; = \frac{\partial}{\partial \mathbf{w}} \left[\frac{1}{2} \Vert \mathbf{w} \Vert_2^2 - \sum_{i = 1}^M \lambda_i y^{(i)} \mathbf{w}^T \mathbf{x}^{(i)} + \sum_{i = 1}^M \lambda_i y^{(i)} b \sum_{i = 1}^M \lambda_i + \colorbox{purple}{$\sum_{i=1}^{M} \xi_i (C - \lambda_i - \mu_i)$} \right] \newline
&#x26; = \mathbf{w} - \sum_{i = 1}^M \lambda_i y^{(i)} \mathbf{x}^{(i)} = 0.
\end{aligned}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\begin{aligned}
\frac{\partial L(\mathbf{w}, b, \mathbf{\lambda}, \colorbox{purple}{$\xi_i$})}{\partial b} &#x26; = \frac{\partial}{\partial b} \left[\frac{1}{2} \Vert \mathbf{w} \Vert_2^2 - \sum_{i = 1}^M \lambda_i y^{(i)} \mathbf{w}^T \mathbf{x}^{(i)} + \sum_{i = 1}^M \lambda_i y^{(i)} b \sum_{i = 1}^M \lambda_i + \colorbox{purple}{$\sum_{i=1}^{M} \xi_i (C - \lambda_i - \mu_i)$} \right] \newline
&#x26; = \sum_{i = 1}^M \lambda_i y^{(i)} = 0.
\end{aligned}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\begin{aligned}
\frac{\partial L(\mathbf{w}, b, \mathbf{\lambda}, \colorbox{purple}{$\xi_i$})}{\colorbox{purple}{$\partial \xi_i$}} &#x26; = \frac{\partial}{\colorbox{purple}{$\partial \xi_i$}} \left[\frac{1}{2} \Vert \mathbf{w} \Vert_2^2 - \sum_{i = 1}^M \lambda_i y^{(i)} \mathbf{w}^T \mathbf{x}^{(i)} + \sum_{i = 1}^M \lambda_i y^{(i)} b \sum_{i = 1}^M \lambda_i + \colorbox{purple}{$\sum_{i=1}^{M} \xi_i (C - \lambda_i - \mu_i)$} \right] \newline
&#x26; = \colorbox{purple}{$C - \lambda_i - \mu_i = 0$}.
\end{aligned}
\end{equation}
$$</p>
<p>Thus, we can solve for $\mathbf{w}$ from the first equation and use the other two as constraints.</p>
<p>$$
\mathbf{w} = \sum_{i = 1}^M \lambda_i y^{(i)} \mathbf{x}^{(i)}.
$$</p>
<p>If we plug back these into our original Lagrangian,</p>
<p>$$
\begin{aligned}
L(\mathbf{w}, b, \mathbf{\lambda}, \colorbox{purple}{$\xi_i$}) &#x26; = \frac{1}{2} \left( \sum_{i = 1}^M \lambda_i y^{(i)} \mathbf{x}^{(i)} \right)^T \left( \sum_{j = 1}^M \lambda_j y^{(j)} \mathbf{x}^{(j)} \right) - \sum_{i = 1}^M \lambda_i y^{(i)} \left( \sum_{j = 1}^M \lambda_j y^{(j)} \mathbf{x}^{(j)} \right) ^T \mathbf{x}^{(i)} + \sum_{i = 1}^M \lambda_i y^{(i)} b + \sum_{i = 1}^M \lambda_i + \colorbox{purple}{$\sum_{i=1}^{M} \xi_i (C - \lambda_i - \mu_i)$}\newline
&#x26; = \sum_{i = 1}^M \lambda_i + \frac{1}{2} \sum_{i = 1}^M \sum_{j = 1}^M y^{(i)} y^{(j)} \lambda_i \lambda_j (\mathbf{x}^{(i)})^T  \mathbf{x}^{(j)} - \sum_{i = 1}^M \sum_{j = 1}^M y^{(i)} y^{(j)} \lambda_i \lambda_j (\mathbf{x}^{(i)})^T \mathbf{x}^{(j)} \newline
&#x26; = \sum_{i = 1}^M \lambda_i - \frac{1}{2} \sum_{i = 1}^M \sum_{j = 1}^M y^{(i)} y^{(j)} \lambda_i \lambda_j (\mathbf{x}^{(i)})^T  \mathbf{x}^{(j)}
\end{aligned}
$$</p>
<p><strong>Note</strong> that I have rewritten $\Vert \mathbf{w} \Vert_{2}^2$ as $\mathbf{w}^T \mathbf{w}$.</p>
<p>So, our dual function is,
$$
g(\mathbf{\lambda}) = \sum_{i = 1}^M \lambda_i - \frac{1}{2} \sum_{i = 1}^M \sum_{j = 1}^M y^{(i)} y^{(j)} \lambda_i \lambda_j (\mathbf{x}^{(i)})^T  \mathbf{x}^{(j)}.
$$</p>
<p>Combining this and our constraints, we get the following dual optimization problem,</p>
<p>$$
\begin{equation}
\begin{aligned}
\underset{\lambda}{\max} &#x26; \quad \sum_{i = 1}^M \lambda_i - \frac{1}{2} \sum_{i = 1}^M \sum_{j = 1}^M y^{(i)} y^{(j)} \lambda_i \lambda_j (\mathbf{x}^{(i)})^T  \mathbf{x}^{(j)}. \newline
\text{subject to} &#x26; \quad \sum_{i = 1}^M \lambda_i y^{(i)} = 0 \newline
&#x26; \quad \colorbox{purple}{$C - \lambda_i - \mu_i = 0$} \quad i = 1, \ldots, M \newline
&#x26; \quad \lambda_i \geq 0, \quad i = 1, \ldots, M \newline
&#x26; \quad \colorbox{purple}{$\mu_i \geq 0$} \quad i = 1, \ldots, M
\end{aligned}
\end{equation}
$$</p>
<p>Which we can easily rewrite as,
$$
\begin{equation}
\begin{aligned}
\underset{\lambda}{\max} &#x26; \quad \sum_{i = 1}^M \lambda_i - \frac{1}{2} \sum_{i = 1}^M \sum_{j = 1}^M y^{(i)} y^{(j)} \lambda_i \lambda_j (\mathbf{x}^{(i)})^T  \mathbf{x}^{(j)}. \newline
\text{subject to} &#x26; \quad \sum_{i = 1}^M \lambda_i y^{(i)} = 0 \newline
&#x26; \quad 0 \leq \lambda_i \colorbox{purple}{$\leq C$} \quad i = 1, \ldots, M
\end{aligned}
\end{equation}
$$</p>
<p>We can see that the only difference lies in the constraint on $\lambda_i$.</p>
<p>In the <strong>hard margin case</strong>, we <strong>do not have an upper-bound</strong>, this is due to the $\xi_i$ term (slackness term), we get the upper-bound $C$.
With hard margins, we assume the data is perfectly separable, i.e., no mistakes in classification.</p>
<p>We can think of the <strong>hard margin case</strong> as a <strong>special case of the soft margin</strong>, if we let $C \to \infty$ we get the hard margin case.</p>
<p>However, I lied earlier, SVMs do not have a <strong>truly closed form solution</strong>, only a quadratic programming/optimization problem <sup><a href="#user-content-fn-10" id="user-content-fnref-10" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup>.
These optimization problems are <strong>well studied and methods to solve them exists</strong> but beyond our scope [^11].</p>
<h6 id="gradient-descent-and-variants">Gradient Descent and variants</h6>
<p>But there is another way to find the best parameters, that we also can use for logistic regression.</p>
<p><strong>Gradient descent</strong> is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient,</p>
<p>$$
\begin{equation}
\mathbf{\theta} = \mathbf{\theta} - \alpha \nabla \ell(\mathbf{\theta}),
\end{equation}
$$</p>
<p>where $\alpha$ is the <strong>learning rate</strong> and $\nabla \ell(\mathbf{\theta})$ is the gradient of the loss function evaluated at $\mathbf{\theta}$.</p>
<!-- **Figure 11:** Gradient descent. -->
<p>Gradient descent picks an initial point $\mathbf{\theta}^{(0)}$ and iteratively performs,</p>
<p>$$
\begin{equation}
\begin{aligned}
\mathbf{\theta}^{(t+1)} &#x26; = \mathbf{\theta}^{(t)} - \alpha \nabla \ell(\mathcal{D}; \mathbf{\theta}^{(t)}) \newline
&#x26; = \mathbf{\theta}^{(t)} - \alpha \frac{1}{M} \sum_{i=1}^{M} \nabla \ell(\mathbf{x}^{(i)}, y^{(i)}; \mathbf{\theta}^{(t)}).
\end{aligned}
\end{equation}
$$</p>
<p>But when do we stop?</p>
<p>We can stop when $\Vert \mathbf{\theta}^{(t+1)} - \mathbf{\theta}^{(t)} \Vert_2 \leq \epsilon$ for some small $\epsilon$.
Intuitively, this means that we are not moving much anymore, i.e., we are close to (some) optimum.</p>
<p>Similarly, we can stop if $\Vert \nabla \ell(\mathcal{D}; \mathbf{\theta}^{(t)}) \Vert_2 \leq \epsilon$.
Again, this can intuitively be interpreted as we are not moving much anymore.</p>
<!-- **Figure 12:** Gradient descent example. -->
<p>Our initial point matters a lot, as viewed in <strong>Figure 12</strong>.
Therefore, we can run gradient descent multiple times with different initial points and pick the best one.</p>
<!-- **Figure 13:** Gradient descent example. -->
<p>Our <em>learning rate</em> matters too of course, if we choose a too big $\alpha$, gradient descent might move too quickly and risk never converging.
If we choose a too small $\alpha$, gradient descent might take too long to converge.</p>
<p>However, there is a drawback to gradient descent, <strong>the computational cost</strong>.
For each iteration, we need to compute the gradient for all samples, this can be very expensive for large datasets.</p>
<p><strong>Stochastic gradient descent</strong> (SGD) is a variant of gradient descent that only uses a (random) single sample to compute the gradient.
At the $t$-th iteration, we randomly sample a single input-output pair and compute the gradient,</p>
<p>$$
\begin{equation}
\mathbf{\theta}^{(t+1)} = \mathbf{\theta}^{(t)} - \alpha \nabla \ell(\mathbf{x}^{(i)}, y^{(i)}; \mathbf{\theta}^{(t)}).
\end{equation}
$$</p>
<!-- **Figure 14:** Stochastic gradient descent VS. Gradient descent. -->
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-600 dark:bg-pink-950/5" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-700 dark:text-pink-300" data-lucide="alert-circle" viewBox="0 0 24 24"><use href="#alert-circle"></use></svg>Problem: Exercise: What is the expected value of SGD over all samples?<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-700 dark:text-pink-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-300 dark:bg-pink-950/5">
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-500 dark:text-pink-200" data-lucide="check" viewBox="0 0 24 24"><use href="#check"></use></svg>Answer<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-500 dark:text-pink-200" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>The expected value of SGD over all samples is the same as the gradient descent,
$$
\begin{equation}
\begin{aligned}
\mathbb{E}[\ell(\mathbf{x}, y; \mathbf{\theta}^{(t+1)})] &#x26; = \frac{1}{M} \sum_{i=1}^{M} \ell(\mathbf{x}^{(i)}, y^{(i)}; \mathbf{\theta}^{(t+1)}) \newline
&#x26; = \nabla \ell(\mathcal{D}; \mathbf{\theta}^{(t)}).
\end{aligned}
\end{equation}
$$</p></div>
</details></div>
</details>
<p><strong>Mini-batch gradient descent</strong> is a compromise between gradient descent and SGD.
At the $t$-th iteration, we randomly sample a mini-batch $\mathcal{B} \subset \mathcal{D}$ and compute the gradient,</p>
<p>$$
\begin{equation}
\begin{aligned}
\mathbf{\theta}^{(t+1)} &#x26; = \mathbf{\theta}^{(t)} - \alpha \nabla \ell(\mathcal{B}; \mathbf{\theta}^{(t)}) \newline
&#x26; = \mathbf{\theta}^{(t)} - \alpha \frac{1}{|\mathcal{B}|} \sum_{(\mathbf{x}^{(i)}, y^{(i)}) \in \mathcal{B}} \nabla \ell(\mathbf{x}^{(i)}, y^{(i)}; \mathbf{\theta}^{(t)}).
\end{aligned}
\end{equation}
$$</p>
<p>Let’s now derive the gradient descent update rule for logistic regression.
Remember that the loss function for logistic regression is,</p>
<p>$$
\begin{equation}
\ell(\mathbf{x}, y; \mathbf{w}, b) = -\log(1 + \exp(-y(\mathbf{w}^T \mathbf{x} + b))).
\end{equation}
$$</p>
<p>We can rewrite our MLE problem (which has no closed-form solution) as an optimization problem,</p>
<p>$$
\begin{equation}
\begin{aligned}
\underset{\mathbf{w}, b}{\max} &#x26; \quad \sum_{i=1}^{M} -\log(1 + \exp(-y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b))) \newline
\underset{\mathbf{w}, b}{\min} &#x26; \quad \sum_{i=1}^{M} \log(1 + \exp(-y^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)} + b))).
\end{aligned}
\end{equation}
$$</p>
<p>Simply using the negative sign to convert the maximization problem to a minimization problem.</p>
<p>Let’s see how the gradient behaves for a single training example $(\mathbf{x}, y)$.
The partial derivative of the loss function with respect to the $j$-th component $w_j$,</p>
<p>$$
\begin{aligned}
\frac{\partial \ell(\mathbf{x}, y; \mathbf{w}, b)}{\partial w_j} &#x26; = \frac{\partial}{\partial w_j} \left[-\log(\sigma(y(\mathbf{w}^T \mathbf{x} + b)))\right] \newline
&#x26; = -\frac{1}{\sigma(y(\mathbf{w}^T \mathbf{x} + b))} \frac{\partial}{\partial w_j} \sigma(y(\mathbf{w}^T \mathbf{x} + b)) \biggr\rvert \text{ Chain rule} \newline
&#x26; = -\frac{1}{\sigma(y(\mathbf{w}^T \mathbf{x} + b))} \sigma(y(\mathbf{w}^T \mathbf{x} + b)) (1 - \sigma(y(\mathbf{w}^T \mathbf{x} + b))) \frac{\partial}{\partial w_j} y(\mathbf{w}^T \mathbf{x} + b) \biggr\rvert \text{ Chain rule} \newline
&#x26; = (\sigma(y(\mathbf{w}^T \mathbf{x} + b)) - 1) \frac{\partial}{\partial w_j} y(\mathbf{w}^T \mathbf{x} + b) \newline
&#x26; = (\sigma(y(\mathbf{w}^T \mathbf{x} + b)) - 1) y \frac{\partial}{\partial w_j} (\mathbf{w}^T \mathbf{x} + b) \newline
&#x26; = (\sigma(y(\mathbf{w}^T \mathbf{x} + b)) - 1) y x_j.
\end{aligned}
$$</p>
<p>The partial derivative of the loss function with respect to $b$,</p>
<p>$$
\begin{aligned}
\frac{\partial \ell(\mathbf{x}, y; \mathbf{w}, b)}{\partial b} &#x26; = \frac{\partial}{\partial b} \left[-\log(\sigma(y(\mathbf{w}^T \mathbf{x} + b)))\right] \newline
&#x26; = -\frac{1}{\sigma(y(\mathbf{w}^T \mathbf{x} + b))} \frac{\partial}{\partial b} \sigma(y(\mathbf{w}^T \mathbf{x} + b)) \text{ Chain rule} \newline
&#x26; = -\frac{1}{\sigma(y(\mathbf{w}^T \mathbf{x} + b))} \sigma(y(\mathbf{w}^T \mathbf{x} + b)) (1 - \sigma(y(\mathbf{w}^T \mathbf{x} + b))) \frac{\partial}{\partial b} y(\mathbf{w}^T \mathbf{x} + b) \text{ Chain rule} \newline
&#x26; = (\sigma(y(\mathbf{w}^T \mathbf{x} + b)) - 1) y \frac{\partial}{\partial b} (\mathbf{w}^T \mathbf{x} + b) \newline
&#x26; = (\sigma(y(\mathbf{w}^T \mathbf{x} + b)) - 1) y.
\end{aligned}
$$</p>
<p>Thus, the (vectorized) gradient descent update rules for logistic regression are,</p>
<p>$$
\begin{equation}
\begin{aligned}
\mathbf{w}^{(t+1)} &#x26; = \mathbf{w}^{(t)} - \alpha \frac{1}{M} \sum_{i=1}^{M} (\sigma(y^{(i)}(\mathbf{w}^{(t)T} \mathbf{x}^{(i)} + b^{(t)}) - 1) y^{(i)} \mathbf{x}^{(i)} \newline
b^{(t+1)} &#x26; = b^{(t)} - \alpha \frac{1}{M} \sum_{i=1}^{M} (\sigma(y^{(i)}(\mathbf{w}^{(t)T} \mathbf{x}^{(i)} + b^{(t)}) - 1) y^{(i)}.
\end{aligned}
\end{equation}
$$</p>
<h6 id="k-nearest-neighbors">$k$-Nearest Neighbors</h6>
<p>Well, this detour to derive the dual problem for SVMs was quite long along with understanding gradient descent has taken quite a bit of time.
But let’s introduce another geometric approach to classification, $k$-Nearest Neighbors (KNN).</p>
<p>The idea is to store all the data $\mathcal{D} = {(\mathbf{x}^{(i)}, y^{(i)})}_{i=1}^{M}$ and when we want to classify a new point $\mathbf{x}$ we use a majority vote over its $k$-nearest neighbors $\mathcal{N}_k(\mathbf{x}) \subset \mathcal{D}$.
KNNs therefore require a distance function $d : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ and the number of neighbors $k$.</p>
<p>Therefore, the classification function is,</p>
<p>$$
\begin{equation}
f_{\text{KNN}}(\mathbf{x}) = \underset{y \in \mathcal{Y}}{\text{argmax}} \sum_{i \in \mathcal{N}_k(\mathbf{x})} \mathbb{I}[y^{(i)} = y],
\end{equation}
$$</p>
<p>where $\mathbb{I}$ is the indicator function.</p>
<p>(Brute force) KNNs work by computing the distance $d_i = d(\mathbf{x}^{(i)}, \mathbf{x})$ from the target point $\mathbf{x}$ to all other points in the dataset $\mathbf{x}^{(i)}$.
We then sort the distances $\{d_i, i = 1, \ldots, M\}$ and take the $k$-nearest neighbors to create the set $\mathcal{N}_k(\mathbf{x})$.</p>
<h4 id="the-regression-task">The Regression Task</h4>
<p>Okay, we are finally done with classification, but what if our target variable is <strong>continuous</strong>?</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-teal-600 dark:bg-teal-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-teal-700 dark:text-teal-400" data-lucide="book-open" viewBox="0 0 24 24"><use href="#book-open"></use></svg>Definition: Definition: The regression task<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-teal-700 dark:text-teal-400" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Given a feature vector $\mathbf{x} \in \mathbf{X} = \mathbb{R}^N$, predict its corresponding output value $y \in \mathbf{Y} = \mathbb{R}$.</p></div>
</details>
<p>Let’s take a moment to think about the difference between classification and regression.</p>
<p>In classification, we need to find a hyperplane that <strong>separates the classes (data) from each other</strong>.
While in regression we need to find a <strong>function that best fits the data</strong>.</p>
<p>While <em>curve fitting</em> is not the same as regression, it is a good analogy.
Given $M$ points from some underlying function (assuming it exists and has negligible noise) our goal is to find this function.</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-600 dark:bg-pink-950/5" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-700 dark:text-pink-300" data-lucide="alert-circle" viewBox="0 0 24 24"><use href="#alert-circle"></use></svg>Problem: Exercise: Here are four questions we need to answer to understand curve fitting (and by extension regression),<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-700 dark:text-pink-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><ol>
<li>Does a small subset of points work, or is the more data the better?</li>
<li>Do the locations of the points (on the curve) matter?</li>
<li>Given the same set of $M$ points, is the curve(s) unique?</li>
<li>If (3) is false, how do we measure the quality of the curve(s)?</li>
</ol><details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-300 dark:bg-pink-950/5">
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-500 dark:text-pink-200" data-lucide="check" viewBox="0 0 24 24"><use href="#check"></use></svg>Answer<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-500 dark:text-pink-200" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><ol>
<li>More data is better, as it gives us a better approximation of the underlying function.</li>
<li>Yes, the locations of the points matter, as they determine the shape of the curve, i.e., they need to be representative of the underlying function.</li>
<li>Estimating a continuous function from a finite set of points is an ill-posed problem and rarely has a unique solution.</li>
<li>We can measure the quality of the curve(s) by how well it fits the data, i.e., the error between the predicted and actual values.</li>
</ol></div>
</details></div>
</details>
<h4 id="the-regression-learning-problem">The Regression Learning Problem</h4>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-teal-600 dark:bg-teal-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-teal-700 dark:text-teal-400" data-lucide="book-open" viewBox="0 0 24 24"><use href="#book-open"></use></svg>Definition: Definition: The regression learning problem<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-teal-700 dark:text-teal-400" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Given a data set of example pairs $\mathcal{D} = \{(\mathbf{x}^{(i)}, y^{(i)}), i = 1, \ldots, M \}$ where $\mathbf{x}^{(i)} \in \mathbb{R}^{N}$ is a feature vector and $y^{(i)} \in \mathbb{R}$ is the output, learn a function $f : \mathbb{R}^{N} \mapsto \mathbb{R}$ that accurately predicts $y$ for any feature vector $\mathbf{x}$.</p></div>
</details>
<p>For the error measure, we will use the <strong>mean squared error (MSE)</strong>,</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-teal-600 dark:bg-teal-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-teal-700 dark:text-teal-400" data-lucide="book-open" viewBox="0 0 24 24"><use href="#book-open"></use></svg>Definition: Definition: Mean Squared Error (MSE)<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-teal-700 dark:text-teal-400" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Given a data set of example pairs $\mathcal{D} = \{(\mathbf{x}^{(i)}, y^{(i)}), i = 1, \ldots, M \}$ and a function $f : \mathbb{R}^{N} \mapsto \mathbb{R}$, the mean squared error of $f$ on $\mathcal{D}$ is,
$$
\text{MSE}(\mathcal{D}, f) = \frac{1}{M} \sum_{i=1}^M (y^{(i)} - f(\mathbf{x}^{(i)}))^2.
$$</p></div>
</details>
<h5 id="linear-regression">Linear Regression</h5>
<!-- **Figure 15:** 1D regression example. -->
<p>I think you are familiar with <strong>linear regression</strong>, in 1D it is simply fitting a line,</p>
<p>$$
\begin{equation}
y = wx + b,
\end{equation}
$$</p>
<p>where $w$ is the slope and $b$ is the intercept. Here we simply have one feature $x$.</p>
<p>In the $N$-dimensional case our $y$ is a linear combination of the $N$ features,</p>
<p>$$
\begin{equation}
y = w_1 x_1 + w_2 x_2 + \ldots + w_N x_N + w_0,
\end{equation}
$$</p>
<p>or equivalently,</p>
<p>$$
\begin{equation}
y = \mathbf{w}^T \mathbf{x} + w_0 = \sum_{j=1}^{N} w_j x_j + w_0 = \sum_{j=0}^{N} w_j x_j,
\end{equation}
$$</p>
<p>with defining $x_0 = 1$.</p>
<h6 id="ordinary-least-squares-ols">Ordinary Least Squares (OLS)</h6>
<p>Just as the classification task, we need to estimate our parameters $(\mathbf{w}, b)$ from the data (remember, machine learning relies on data).</p>
<p>The <strong>ordinary least squares (OLS)</strong> method <strong>minimizes the sum of the squared differences</strong> between the observed values and the predicted values, or in other words, <strong>minimizes the MSE</strong>,</p>
<p>$$
\begin{equation}
\mathbf{w}^{\star}, b^{\star} = \underset{\mathbf{w}, b}{\arg \min} \frac{1}{M} \sum_{i=1}^{M} (y^{(i)} - \mathbf{w}^T \mathbf{x}^{(i)} - b)^2.
\end{equation}
$$</p>
<p>Let’s solve OLS for <strong>one feature</strong> and see how it looks like.</p>
<p>As always, we take the derivative of the objective with respect to $w$ (arbitrarily chosen) and set it to zero,</p>
<p>$$
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial w} \frac{1}{M} \sum_{i=1}^{M} (y^{(i)} - w x^{(i)} - b)^2 &#x26; = 0 \newline
2 \frac{1}{M} \sum_{i=1}^{M} (y^{(i)} - w x^{(i)} - b) (-x^{(i)}) &#x26; = 0 \biggr\rvert \text{ Chain rule} \newline
\left( \sum_{i=1}^{M} (x^{(i)})^2 \right) w + \left( \sum_{i=1}^{M} x^{(i)} \right) b &#x26; = \sum_{i=1}^{M} x^{(i)} y^{(i)}
\end{aligned}
\end{equation}
$$</p>
<p>Equivalently for $b$,</p>
<p>$$
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial b} \frac{1}{M} \sum_{i=1}^{M} (y^{(i)} - w x^{(i)} - b)^2 &#x26; = 0 \newline
2 \frac{1}{M} \sum_{i=1}^{M} (y^{(i)} - w x^{(i)} - b) (-1) &#x26; = 0 \biggr\rvert \text{ Chain rule} \newline
\left( \sum_{i=1}^{M} x^{(i)} \right) w + M b &#x26; = \sum_{i=1}^{M} y^{(i)} \biggr\rvert b \text{ is independent of sum} \newline
\end{aligned}
\end{equation}
$$</p>
<p>Thus, we can write them in matrix form,</p>
<p>$$
\begin{equation}
\begin{bmatrix}
\sum_{i=1}^{M} (x^{(i)})^2 &#x26; \sum_{i=1}^{M} x^{(i)} \newline
\sum_{i=1}^{M} x^{(i)} &#x26; M
\end{bmatrix}
\begin{bmatrix}
w \newline
b
\end{bmatrix} =
\begin{bmatrix}
\sum_{i=1}^{M} x^{(i)} y^{(i)} \newline
\sum_{i=1}^{M} y^{(i)}
\end{bmatrix}.
\end{equation}
$$</p>
<p>Solving for $(w, b)$ yields,</p>
<p>$$
\begin{equation}
\begin{bmatrix}
w \newline
b
\end{bmatrix} =
\begin{bmatrix}
\sum_{i=1}^{M} (x^{(i)})^2 &#x26; \sum_{i=1}^{M} x^{(i)} \newline
\sum_{i=1}^{M} x^{(i)} &#x26; M
\end{bmatrix}^{-1}
\begin{bmatrix}
\sum_{i=1}^{M} x^{(i)} y^{(i)} \newline
\sum_{i=1}^{M} y^{(i)}
\end{bmatrix}.
\end{equation}
$$</p>
<h6 id="general-ols-derivation">General OLS Derivation</h6>
<p>For the general case we need to create a <strong>data matrix</strong>.</p>
<p>Let $\mathbf{x}^{(i)} \in \mathbb{R}^{N + 1}$, where we have defined $x_0^{(i)} = 1$.
Thus, let $\mathbf{X} \in \mathbb{R}^{M \times (N + 1)}$ with rows $\mathbf{x}^{(i)} \in \mathbb{R}^{N + 1}$,</p>
<p>$$
\begin{equation}
\mathbf{X} =
\begin{bmatrix}
— &#x26; (\mathbf{x}^{(1)})^T &#x26; — \newline
— &#x26; (\mathbf{x}^{(2)})^T &#x26; — \newline
&#x26; \vdots &#x26; \newline
— &#x26; (\mathbf{x}^{(M)})^T &#x26; —
\end{bmatrix}.
\end{equation}
$$</p>
<p>Equivalently, let $\mathbf{y} \in \mathbb{R}^{M}$ be a column vector with elements $y^{(i)}$,</p>
<p>$$
\begin{equation}
\mathbf{y} =
\begin{bmatrix}
y^{(1)} \newline
y^{(2)} \newline
\vdots \newline
y^{(M)}
\end{bmatrix}.
\end{equation}
$$</p>
<p>As before $\mathbf{w} \in \mathbb{R}^{N +1}$, where $w_0 = b$.</p>
<p>One can verify that,</p>
<p>$$
\mathbf{y} - \mathbf{X} \mathbf{w} =
\begin{bmatrix}
y^{(1)} \newline
y^{(2)} \newline
\vdots \newline
y^{(M)}
\end{bmatrix} -
\begin{bmatrix}
— &#x26; (\mathbf{x}^{(1)})^T &#x26; — \newline
— &#x26; (\mathbf{x}^{(2)})^T &#x26; — \newline
&#x26; \vdots &#x26; \newline
— &#x26; (\mathbf{x}^{(M)})^T &#x26; —
\end{bmatrix} \mathbf{w} =
\begin{bmatrix}
y^{(1)} \newline
y^{(2)} \newline
\vdots \newline
y^{(M)}
\end{bmatrix} -
\begin{bmatrix}
\mathbf{x}^{(1)T} \mathbf{w} \newline
\mathbf{x}^{(2)T} \mathbf{w} \newline
\vdots \newline
\mathbf{x}^{(M)T} \mathbf{w}
\end{bmatrix} =
\begin{bmatrix}
y^{(1)} - \mathbf{x}^{(1)T} \mathbf{w} \newline
y^{(2)} - \mathbf{x}^{(2)T} \mathbf{w} \newline
\vdots \newline
y^{(M)} - \mathbf{x}^{(M)T} \mathbf{w}
\end{bmatrix}.
$$</p>
<p>Thus, the MSE can be written as,</p>
<p>$$
\begin{equation}
\frac{1}{M} (\mathbf{y} - \mathbf{X} \mathbf{w})^T (\mathbf{y} - \mathbf{X} \mathbf{w}) = \frac{1}{M} \sum_{i=1}^{M} (y^{(i)} - \mathbf{x}^{(i)T} \mathbf{w})^2.
\end{equation}
$$</p>
<p>Therefore,</p>
<p>$$
\begin{equation}
\begin{aligned}
\mathbf{w}^{\star} &#x26; = \underset{\mathbf{w}}{\arg \min} \frac{1}{M} \sum_{i=1}^{M} (y^{(i)} - \mathbf{x}^{(i)T} \mathbf{w})^2 \newline
&#x26; = \underset{\mathbf{w}}{\arg \min} \frac{1}{M} (\mathbf{y} - \mathbf{X} \mathbf{w})^T (\mathbf{y} - \mathbf{X} \mathbf{w}) \newline
\end{aligned}
\end{equation}
$$</p>
<p>Taking the derivative with respect to $\mathbf{w}$ and setting it to zero yields,</p>
<p>$$
\begin{equation}
\begin{aligned}
\nabla_{\mathbf{w}} \frac{1}{M} (\mathbf{y} - \mathbf{X} \mathbf{w})^T (\mathbf{y} - \mathbf{X} \mathbf{w}) &#x26; = 0 \newline
\frac{1}{M} \nabla_{\mathbf{w}} (\mathbf{y}^T \mathbf{y} - \mathbf{y}^T \mathbf{X} \mathbf{w} - \mathbf{w}^T \mathbf{X}^T \mathbf{y} + \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}) &#x26; = 0 \biggr\rvert \text{ Multiply the parenthesis} \newline
\frac{1}{M} \nabla_{\mathbf{w}} (\mathbf{y}^T \mathbf{y} - \mathbf{y}^T \mathbf{X} \mathbf{w} - \mathbf{y}^T \mathbf{X} \mathbf{w} + \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}) &#x26; = 0 \biggr\rvert (AB)^T = B^T A^T \newline
\frac{1}{M} \nabla_{\mathbf{w}} (\mathbf{y}^T \mathbf{y} - 2 \mathbf{y}^T \mathbf{X} \mathbf{w} + \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}) &#x26; = 0 \newline
\frac{1}{M} \nabla_{\mathbf{w}} (\mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}- 2 \mathbf{y}^T \mathbf{X} \mathbf{w}) &#x26; = 0 \biggr\rvert \text{ Does not depend on } \mathbf{w} \text{, so we can ignore it} \newline
\frac{2}{M} (\mathbf{X}^T \mathbf{X} \mathbf{w} - \mathbf{X}^T \mathbf{y}) &#x26; = 0 \newline
\end{aligned}
\end{equation}
$$</p>
<p>Here we rely on two identities for the derivative, $\nabla_{\mathbf{w}} \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w} = (\mathbf{X}^T \mathbf{X} + (\mathbf{X}^T \mathbf{X})^T) \mathbf{w} = 2 \mathbf{X}^T \mathbf{X} \mathbf{w}$
and
$\nabla_{\mathbf{w}} \mathbf{y}^T \mathbf{X} \mathbf{w} = (\mathbf{y}^T \mathbf{X})^T = \mathbf{X} \mathbf{y}$.</p>
<p>Finally, solving for $\mathbf{w}$ yields,</p>
<p>$$
\begin{equation}
\begin{aligned}
\frac{2}{M} (\mathbf{X}^T \mathbf{X} \mathbf{w} - \mathbf{X}^T \mathbf{y}) &#x26; = 0 \newline
\mathbf{X}^T \mathbf{X} \mathbf{w} &#x26; = \mathbf{X}^T \mathbf{y} \newline
\mathbf{w} &#x26; = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}.
\end{aligned}
\end{equation}
$$</p>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-600 dark:bg-pink-950/5" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-700 dark:text-pink-300" data-lucide="alert-circle" viewBox="0 0 24 24"><use href="#alert-circle"></use></svg>Problem: Exercise: Can the same solution be derived from a probabilistic perspective using MLE?<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-700 dark:text-pink-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><details class="group relative px-4 py-3 my-6 border-l-4 text-base border-pink-300 dark:bg-pink-950/5">
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-pink-500 dark:text-pink-200" data-lucide="check" viewBox="0 0 24 24"><use href="#check"></use></svg>Answer<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-pink-500 dark:text-pink-200" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Yes, the same solution can be derived from a probabilistic perspective using MLE.
I will leave the derivation as an exercise for you, but the assumption is that our observation $\mathbf{y}$ is from a conditional Gaussian distribution with mean $\mathbf{X} \mathbf{w}$ and variance $\sigma^2 \mathbf{I}$.</p></div>
</details></div>
</details>
<h3 id="unsupervised-learning">Unsupervised Learning</h3>
<section data-footnotes="" class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes</h2>
<ol>
<li id="user-content-fn-1">
<p><a href="https://en.wikipedia.org/wiki/Machine_learning" rel="nofollow noreferrer noopener" target="_blank">Wikipedia, Machine Learning</a> <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2">
<p><a href="https://en.wikipedia.org/wiki/Behaviorism" rel="nofollow noreferrer noopener" target="_blank">Wikipedia, Behaviorism</a> <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3">
<p><a href="https://en.wikipedia.org/wiki/Cognitivism" rel="nofollow noreferrer noopener" target="_blank">Wikipedia, Cognitivism</a> <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4">
<p><a href="https://en.wikipedia.org/wiki/Connectionism" rel="nofollow noreferrer noopener" target="_blank">Wikipedia, Connectionism</a> <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-5">
<p><a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="nofollow noreferrer noopener" target="_blank">Wikipedia, Iris Flower Data Set</a> <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 5" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-6">
<p><a href="https://en.wikipedia.org/wiki/Likelihood_function" rel="nofollow noreferrer noopener" target="_blank">Wikipedia, Likelihood function</a> <a href="#user-content-fnref-6" data-footnote-backref="" aria-label="Back to reference 6" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-7">
<p><a href="https://machinelearningmastery.com/bayes-optimal-classifier/" rel="nofollow noreferrer noopener" target="_blank">A Gentle Introduction to the Bayes Optimal Classifier</a> <a href="#user-content-fnref-7" data-footnote-backref="" aria-label="Back to reference 7" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-8">
<p><a href="https://en.wikipedia.org/wiki/Lagrangian" rel="nofollow noreferrer noopener" target="_blank">Wikipedia, Lagrangian</a> <a href="#user-content-fnref-8" data-footnote-backref="" aria-label="Back to reference 8" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-10">
<p><a href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization" rel="nofollow noreferrer noopener" target="_blank">Sequential Minimal Optimization</a> <a href="#user-content-fnref-10" data-footnote-backref="" aria-label="Back to reference 9" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section>


</body></html> </article>  <nav class="col-start-2 grid grid-cols-1 gap-4 sm:grid-cols-2"> <a href="/blog/uncertainty#post-title" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-lg group flex items-center justify-start size-full" aria-disabled="false">  <svg width="1em" height="1em" viewBox="0 0 24 24" class="mr-2 size-4 transition-transform group-hover:-translate-x-1" data-icon="lucide:arrow-left">   <use href="#ai:lucide:arrow-left"></use>  </svg> <div class="flex flex-col items-start overflow-hidden text-wrap"> <span class="text-muted-foreground text-left text-xs"> Previous Post </span> <span class="w-full text-left text-sm text-balance text-ellipsis"> Uncertainty makes the soul </span> </div>  </a>  <a href="/blog/tools#post-title" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-lg group flex items-center justify-end size-full" aria-disabled="false">  <div class="flex flex-col items-end overflow-hidden text-wrap"> <span class="text-muted-foreground text-right text-xs"> Next Post </span> <span class="w-full text-right text-sm text-balance text-ellipsis"> Sharpen your axe </span> </div> <svg width="1em" height="1em" viewBox="0 0 24 24" class="ml-2 size-4 transition-transform group-hover:translate-x-1" data-icon="lucide:arrow-right">   <use href="#ai:lucide:arrow-right"></use>  </svg>  </a> </nav> <div class="col-start-2"> <section class="mx-auto mt-12"> <script data-astro-rerun src="https://giscus.app/client.js" data-repo="rezaarezvan/rezarezvan.com" data-repo-id="R_kgDOHvQr3w" data-category="General" data-category-id="DIC_kwDOHvQr384CiWVC" data-mapping="og:title" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="en" data-loading="lazy" crossorigin="anonymous" async></script> </section> <script>
  function updateGiscusTheme() {
    const element = document.documentElement
    const theme = element.getAttribute('data-theme')
    const iframe = document.querySelector('iframe.giscus-frame')
    if (!iframe) return
    iframe.contentWindow.postMessage(
      { giscus: { setConfig: { theme } } },
      'https://giscus.app',
    )
  }

  const observer = new MutationObserver(updateGiscusTheme)
  observer.observe(document.documentElement, {
    attributes: true,
    attributeFilter: ['class'],
  })

  window.onload = () => {
    updateGiscusTheme()
  }
</script> </div> </section> <button data-slot="button" class="items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 size-9 group fixed right-8 bottom-8 z-50 hidden" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top"> <svg width="1em" height="1em" class="mx-auto size-4 transition-all group-hover:-translate-y-0.5" data-icon="lucide:arrow-up">   <symbol id="ai:lucide:arrow-up" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 12l7-7l7 7m-7 7V5"/></symbol><use href="#ai:lucide:arrow-up"></use>  </svg> </button> <script type="module">document.addEventListener("astro:page-load",()=>{const o=document.getElementById("scroll-to-top"),t=document.querySelector("footer");o&&t&&(o.addEventListener("click",()=>{window.scrollTo({top:0,behavior:"smooth"})}),window.addEventListener("scroll",()=>{const e=t.getBoundingClientRect().top<=window.innerHeight;o.classList.toggle("hidden",window.scrollY<=300||e)}))});</script>  </div> </main> <footer class="py-4"> <div class="mx-auto flex max-w-3xl flex-col items-center justify-center gap-y-2 px-4 sm:flex-row sm:justify-between"> <div class="flex flex-wrap items-center justify-center gap-x-2 text-center"> <span class="text-muted-foreground text-sm">
&copy; 2025 • rezarezvan.com </span> </div> </div> </footer> <div id="backdrop" class="invisible fixed top-0 left-0 z-50 flex h-screen w-full justify-center bg-[rgba(0,0,0,0.5)] p-6 backdrop-blur-sm" data-astro-transition-persist="astro-t6dxx5el-4"> <div id="pagefind-container" class="m-0 flex h-fit max-h-[80%] w-full max-w-screen-sm flex-col overflow-auto rounded border border-black/15 bg-neutral-100 p-2 px-4 py-3 shadow-lg dark:border-white/20 dark:bg-neutral-900"> <div id="search" class="pagefind-ui pagefind-init" data-pagefind-ui data-bundle-path="/pagefind/" data-ui-options="{&#34;showImages&#34;:false,&#34;excerptLength&#34;:15,&#34;resetStyles&#34;:false}"></div> <script type="module" src="/_astro/Search.astro_astro_type_script_index_0_lang.tZYucdM2.js"></script> <div class="dark:prose-invert mr-2 pt-4 pb-1 text-right text-xs">
Press <span class="prose dark:prose-invert text-xs"><kbd class="">Esc</kbd></span> or click anywhere to close
</div> </div> </div> <script>
  document.addEventListener('DOMContentLoaded', () => {
    const magnifyingGlass = document.getElementById('magnifying-glass')
    const backdrop = document.getElementById('backdrop')

    function openPagefind() {
      const searchDiv = document.getElementById('search')
      const search = searchDiv.querySelector('input')
      setTimeout(() => {
        search.focus()
      }, 0)
      backdrop?.classList.remove('invisible')
      backdrop?.classList.add('visible')
    }

    function closePagefind() {
      const searchDiv = document.getElementById('search')
      const search = searchDiv.querySelector('input')
      if (search) {
        search.value = ''
      }
      backdrop?.classList.remove('visible')
      backdrop?.classList.add('invisible')
    }

    // open pagefind
    magnifyingGlass?.addEventListener('click', () => {
      openPagefind()
    })

    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape') {
        closePagefind()
      }
    })

    // close pagefind when searched result(link) clicked
    document.addEventListener('click', (event) => {
      if (event.target.classList.contains('pagefind-ui__result-link')) {
        closePagefind()
      }
    })

    backdrop?.addEventListener('click', (event) => {
      if (!event.target.closest('#pagefind-container')) {
        closePagefind()
      }
    })

    // prevent form submission
    const form = document.getElementById('form')
    form?.addEventListener('submit', (event) => {
      event.preventDefault()
    })
  })
</script>  </div> </body></html>