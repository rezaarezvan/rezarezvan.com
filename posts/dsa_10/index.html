<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8"><meta property="og:site_name" content="rezvan"><title>DSA: Part 10 - Complexity (2) | rezvan</title>
  <meta property="og:title" content="DSA: Part 10 - Complexity (2) | rezvan"><meta property="og:description" content="">
  <meta property="og:type" content="blog">
  <meta property="og:link" content="https://rezvan.xyz/posts/dsa_10/"><link rel="shortcut icon" type="image/png" href=https://rezvan.xyz//images/icon.png />
  <meta property="og:image" content="https://rezvan.xyz//images/icon.png" /><meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz//css/main.css" />    
</head>

<body>
  <div class="wrapper">
	<div class="content">
		<div class="header_main">
	<a href="https://rezvan.xyz/"><p class="header_title">rezvan</p><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

	</a>
  <br>
  <nav id="main">
    
      <a href="/About/">About</a>
    
      <a href="/CV/">CV</a>
    
  </nav></div>

  <article><div class="title_wrapper">
			<h1 class="title">DSA: Part 10 - Complexity (2)</h1><p class="single_time">Dec 9, 2022</p></div>
		<section class="post">
			<p>We have actually covered everything in this course - in this part we&rsquo;ll do some exercises!</p>
<h3 id="order-of-growth-of-functions">Order of growth of Functions</h3>
<p>Let&rsquo;s find out the complexity ($\mathcal{O}$) of:
$$
T(n) = 5(3n^2 + 2n + 6)(4\ log_{10}(n) + 1)
$$</p>
<p>Since we seek the growth rate - we can use our rules about complexity. We can remove all constants:
$$
T(n) = (n^2 + n)(log_{10}(n))
$$</p>
<p>The next rule we can apply is, &ldquo;the most dominating factor &lsquo;wins&rsquo;&rdquo; as I like to call it. Therefore:
$$
T(n) = (n^2)(log_{10}(n))
$$</p>
<p>Then we just multiply!
$$
T(n) = n^2\ log_{10}(n)
$$</p>
<p>And since we usually write $log$ when using Big-O notation:
$$
T(n) =  n^2\ log(n)
$$</p>
<p>Now we can say that $T(n)$ has a $\mathcal{O}(n^2\ log(n))$ complexity!. Since we are talking about $\mathcal{O}$, this means this function has a <strong>lower</strong> bound of this. This means that $T(n)$ also has a complexity of $\mathcal{O}(n^3)$ for example.</p>
<p>So what we <em>really</em> mean is that $T(n)$ has a $\Theta(n^2\ log(n))$ complexity.</p>
<p><strong>Suppose an algorithm takes time <em>t</em> on an input of size <em>n</em>. How many times longer does it take on an input of size 10n if&hellip;</strong></p>
<ul>
<li>
<p>If the algorithm is $\Theta(n)$?</p>
</li>
<li>
<p>If the algorithm is $\Theta(n^2)$?</p>
</li>
<li>
<p>If the algorithm is $\Theta(n^3)$?</p>
</li>
<li>
<p>If the algorithm is $\Theta(n\ log(n))$?</p>
</li>
<li>
<p>If the algorithm is $\Theta(log(n))$?</p>
</li>
</ul>
<p>This is quite easy! We just plug in our new $n$, and see how much $t$ grows!</p>
<ul>
<li>
<p>If the algorithm is $\Theta(n)$?</p>
<ul>
<li>We get $10n \rightarrow 10t$!</li>
</ul>
</li>
<li>
<p>If the algorithm is $\Theta(n^2)$?</p>
<ul>
<li>We get $100n \rightarrow 100t$!</li>
</ul>
</li>
<li>
<p>If the algorithm is $\Theta(n^3)$?</p>
<ul>
<li>We get $1000n \rightarrow 1000t$!</li>
</ul>
</li>
<li>
<p>If the algorithm is $\Theta(n\ log(n))$?</p>
<ul>
<li>
<p>We get $10n\ \cdot log(10n)$!</p>
</li>
<li>
<p>This isn&rsquo;t just $10 log(10)&quot; times more - it&rsquo;s a <em>little</em> bit longer, or so called &ldquo;logarithmic linear&rdquo;.</p>
</li>
</ul>
</li>
<li>
<p>If the algorithm is $\Theta(log(n))$?</p>
<ul>
<li>
<p>We get $log(10n)$!</p>
</li>
<li>
<p>This means just a constant <strong>more</strong> time!</p>
</li>
</ul>
</li>
</ul>
<h3 id="complexity-analysis">Complexity Analysis</h3>
<p>Let&rsquo;s analyze the following snippet of code and it&rsquo;s complexity.</p>
<pre tabindex="0"><code>found = false
for x in list:
    for y in list:
        if x + y == 0:
            found = true
</code></pre><p>If we say that the length of <code>list</code> is $n$. In the first loop we will have a complexity of $\mathcal{O}(n)$.
The inner loop will follow, using our previous rule of &rsquo;nested loops means multiplication&rsquo;.
This means our final program will have the complexity of $\mathcal{O}(n^2)$.</p>
<p>Now let&rsquo;s see over this code snippet:</p>
<pre tabindex="0"><code>found = false
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = list[i]
        y = list[j]
        if x + y == 0:
            found = true
</code></pre><p>In this snippet - we&rsquo;ll have the first loop iterating $n$ times - 
however, the second loop, it will iterate $(0, \dots ,n -1), (1, \dots , n - 2), \dots$</p>
<p>This means that the number of times the second loop will run is between 1 and $n$ times - using our definition of complexity.
Let&rsquo;s call the number of times our loop runs $m$, $m \leq n$ which means m has an complexity of $\mathcal{O}(n)$.</p>
<p>This finally means we have a total complexity of $\mathcal{O}(n^2)$</p>
<p>Now let&rsquo;s do the same, but for three numbers!</p>
<pre tabindex="0"><code>found = false
for i in 0 .. n - 1:
    for j in i .. n - 1:
        for k in j .. n - 1:
            x = list[i]
            y = list[j]
            z = list[k]

            if x + y + z == 0:
                found = true
</code></pre><p>Exactly the same logic goes as from the last question to this, we can prove that each loop has a complexity of $\mathcal{O}(n)$.</p>
<p>Which gives the total complexity of $\mathcal{O}(n^3)$.</p>
<p>Now let&rsquo;s look at a similar program:</p>
<pre tabindex="0"><code>pairs_list = []
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = list[i]
        y = list[j]
        pairs_list.add(x + y)

found = false
for xplusy in pairs_list:
    for zplusw in pairs_list:
        if xplusy + zplusw == 0:
            found = true
</code></pre><p>As we&rsquo;ve have stated above, the first part of the program will have a complexity of $\mathcal{O}(n^2)$.
Note that the <code>add()</code> function takes $\mathcal{O}(1)$ for dynamic arrays.</p>
<p>However, in the next block, the new array length is $n^2$, since we have added all possible permutations of pairs.
So the loops will now through $n^2$ elements. Which in total results a complexity of $\mathcal{O}(n^4)$.</p>
<p>From our earlier rules, we &lsquo;add&rsquo; blocks of codes, so the complexity is $\mathcal{O}(n^2) + \mathcal{O}(n^4)$.
Which means the resulting complexity becomes $\mathcal{O}(n^4)$.</p>
<h3 id="data-structure-complexities">Data Structure Complexities</h3>
<p>Let&rsquo;s refresh our memory and state all the complexities for our data structures and their functions.</p>
<ul>
<li>
<p>Dynamic Arrays:</p>
<ul>
<li>
<p>Get/Set:</p>
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
<li>
<p>Add/Remove <strong>at end</strong>:</p>
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
<li>
<p>Add/remove <strong>elsewhere</strong>:</p>
<ul>
<li>$\mathcal{O}(n)$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Stacks/queues:</p>
<ul>
<li>
<p>Push/Pop:</p>
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
<li>
<p>Enqueue/Dequeue:</p>
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Binary Heaps:</p>
<ul>
<li>
<p>Add/RemoveMin (or Max):</p>
<ul>
<li>$\mathcal{O}(log(n))$</li>
</ul>
</li>
<li>
<p>getMin (or Max):</p>
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>BSTs:</p>
<ul>
<li>
<p>Add/Remove/Search (worst case, meaning it&rsquo;s already sorted):</p>
<ul>
<li>$\mathcal{O}(n)$</li>
</ul>
</li>
<li>
<p>Otherwise:</p>
<ul>
<li>$\mathcal{O}(log(n))$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Stacks/queues:</p>
<ul>
<li>Add/Remove/Search (Always!):
<ul>
<li>$\mathcal{O}(log(n))$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Hash Tables:</p>
<ul>
<li>Add/Remove/Search (Given that the hash function is &lsquo;good&rsquo;):
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>General Tree:</p>
<ul>
<li>
<p>If you <strong>down</strong> in a tree, you&rsquo;ll visit:</p>
<ul>
<li>$\mathcal{O}(height)$ nodes</li>
</ul>
</li>
<li>
<p>If you explore every node, you&rsquo;ll visit:</p>
<ul>
<li>$\mathcal{O}(n)$ nodes</li>
</ul>
</li>
<li>
<p>A tree has the <strong>worst</strong> case $\mathcal{O}(n)$ height.</p>
</li>
<li>
<p>A <strong>balanced</strong> tree is <strong>always</strong> $\mathcal{O}(log(n))$ height.</p>
</li>
</ul>
</li>
</ul>
<h3 id="analyzing-more-complexities">Analyzing more complexities</h3>
<p>Let&rsquo;s take a look at program which utilise different data structures now:</p>
<pre tabindex="0"><code>pairs_list = []
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = list[i]
        y = list[j]
        pairs_list.add(x + y)

merge_sort(pairs_list)

found = false
for xplusy in pairs_list:
    if binary_search(pairs_list, -xplusy):
        found = true
</code></pre><p>So, we&rsquo;ve seen that first part, we know it&rsquo;s $\mathcal{O}(n^2)$.
But now we see a <code>merge_sort()</code> - this has a complexity of $\mathcal{O}(n\ log(n))$.
As we stated before, the list after the first block has a length of $n^2$. 
Which means <code>merge_sort()</code> will have a complexity of $\mathcal{O}(n^2\ log(n^2))$</p>
<p>This will just sort it so no length is added.</p>
<p>Then the next block, the for loop will have a complexity of $\mathcal{O}(n^2)$.
The binary search algorithm, has a complexity of $\mathcal{O}(log(n^2))$.</p>
<p>So this block will in total have a complexity of $\mathcal{O}(n^2\ log(n^2))$.</p>
<p>So if we add these blocks together and apply our rules we will get a total complexity of:
$\mathcal{O}(n^2\ log(n^2))$. We can apply some log rules to this:
$$
\mathcal{O}(n^2\ log(n^2))
\newline
\mathcal{O}(n^2\ 2\ log(n))
\newline
\mathcal{O}(n^2\ log(n))
$$</p>
<p>So finally our answer is, $\mathcal{O}(n^2 log(n))$</p>
<p>Let&rsquo;s now look at a case using a tree:</p>
<pre tabindex="0"><code>pairs_set = empty AVL_tree
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = pairs_set[i]
        y = pairs_set[j]

        if pairs_set.contains(-(x+y)):
            return true

        pairs_set.add(x+y)
</code></pre><p>As we&rsquo;ve seen before, the loops are $\mathcal{O}(n^2)$ - now the interesting part is the <code>contains()</code> to check if an element is present.
To check whether an element is present in a tree, has a complexity of $\mathcal{O}(log(n))$.
The rest of the operations are constant so we can ignore them (including the <code>add()</code> for the AVL tree).</p>
<p>So therefore the final complexity is $\mathcal{O}(n^2\ log(n))$. In the absolute worst case the <code>contains()</code> will be $\mathcal{O}(n)$, but let&rsquo;s ignore that :).</p>
<p>Now let&rsquo;s look at a hash table:</p>
<pre tabindex="0"><code>pairs_set = empty Hash_table
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = pairs.set[i]
        y = pairs_set[j]

        if pairs_set.contains(-(x + y)):
            return true

        pairs_set.add(x + y)
</code></pre><p>As per usual, the loops makes this $\mathcal{O}(n^2)$, now, a search in a hash table is $\mathcal{O}(1)$, if our hash function is &lsquo;good&rsquo;.</p>
<p>The rest of the operations are constant. So therefore the overall complexity is $\mathcal{O}(n^2)$.</p>
<p>Now let&rsquo;s look at a BST example:</p>
<pre tabindex="0"><code>pairs_set = empty BST
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = pairs_set[i]
        y = pairs_set[j]

        if pairs_set.contains(-(x + y)):
            return true

        pairs_set.add(x + y)
</code></pre><p>The usual $\mathcal{O}(n^2)$ loops :). Now the <code>contains()</code> is the interesting part.
Since this is a BST, the <code>contains()</code> will have a complexity of $\mathcal{O}(n)$ in the worst case, since the BST can become unbalanced.</p>
<p>The same applies for <code>add()</code>. Since the rest of the operations are constant so therefore it will be, $\mathcal{O}(n^4)$.</p>
<h3 id="different-kinds-of-complexiites">Different kinds of complexiites</h3>
<p>We also need to consider the different cases</p>
<ul>
<li>
<p>Best-case</p>
<ul>
<li>This is not useful.</li>
</ul>
</li>
<li>
<p>Worst-case</p>
<ul>
<li>This is the most useful.</li>
</ul>
</li>
<li>
<p>Average-case</p>
<ul>
<li>Can be useful sometimes, mostly gives us a &lsquo;indicator&rsquo;.</li>
</ul>
</li>
</ul>
<p>Let&rsquo;s now talk about <strong>expected</strong> and <strong>amortised</strong> complexity.</p>
<h4 id="expected-complexity">Expected Complexity</h4>
<p>This is useful for randomised algorithms! It&rsquo;s the average over all possible random choice for a particular input.</p>
<p>For example, if we choose a random pivot, we turn quicksort from average-case $\mathcal{O}(n\ log(n))$ to expected $\mathcal{O}(n\ log(n))$</p>
<h4 id="amortised-complexity">Amortised Complexity:</h4>
<p>Amortised complexity is, the average over any sequence of operations, this is super useful!</p>
<p>For example, we use this to make dynamic arrays have a amortised complexity of $\mathcal{O}(1)$.</p>
<p>However, when we&rsquo;re calculating the total runtime of a program, it&rsquo;s safe to forget about this amortised bit and just treat each operation as costing $\mathcal{O}(1)$.</p>
<h3 id="conclusion">Conclusion</h3>
<p>This was it for this part - and the final part in this series. I really enjoyed this DSA course, super fun :).</p>

		</section>
  </article>
	</div>

	<footer><p class="footer_msg">Memento mori</p></footer>

  </div>
</body>
</html>
