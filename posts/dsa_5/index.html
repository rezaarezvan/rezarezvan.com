<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8"><meta property="og:site_name" content="rezvan"><title>DSA: Part 5 - Hash Tables | rezvan</title>
  <meta property="og:title" content="DSA: Part 5 - Hash Tables | rezvan"><meta property="og:description" content="">
  <meta property="og:type" content="blog">
  <meta property="og:link" content="https://rezvan.xyz/posts/dsa_5/"><link rel="shortcut icon" type="image/png" href=https://rezvan.xyz//images/icon.png />
  <meta property="og:image" content="https://rezvan.xyz//images/icon.png" /><meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz//css/main.css" />    
</head>

<body>
  <div class="wrapper">
	<div class="content">
		<div class="header_main">
	<a href="https://rezvan.xyz/"><p class="header_title">rezvan</p><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

	</a>
  <br>
  <nav id="main">
    
      <a href="/About/">About</a>
    
      <a href="/CV/">CV</a>
    
  </nav></div>

  <article><div class="title_wrapper">
			<h1 class="title">DSA: Part 5 - Hash Tables</h1><p class="single_time">Nov 29, 2022</p></div>
		<section class="post">
			<h3 id="hash-tables">Hash Tables</h3>
<p>Hash tables are one of the most famous (and widely used) data structures. But why are they so popular and powerful? 
Before we can answer that question we need to look back at sets and maps.</p>
<h3 id="sets-and-maps-in-context">Sets and Maps in Context</h3>
<p>Just to refresh our memory - a set is a <em>collection of items</em>, where duplicates aren&rsquo;t allowed.
Maps are <strong>sets</strong> of <em>keys</em>, each having a associate value - or you can formulate it as - a <strong>set</strong> of <em>key-value pairs</em>.</p>
<p>Maps are used in almost all programs and applications since they&rsquo;re very powerful and intuitive. They can act as databases (some databases are just straight up maps as well):</p>
<ul>
<li>Look up a person by their social security number.</li>
<li>Look up a file in a computer by its name/filetype/size.</li>
<li>Find all words appearing in a text/website/book.</li>
</ul>
<p>Only problem is that, our usual implementation of maps makes the <code>search()</code> and <code>update()</code> are quite slow, in fact, the complexity is usually
$\mathcal{O}(n)$ or in case we&rsquo;re using a <em>mutlimap</em> $\mathcal{O}(n \cdot m)$.</p>
<p>We would like a complexity of $\mathcal{O}(log(n))$ or $\mathcal{O}(1)$. Even if we implement maps using all the different data structures we have - we still can&rsquo;t achieve this:</p>
<ul>
<li>
<p>Dynamic Array of key-value pairs</p>
<ul>
<li>Search takes linear time</li>
<li>Insertion takes linear time</li>
</ul>
</li>
<li>
<p>A Linked List</p>
<ul>
<li>Same as the dynamic array</li>
</ul>
</li>
<li>
<p>A <em>sorted</em> Dynamic Array</p>
<ul>
<li>Search takes logarithmic time (binary search).</li>
<li>Insertion still takes linear time&hellip;</li>
</ul>
</li>
</ul>
<p>The answer is <em><strong>Hash Tables</strong></em>!</p>
<h3 id="separate-chaining-hash-tables">Separate Chaining Hash Tables</h3>
<p>One thing to know before we dive in is - there are two different strategies when it comes to has tables, we&rsquo;ll begin with separate chaining.</p>
<h4 id="hash-functions">Hash Functions</h4>
<p>Before we start talking about what a hash <strong>table</strong> is we first need to look into what a hash <strong>function</strong> is. The problem with ordinary maps that they are a unordered data structure.
We need some kind of function which calculates, based on different parameters, where <code>Item x</code> should go into our map, because afterwards we can just search for that index, which is much faster. This is what hash functions are for!</p>
<p>At the same time our hash functions needs to be &lsquo;good&rsquo;. We can&rsquo;t have a hash functions that only returns index 1 and stack every item into the same slot. But we also can&rsquo;t be lazy, in the example of sorting a collection of people.
We can&rsquo;t just go by their first letter in their name, there&rsquo;s a lot more names that begin with a &lsquo;A&rsquo; than a &lsquo;Z&rsquo; for example.</p>
<p>So we want a hash functions that also <em>evenly</em> distributes the objects, and at the same time efficient to compute.</p>
<p>With this in mind we can now define a hash table.</p>
<h3 id="hash-tables-1">Hash Tables</h3>
<p>A hash table uses a hash function to compute the array index. It&rsquo;s not possible to put more than one object into one array slot, so if our hash function returns the same index for different objects (we will later see, by definition, this is a bad hash function) we need a solution.
Separate chaining is one of them.</p>
<h3 id="separate-chaining">Separate Chaining</h3>
<p>In a separate chaining hash table, instead of having just one array slot, we instead have the value/slots being a pointer. This pointer then points to collections of all the values/objects having the same hash value.</p>
<p>Usually this is a linked list, but any searchable collection works (A dynamic array for example).</p>
<h3 id="open-addressing-hash-tables">Open Addressing Hash Tables</h3>
<p>In the other case, being open addressing, each slot contains exactly one object/value. However - if we ever encounter a conflict, we just move that value/object into a free slot, the tricky part is a efficient way of finding a free slot.
We&rsquo;ll see more of this later.</p>
<h3 id="separate-chaining-hash-set">Separate Chaining: Hash Set</h3>
<p>For our implementation of Hash tables using separate chaining we could use sets - and for the underlying collection, we usually use linked lists.</p>
<p>So a kind of implementation (in pseudo-code) would be:</p>
<pre tabindex="0"><code>class SeparateChainingHashSet &lt;Item&gt; implements Set&lt;Item&gt;:
    table: Array of Set&lt;Item&gt;
    
    // A given good hash function
    hash(x : Item) -&gt; int:
        // Returns good hash value

    contains(x : Item) -&gt; boolean:
        bucket : Set&lt;Item&gt; = table[hash(x)]
        return bucket.contains(x)

    add(x : Item):
        bucket : Set&lt;Item&gt; = table[hash(x)]
        bucket.add(x)

    remove(x : Item):
        bucket : Set&lt;Item&gt; = table[hash(x)]
        bucket.remove(x)
</code></pre><p>As we can there is quite a lot of &lsquo;mental overhead&rsquo;, using a ordinary map it also becomes faster since we have actual key-value pairs, therefore being faster.
Also we need know the size of the Hash table and also handle null pointers so let&rsquo;s fix that as well!</p>
<pre tabindex="0"><code>class SeparateChainingHashMap&lt;Key, Value&gt; implements Map&lt;Key, Value&gt;:
    table: Array of Map&lt;Key, Value&gt;
    size : int = 0

    hash(k : Key) -&gt; int:
        // Returns good hash value

    contains(k : Key) -&gt; boolean:
        bucket : Map&lt;Key, Value&gt; = table[hash(k)]
        return bucket.contains(k)
    
    get(k : Key) -&gt; Value:
        bucket : Map&lt;Key, Value&gt; = table[hash(k)]
        return bucket.get(k)

    put(k : Key, v : Value):
        bucket : Map&lt;Key, Value&gt; = table[hash(k)]
        if bucket == NULL:
            bucket = table[hash(x)] = new Map()
        if not bucket.contains(k):
            bucket.put(k, v)
            size += 1

    remove(k : Key):
        bucket : Map&lt;Key, Value&gt; = table[hash(k)]
        bucket.remove(k)
</code></pre><p>If we actually try to implement it with the underlying data structure, the linked list, it would look something like:</p>
<pre tabindex="0"><code>class SeparateChainingHashMap&lt;Key, Value&gt; implements Map&lt;Key, Value&gt;:
    ...
    class Node:
        key : Key
        val : Value
        next : Node

    get(k : Key) -&gt; Value:
        node : Node = table[hash(k)]
        while node != NULL:
            if k == node.key
                return node.val
            node = node.next
        return NULL

    put(k : Key, v : Value):
        h : int = hash(k)
        node : Node = table[h]
        while node != NULL:
            if k == node.key:
                node.val = v
                return
            node = node.next
        // If we are given a new key
        table[h] = Node(k, v, table[h])
</code></pre><h3 id="load-factor">Load Factor</h3>
<p>We should always assume that we have a good hash functions, since in most libraries, they are good! But to understand why it&rsquo;s &lsquo;good&rsquo; we need to understand something called <em>Load factor</em>.
The load factor is the average number of elements per slot/index.</p>
<p>So load factor = $\frac{N}{M}$, where N is # of elements and M is the array size. 
We haven&rsquo;t discussed it yet but, what&rsquo;s the complexity of <code>contains</code>, <code>add</code>, and <code>put</code> for example? Well it depends on the total array size and the number of elements per slot. The load factor!</p>
<p>Therefore the complexity of each operation is $\mathcal{O}(\frac{N}{M})$</p>
<p>Which tells us, if we can keep N and M roughly the same all of our operations will be constant! This is the magic with Hash tables.</p>
<p>So our array size must grow when the hash table grows. To achieve this we use a dynamic array, when we need to resize the hash table array, we create a new array and copy all old elements using <code>add()</code>.</p>
<pre tabindex="0"><code>put(k : Key, v : Value):
    if size ≥ 8 * table.length:
        resize(2 * table.length)
    bucket : Map&lt;Key, Value&gt; = table[hash(k)]
        if bucket == NULL:
            bucket = table[hash(x)] = new Map()
        if not bucket.contains(k):
            bucket.put(k, v)
            size += 1

remove(k : Key):
    if size ≤ 2 * table.length:
        resize(table.length / 2)
    bucket : Map&lt;Key, Value&gt; = table[hash(k)]
        bucket.remove(k)

resize(buckets : int):
    oldtable = table
    table = new Array with size buckets
    size = 0
    for bucket in oldtable:
        for k,v in bucket:
            this.put(k, v)
</code></pre><p>In this example I&rsquo;ve chosen to resize the table with a factor of 2. However, this is can actually lead to some problems.
The &lsquo;optimal&rsquo; way is choose <em>prime</em> closest to 2 * oldsize.</p>
<h3 id="hash-and-compress">Hash and compress</h3>
<p>In actuality, the hash functions is a composition of functions. First we get the <em>hash code</em> for a given object/value. Then we need to <em>compress</em> it to fit inside our hash table.</p>
<p>How we compress our is quite simple (often) - it is:</p>
<pre tabindex="0"><code>index = hash_code % array_size
i = h % M
</code></pre><p>This is called &lsquo;modular&rsquo; hashing and it&rsquo;s the most common one. One thing to remember that the <em>hash code</em> <strong>never</strong> changes. Only the compressed version can change since we can change the size of it.</p>
<p>So we might see different outputs for <code>hash(x)</code> depending on size - but remember, the actual <em>hash code</em> <strong>never</strong> changes.</p>
<h3 id="requirements-on-hash-functions">Requirements on hash functions</h3>
<p>There is one very strict requirement on hash functions:</p>
<ul>
<li>Equal objects must have equal hash codes</li>
</ul>
<p>Or in coding terms:</p>
<pre tabindex="0"><code>if x === y then x.hash() == y.hash()
</code></pre><p>There are some desirable properties:</p>
<ul>
<li>
<p>If x and y have the same <em>hash code</em>, then they are &rsquo;equal&rsquo;</p>
</li>
<li>
<p>The distribution should be uniform and independent</p>
</li>
</ul>
<h3 id="open-addressing-probing">Open Addressing: Probing</h3>
<p>As we defined earlier, open addressing is a method where we find a new empty slot if there&rsquo;s a conflict. Probing is how we find this empty slot.</p>
<p>The easiest approach is so called <em>linear probing</em>, where we simply go to the next index (increase with some constant) and check if it&rsquo;s empty, with wrapping around the array.</p>
<pre tabindex="0"><code>class LinearProbingHashSet&lt;Item&gt;:
    table: Array of Item
    size : int = 0

    add(x : Item):
        i = hash(x)
        while table[i] != NULL:
                if x == table[i]:
                    return
                i = (i + 1) % table.length
        table[i] = x
        size += 1
</code></pre><p>Or in the the Hash Map case:</p>
<pre tabindex="0"><code>class LinearProbingHashMap&lt;Key, Value&gt;:
    table: Map&lt;Key, Value&gt;
    size : int = 0

    add(k : Key, v : Value):
        i = hash(k)
        while table[i] != NULL:
                if k == table[i]:
                    return
                i = (i + 1) % table.length
        table[i] = value
        size += 1
</code></pre><h3 id="open-addressing-compared-to-separate-chaining">Open Addressing compared to Separate Chaining</h3>
<p>Let&rsquo;s now compare both of these approaches now that we have a grasp of them.</p>
<ul>
<li>
<p>Separate Chaining</p>
<ul>
<li>Load factor can be &gt; 1 without performance loss.</li>
<li>The extra list nodes take up unnecessary memory.</li>
<li>Since the list nodes are (usually) scattered in memory - we can not optimize via for example CPU caching.</li>
</ul>
</li>
<li>
<p>Open Addressing</p>
<ul>
<li>Load factor <strong>must</strong> be &lt; 1, and the performance drops when load factor &gt; 3/4.</li>
<li>Doesn&rsquo;t take up unnecessary memory with extra list nodes.</li>
<li>Elements with the same hash code tend to be close in memory - therefore we can utilize CPU caching.</li>
</ul>
</li>
</ul>
<h3 id="deletion">Deletion</h3>
<p>If we want to delete object while using Open Addressing - we will encounter a very famous problem called &lsquo;clusters&rsquo;.
A short definition of clusters is, due to Open Addressing, we will get small &lsquo;clusters&rsquo; or &lsquo;chunks&rsquo;. These are quite valuable since they tell us which objects have similar hash codes.</p>
<p>But first to understand why, let&rsquo;s see how we <em>would</em> delete items.</p>
<h4 id="naive-approach">Naive approach</h4>
<p>Our naive approach would look something like:</p>
<pre tabindex="0"><code>remove(x : Item):
    i = hash(x)
    while table[i] != NULL:
        if x == table[i]:
            table[i] = NULL
        i = (i + 1) % table.length
</code></pre><p>And in the map case:</p>
<pre tabindex="0"><code>remove(k : Key):
    i = hash(x)
    while table[i] != NULL:
        if k == table[i]:
            table[i] = NULL
        i = (i + 1) % table.length
</code></pre><p>But this won&rsquo;t work - if we then want to find something to the <strong>right</strong> of this deleted cell - but their hash is <strong>before</strong> the deleted cell, we won&rsquo;t be able to, since we find the &lsquo;NULL&rsquo; value before we can find the actual object.</p>
<p>There are two possible solutions:</p>
<ul>
<li>
<p>Lazy deletion:</p>
<ul>
<li>We don&rsquo;t necessarily &lsquo;delete&rsquo; the item, just &lsquo;mark&rsquo; it deleted, rather than empty.</li>
</ul>
</li>
<li>
<p>Recalculation:</p>
<ul>
<li>We reinsert all elements in the cluster that are to the <strong>right</strong> of the deleted element.</li>
</ul>
</li>
</ul>
<p>One problem we will also encounter is, we won&rsquo;t be able to properly count # of deleted cells - therefore our resizing calculations will suffer - the solution is to have a variable for each deleted cell.
If this number exceeds some threshold we resize.</p>
<h3 id="final-implementations">Final implementations</h3>
<p>So we&rsquo;ve cover all topics and problems that comes with implementing Hash tables - so let&rsquo;s implement the final version.</p>
<p>For a Hash <strong>Set</strong>:</p>
<pre tabindex="0"><code>class LinearProbingHashSet&lt;Item&gt;:
    table: Array of HashCell
    size: int = 0
    n_deleted: int = 0
    DELETED: HashCell = HashCell(value = NULL)

    class HashCell:
        value: Item

    load_factor() -&gt; float:
        return (size + deleted) / table.length

    add(x : Item):
        if load_factor() &gt; 0.75:
            resize(2 * table.length)
        i = hash(x)

        while table[i] != NULL and table[i] != DELETED:
            if x == table[i].value:
                return
            i = (i + 1) % table.length
        size += 1
        if table[i] = DELETED:
            n_deleted -= 1

        table[i] = HashCell(value = x)

    contains(x : Item) -&gt; boolean:
        i = hash(x)
        while table[i] != NULL:
            if x == table[i].value:
                return True
            i = (i + 1) % table.length
        return False

    remove(x : Item):
        i = hash(x)
        while table[i] != NULL:
            if x == table[i].value:
                table[i] = DELETED
                size -= 1
                n_deleted += 1
                if load_factor() &lt; 0.25:
                    resize(table.length // 2)
                return
            i = (i + 1) % table.length

    resize(buckets: int):
        oldtable = table
        table = new Array with size buckets
        size = n_deleted = 0
        for cell in oldtable:
            if cell != NULL and cell != DELETED:
                table.add(cell.value)
</code></pre><p>And in the <strong>Map</strong> case:</p>
<pre tabindex="0"><code>class LinearProbingHashMap&lt;Key, Value&gt;:
    table: Array of HashCell
    size: int = 0
    n_deleted: int = 0
    DELETED: HashCell = HashCell(key = NULL,value = NULL)

    class HashCell:
        key: Key
        value: Value

    load_factor() -&gt; float:
        return (size + deleted) / table.length

    put(k : Key, v : Value):
        if load_factor() &gt; 0.75:
            resize(2 * table.length)
        i = hash(k)

        while table[i] != NULL and table[i] != DELETED:
            if k == table[i].key:
                return
            i = (i + 1) % table.length
        size += 1
        if table[i] = DELETED:
            n_deleted -= 1

        table[i] = HashCell(key = k, value = v)

    contains(k : Key) -&gt; boolean:
        i = hash(k)
        while table[i] != NULL:
            if k == table[i].key:
                return True
            i = (i + 1) % table.length
        return False

    remove(k : Key):
        i = hash(k)
        while table[i] != NULL:
            if k == table[i].value:
                table[i] = DELETED
                size -= 1
                n_deleted += 1
                if load_factor() &lt; 0.25:
                    resize(table.length // 2)
                return
            i = (i + 1) % table.length

    resize(buckets: int):
        oldtable = table
        table = new Array with size buckets
        size = n_deleted = 0
        for cell in oldtable:
            if cell != NULL and cell != DELETED:
                table.add(cell.key, cell.value)
</code></pre><h3 id="clustering">Clustering</h3>
<p>As mentioned before - clusters is by-product of open addressing - but these make the search times slower, in the worst time it becomes linear!</p>
<p>The famous Donald E. Knuth formulated the so called &lsquo;Knuth&rsquo;s parking problem&rsquo; which shows how rapid this cluster problem grows.</p>
<h3 id="alternatives-to-linear-probing">Alternatives to linear probing</h3>
<p>Instead of linearly increase, we could do <em>quadratic</em>, <em>double hashing</em> which depends on a <em>second</em> hash function, or <em>moving</em> alternatives such as:
<em>cuckoo</em>, <em>hopscotch</em> and <em>Robin hood</em> hashing. We will only use linear probing though&hellip;</p>
<h3 id="summary">Summary</h3>
<p>There&rsquo;s <strong>a lot</strong> to hashing and hash tables in general. But I think we&rsquo;ll leave it here, one final thing I want to bring up is:</p>
<p>Hash tables are not ordered! If we want to find a maximum value in a hash table - it will take linear time! Since there&rsquo;s no underlying order.</p>
<p>In the next part we&rsquo;ll begin looking at trees - starting at <strong>B</strong>alanced <strong>S</strong>earch <strong>T</strong>rees or BSTs for short. We&rsquo;ll actually compare them to Hash maps, since they are quite similar in some aspects.</p>

		</section>
  </article>
	</div>

	<footer><p class="footer_msg">Memento mori</p></footer>

  </div>
</body>
</html>
