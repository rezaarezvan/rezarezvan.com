<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>rezvan | My BSc Thesis, ClaudesLens</title><link rel=icon type=image/png href=https://rezvan.xyz/images/icon.png><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Introduction I have finally finished undergrad and would like to make a blog post about what I have been working on these past ~6 months. The tile of our thesis is:
ClaudesLens: Uncertainty Quantification in Computer Vision Models
However, before I dive into the project and what we actually did, let me tell you what we wanted to do.
BayesLens Originally, we wanted to create &ldquo;Uncertainty-Aware Attention Mechanisms&rdquo;. What we specifically had in mind was to create a transformer model that used Bayesian Neural Networks (BNNs), and even more ambitiously, apply this to self-driving cars."><meta property="og:image" content="https://raw.githubusercontent.com/rezaarezvan/rezvan.xyz/main/images/icon.png"><meta property="og:url" content="https://rezvan.xyz/posts/claudeslens/"><meta property="og:site_name" content="rezvan"><meta property="og:title" content="My BSc Thesis, ClaudesLens"><meta property="og:description" content="Introduction I have finally finished undergrad and would like to make a blog post about what I have been working on these past ~6 months. The tile of our thesis is:
ClaudesLens: Uncertainty Quantification in Computer Vision Models
However, before I dive into the project and what we actually did, let me tell you what we wanted to do.
BayesLens Originally, we wanted to create “Uncertainty-Aware Attention Mechanisms”. What we specifically had in mind was to create a transformer model that used Bayesian Neural Networks (BNNs), and even more ambitiously, apply this to self-driving cars."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-18T21:03:25+02:00"><meta property="article:modified_time" content="2024-06-19T23:05:41+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="My BSc Thesis, ClaudesLens"><meta name=twitter:description content="Introduction I have finally finished undergrad and would like to make a blog post about what I have been working on these past ~6 months. The tile of our thesis is:
ClaudesLens: Uncertainty Quantification in Computer Vision Models
However, before I dive into the project and what we actually did, let me tell you what we wanted to do.
BayesLens Originally, we wanted to create “Uncertainty-Aware Attention Mechanisms”. What we specifically had in mind was to create a transformer model that used Bayesian Neural Networks (BNNs), and even more ambitiously, apply this to self-driving cars."><link rel=stylesheet href=https://rezvan.xyz/css/combined.min.51ec65976b416262d827f98c76ca037bf7aeea27590e3c120eac32afbfbc40ee.css integrity="sha256-Uexll2tBYmLYJ/mMdsoDe/eu6idZDjwSDqwyr7+8QO4="><link id=lightSyntaxStyle rel=stylesheet href=https://rezvan.xyz/css/light_syntax.min.d9e0828a4ff7f2d7317942062fc751fa487b2ac2c47b934ad082abd7d3ca6690.css integrity="sha256-2eCCik/38tcxeUIGL8dR+kh7KsLEe5NK0IKr19PKZpA="><link id=darkModeStyle rel=stylesheet href=https://rezvan.xyz/css/dark.min.49ad20f2859f81550f852c48875ca9e72e4267459ed6fe82fc9b4f3cf7fdc4e8.css integrity="sha256-Sa0g8oWfgVUPhSxIh1yp5y5CZ0We1v6C/JtPPPf9xOg=" disabled><link id=darkSyntaxStyle rel=stylesheet href=https://rezvan.xyz/css/dark_syntax.min.1a878f3d8fb43359bd3b44bc70c4074f682f11066c6537bf6248b696cbe56586.css integrity="sha256-GoePPY+0M1m9O0S8cMQHT2gvEQZsZTe/Yki2lsvlZYY=" disabled><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body><div class=content><header><nav id=site-navbar><div class=navbar-content><a href=https://rezvan.xyz/ class=logo>rezvan.xyz</a><div class=navbar-links><a href=/principles class=nav-link><span class=bracket>[</span>principles<span class=bracket>]</span></a>
<a href=/cv class=nav-link><span class=bracket>[</span>cv<span class=bracket>]</span></a>
<a href=/posts class=nav-link><span class=bracket>[</span>posts<span class=bracket>]</span></a>
<a href=/school class=nav-link><span class=bracket>[</span>school<span class=bracket>]</span></a></div><div class=theme-toggle><span id=dark-mode-toggle onclick=toggleTheme() aria-label="Toggle theme">○
</span><script src=https://rezvan.xyz/js/themetoggle.js></script></div></div></nav></header><main><article><div class=title><h1>My BSc Thesis, ClaudesLens</h1><div class=meta>Posted on Jun 18, 2024</div><div class=meta>(Last updated: Jun 19, 2024)</div></div><section class=body><h1 id=introduction>Introduction</h1><p>I have finally finished undergrad and would like to make a blog post about what I have been working on these past ~6 months.
The tile of our thesis is:</p><blockquote><p>ClaudesLens: Uncertainty Quantification in Computer Vision Models</p></blockquote><p>However, before I dive into the project and what we actually did, let me tell you what we <em>wanted</em> to do.</p><h1 id=bayeslens>BayesLens</h1><p>Originally, we wanted to create &ldquo;Uncertainty-Aware Attention Mechanisms&rdquo;.
What we specifically had in mind was to create a transformer model that used Bayesian Neural Networks (BNNs), and even more ambitiously, apply this to self-driving cars.</p><p>Needless to say, this was a bit too ambitious for a BSc thesis, so we had to scale down our project a bit.
We didn&rsquo;t have the prerequisite knowledge or the compute to do such a task within that time frame and with other courses.</p><p>So about ~1/3 into the project, when our supervisor wanted us to explore the <em>entropy</em> of predictions and got really excited about our results, we got <strong>ClaudesLens</strong>.</p><h1 id=claudeslens>ClaudesLens</h1><p>From the results using entropy as a measure of uncertainty, we decided to focus on this instead.
I&rsquo;ll go into more detail and motivate how this approach works, but believe that this is a very natural way to quantify uncertainty.</p><p>I plan to explain this project from the ground up, from first principles so to say, so let&rsquo;s start what lies at the heart of this project: <strong>Neural Networks</strong>.</p><h1 id=neural-networks>Neural Networks</h1><p>There are many ways to explain neural networks, in this post I will use a mathematical approach which will let us view the entire network as a single function.</p><h3 id=the-neuron>The Neuron</h3><p>At the core of a neural network lies the neuron, which is inspired by the biological neuron.</p><p>Each neuron takes in one or more scalars, $x_j$, as input and outputs a single scalar, $y$.
Each input, $x_j$, is scaled by an associated weight denoted as $w_j$. The neuron also has a special input called the bias, $b$.</p><p>The neuron has two stages it goes through, <strong>summation</strong> and <strong>activation</strong>.</p><p><img src=/posts/images/neuron.png alt=neuron>
<strong>Figure 1:</strong> A single neuron with <em>n</em> inputs and one output, showcasing the summation and activation components.</p><p>The summation stage is where the neuron calculates the weighted sum of the inputs and the bias:
$$
z = \sum_{j=1}^{n} w_j x_j + b
$$</p><p>The activation function, denoted as $f$, calculates the neuron’s output $y = f(z)$ based on the weighted summation.
Activation functions introduce non-linearity, enabling neural networks to approximate complex, non-linear functions.</p><h3 id=the-network>The Network</h3><p>Lets build upon what we now have learned and see how we can extend this.</p><p>We can represent the inputs of a neuron as a vector,
$$
\mathbf{x} = \left[x_1, x_2, \ldots, x_n\right],
$$</p><p>where each element corresponds to an input to the neuron.</p><p>Similarly, we can represent the associated weights as a vector,
$$
\mathbf{w} = \left[w_1, w_2, \ldots, w_n\right],
$$</p><p>with this the summation can be simplified to a dot product,
$$
z = \mathbf{w} \cdot \mathbf{x} + b.
$$</p><p>But only using one neuron will only get us so far, if we instead have multiple neurons and try to mimic the structure of the brain, we can get something more powerful.</p><p>A <em>layer</em> is a collection of neurons, stacked on top of each other.
Very often when we are referring to a layer, are we referring to a <em>fully connected layer</em>, where each neuron in the layer is connected to all the neurons in the previous layer.</p><p>In the case of a network, we can now talk about the input layer and the output layer.</p><p>&lt;TODO, need to think deeply about how to explain this properly, let&rsquo;s continue tomorrow:]></p><p><img src=/posts/images/nn.png alt=nn></p></section></article><nav class=navigation><div class="nav-item previous"></div><div class="nav-item next"></div></nav></main><footer id=site-footer><div class=social-links><a href=https://github.com/rezaarezvan title class=social-link><span class=bracket>[</span>github<span class=bracket>]</span></a>
<a href=https://x.com/rzvan__/ title class=social-link><span class=bracket>[</span>x<span class=bracket>]</span></a></div><div class=footer-marquee><div class=footer-marquee__content><span class=footer-marquee__item>memento mori • amor fati • sic parvis magna • per aspera ad astra</span>
<span class=footer-marquee__item>memento mori • amor fati • sic parvis magna • per aspera ad astra</span></div></div></footer></div></body></html>