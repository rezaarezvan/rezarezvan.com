<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Astro v4.11.5"><link rel="icon" type="image" href="/favicon.ico"><title>My BSc Thesis, ClaudesLens</title><!-- KaTeX support --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script><!-- inline KaTeX --><link rel="stylesheet" href="/_astro/index.B_-I1NWR.css">
<style>article[data-astro-cid-gjtny2mx]{max-width:80ch;margin:0 auto}
</style>
<link rel="stylesheet" href="/_astro/_slug_.J0rP0O4C.css"><script type="module">document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})});
</script></head> <body> <div class="container mx-auto px-4 flex flex-col md:flex-row min-h-screen"> <aside class="w-full md:w-64 border-b md:border-r md:border-b-0 border-[var(--border-color)] border-dashed pt-8"> <header class="flex flex-col h-full"> <div class="flex items-center mb-4"> <script>
  function setTheme(mode) {
    localStorage.setItem("theme-storage", mode);
    document.documentElement.setAttribute('data-theme', mode);
  }
  function toggleTheme() {
    const currentTheme = localStorage.getItem("theme-storage") || "light";
    const newTheme = currentTheme === "light" ? "dark" : "light";
    setTheme(newTheme);
  }
  const savedTheme = localStorage.getItem("theme-storage") || "light";
  setTheme(savedTheme);
  window.toggleTheme = toggleTheme;
</script> <button id="theme-toggle" onclick="toggleTheme()" aria-label="Toggle theme" class="flex items-center justify-center w-6" data-astro-cid-x3pjskd3>○</button>  <a href="/" class="text-2xl font-semibold ml-3 h-10 pr-3">rezvan.xyz</a> </div> <nav class="flex flex-wrap gap-2 md:flex-col md:gap-2"> <a href="/principles" class="hover:text-orange-500 dark:hover:text-orange-400 transition-colors">
[principles]
</a><a href="/cv" class="hover:text-orange-500 dark:hover:text-orange-400 transition-colors">
[cv]
</a><a href="/posts" class="hover:text-orange-500 dark:hover:text-orange-400 transition-colors">
[posts]
</a><a href="/school" class="hover:text-orange-500 dark:hover:text-orange-400 transition-colors">
[school]
</a> </nav> </header> </aside> <main class="flex-grow px-4 md:px-8 py-8 overflow-y-auto">  <article class="prose prose-sm sm:prose lg:prose-lg xl:prose-xl max-w-none px-4 sm:px-0" data-astro-cid-gjtny2mx> <h1 class="text-3xl sm:text-4xl font-bold mb-4" data-astro-cid-gjtny2mx>My BSc Thesis, ClaudesLens</h1> <p class="text-sm text-muted-foreground mb-4" data-astro-cid-gjtny2mx>
Date: 6/18/2024 </p> <div class="markdown-content" data-astro-cid-gjtny2mx>  <h1 id="introduction">Introduction</h1>
<p>I have finally finished undergrad and would like to make a blog post about what I have been working on these past ~6 months.
The tile of our thesis is:</p>
<blockquote>
<p>ClaudesLens: Uncertainty Quantification in Computer Vision Models</p>
</blockquote>
<p>However, before I dive into the project and what we actually did, let me tell you what we <em>wanted</em> to do.</p>
<h1 id="bayeslens">BayesLens</h1>
<p>Originally, we wanted to create “Uncertainty-Aware Attention Mechanisms”.
What we specifically had in mind was to create a transformer model that used Bayesian Neural Networks (BNNs), and even more ambitiously, apply this to self-driving cars.</p>
<p>Needless to say, this was a bit too ambitious for a BSc thesis, so we had to scale down our project a bit.
We didn’t have the prerequisite knowledge or the compute to do such a task within that time frame and with other courses.</p>
<p>So about ~1/3 into the project, when our supervisor wanted us to explore the <em>entropy</em> of predictions and got really excited about our results, we got <strong>ClaudesLens</strong>.</p>
<h1 id="claudeslens">ClaudesLens</h1>
<p>From the results using entropy as a measure of uncertainty, we decided to focus on this instead.
I’ll go into more detail and motivate how this approach works, but believe that this is a very natural way to quantify uncertainty.</p>
<p>I plan to explain this project from the ground up, from first principles so to say, so let’s start what lies at the heart of this project: <strong>Neural Networks</strong>.</p>
<h1 id="neural-networks">Neural Networks</h1>
<p>There are many ways to explain neural networks, in this post I will use a mathematical approach which will let us view the entire network as a single function.</p>
<h3 id="the-neuron">The Neuron</h3>
<p>At the core of a neural network lies the neuron, which is inspired by the biological neuron.</p>
<p>Each neuron takes in one or more scalars, $x_j$, as input and outputs a single scalar, $y$.
Each input, $x_j$, is scaled by an associated weight denoted as $w_j$. The neuron also has a special input called the bias, $b$.</p>
<p>The neuron has two stages it goes through, <strong>summation</strong> and <strong>activation</strong>.</p>
<p><img src="/images/posts/neuron.png" alt="neuron">
<strong>Figure 1:</strong> A single neuron with <em>n</em> inputs and one output, showcasing the summation and activation components.</p>
<p>The summation stage is where the neuron calculates the weighted sum of the inputs and the bias:
$$
z = \sum_{j=1}^{n} w_j x_j + b
$$</p>
<p>The activation function, denoted as $f$, calculates the neuron’s output $y = f(z)$ based on the weighted summation.
Activation functions introduce non-linearity, enabling neural networks to approximate complex, non-linear functions.</p>
<h3 id="the-network">The Network</h3>
<p>Lets build upon what we now have learned and see how we can extend this.</p>
<p>We can represent the inputs of a neuron as a vector,
$$
\mathbf{x} = \left[x_1, x_2, \ldots, x_n\right],
$$</p>
<p>where each element corresponds to an input to the neuron.</p>
<p>Similarly, we can represent the associated weights as a vector,
$$
\mathbf{w} = \left[w_1, w_2, \ldots, w_n\right],
$$</p>
<p>with this the summation can be simplified to a dot product,
$$
z = \mathbf{w} \cdot \mathbf{x} + b.
$$</p>
<p>But only using one neuron will only get us so far, if we instead have multiple neurons and try to mimic the structure of the brain, we can get something more powerful.</p>
<p>A <em>layer</em> is a collection of neurons, stacked on top of each other.
Very often when we are referring to a layer, are we referring to a <em>fully connected layer</em>, where each neuron in the layer is connected to all the neurons in the previous layer.</p>
<p>In the case of a network, we can now talk about the input layer and the output layer.</p>
<p>&#x3C;TODO, need to think deeply about how to explain this properly, let’s continue tomorrow:]></p>
<p><img src="/images/posts/nn.png" alt="nn"></p>  </div> </article>  </main> </div> </body></html> 