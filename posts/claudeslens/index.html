<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Astro v4.11.5"><link rel="icon" type="image" href="/favicon.ico"><title>My BSc Thesis, ClaudesLens</title><!-- KaTeX support --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script><!-- inline KaTeX --><link rel="stylesheet" href="/_astro/index.CwgzIfsj.css">
<style>article[data-astro-cid-gjtny2mx]{max-width:60ch;margin:0 auto}
</style>
<link rel="stylesheet" href="/_astro/_slug_.hCvEQTvV.css"><script type="module">document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})});
</script></head> <body> <div class="container mx-auto px-4 flex flex-col md:flex-row min-h-screen"> <aside class="w-full md:w-64 border-b md:border-r md:border-b-0 border-[var(--border-color)] border-dashed pt-8"> <header class="flex flex-col h-full"> <div class="flex items-center mb-4"> <script>
  function setTheme(mode) {
    localStorage.setItem("theme-storage", mode);
    document.documentElement.setAttribute('data-theme', mode);
  }
  function toggleTheme() {
    const currentTheme = localStorage.getItem("theme-storage") || "light";
    const newTheme = currentTheme === "light" ? "dark" : "light";
    setTheme(newTheme);
  }
  const savedTheme = localStorage.getItem("theme-storage") || "light";
  setTheme(savedTheme);
  window.toggleTheme = toggleTheme;
</script> <button id="theme-toggle" onclick="toggleTheme()" aria-label="Toggle theme" class="w-6 h-6 cursor-pointer"> <div class="w-5 h-5 border-2 border-primary rounded-full transition-colors duration-300 ease-in-out hover:bg-primary"></div> </button> <a href="/" class="text-2xl font-semibold ml-3 h-10 pr-3">rezvan.xyz</a> </div> <nav class="flex flex-wrap gap-2 md:flex-col md:gap-2"> <a href="/principles" class="transition-colors">
[principles]
</a><a href="/cv" class="transition-colors">
[cv]
</a><a href="/posts" class="transition-colors">
[posts]
</a><a href="/chalmers" class="transition-colors">
[chalmers]
</a><a href="/cityu" class="transition-colors">
[cityu]
</a> </nav> </header> </aside> <main class="flex-grow px-4 md:px-8 py-8 overflow-y-auto">  <article class="prose prose-sm sm:prose lg:prose-lg xl:prose-xl max-w-none px-4 sm:px-0" data-astro-cid-gjtny2mx> <h1 class="text-3xl sm:text-4xl font-bold mb-4" data-astro-cid-gjtny2mx>My BSc Thesis, ClaudesLens</h1> <p class="text-sm text-muted-foreground mb-4" data-astro-cid-gjtny2mx>
Date: 6/19/2024 </p> <div class="markdown-content" data-astro-cid-gjtny2mx>  <h1 id="introduction">Introduction</h1>
<p>I have finally finished undergrad and would like to make a blog post about what I have been working on these past ~6 months.
The tile of our thesis is:</p>
<blockquote>
<p>ClaudesLens: Uncertainty Quantification in Computer Vision Models</p>
</blockquote>
<p>Which you can read <a href="https://arxiv.org/abs/2406.13008">here</a></p>
<p>However, before I dive into the project and what we actually did, let me tell you what we <em>wanted</em> to do.</p>
<h1 id="bayeslens">BayesLens</h1>
<p>Originally, we wanted to create “Uncertainty-Aware Attention Mechanisms”.
What we specifically had in mind was to create a transformer model that used Bayesian Neural Networks (BNNs), and even more ambitiously, apply this to self-driving cars.</p>
<p>Needless to say, this was a bit too ambitious for a BSc thesis, so we had to scale down our project a bit.
We didn’t have the prerequisite knowledge or the compute to do such a task within that time frame and with other courses.</p>
<p>So about ~1/3 into the project, when our supervisor wanted us to explore the <em>entropy</em> of predictions and got really excited about our results, we got <strong>ClaudesLens</strong>.</p>
<h1 id="claudeslens">ClaudesLens</h1>
<p>From the results using entropy as a measure of uncertainty, we decided to focus on this instead.
I’ll go into more detail and motivate how this approach works, but believe that this is a very natural way to quantify uncertainty.</p>
<p>I plan to explain this project from the ground up, from first principles so to say, so let’s start what lies at the heart of this project: <strong>Neural Networks</strong>.</p>
<h1 id="neural-networks">Neural Networks</h1>
<p>There are many ways to explain neural networks, in this post I will use a mathematical approach which will let us view the entire network as a single function.</p>
<h3 id="the-neuron">The Neuron</h3>
<p>At the core of a neural network lies the neuron, which is inspired by the biological neuron.</p>
<p>Each neuron takes in one or more scalars, $x_j$, as input and outputs a single scalar, $y$.
Each input, $x_j$, is scaled by an associated weight denoted as $w_j$. The neuron also has a special input called the bias, $b$.</p>
<p>The neuron has two stages it goes through, <strong>summation</strong> and <strong>activation</strong>.</p>
<p><img src="/images/posts/neuron.png" alt="neuron">
<strong>Figure 1:</strong> A single neuron with <em>n</em> inputs and one output, showcasing the summation and activation components.</p>
<p>The summation stage is where the neuron calculates the weighted sum of the inputs and the bias:
$$
z = \sum_{j=1}^{n} w_j x_j + b
$$</p>
<p>The activation function, denoted as $f$, calculates the neuron’s output $y = f(z)$ based on the weighted summation.
Activation functions introduce non-linearity, enabling neural networks to approximate complex, non-linear functions.</p>
<h3 id="the-network">The Network</h3>
<p>Lets build upon what we now have learned and see how we can extend this.</p>
<p>We can represent the inputs of a neuron as a vector,
$$
\mathbf{x} = \left[x_1, x_2, \ldots, x_n\right],
$$</p>
<p>where each element corresponds to an input to the neuron.</p>
<p>Similarly, we can represent the associated weights as a vector,
$$
\mathbf{w} = \left[w_1, w_2, \ldots, w_n\right],
$$</p>
<p>with this the summation can be simplified to a dot product,
$$
z = \mathbf{w} \cdot \mathbf{x} + b.
$$</p>
<p>But only using one neuron will only get us so far, if we instead have multiple neurons and try to mimic the structure of the brain, we can get something more powerful.</p>
<p>A <em>layer</em> is a collection of neurons, stacked on top of each other.
Very often when we are referring to a layer, are we referring to a <em>fully connected layer</em>, where each neuron in the layer is connected to all the neurons in the previous layer.</p>
<p>In the case of a network, we can now talk about the input layer and the output layer.</p>
<p><img src="/images/posts/nn.png" alt="nn">
<strong>Figure 2:</strong> A simple neural network with one hidden layer.</p>
<p>As we see in the picture, we now have multiple neurons with numerous inter-neuron connections, along with multiple outputs.</p>
<p>The matrix-vector equation,
$$
\mathbf{a} = \mathbf{W_1} \mathbf{x} + \mathbf{b_1} = [a_1, a_2, \ldots, a_m],
$$</p>
<p>yields each output of each neuron in the hidden layer.</p>
<p>$\mathbf{W_1}$ is the <em>weight matrix</em> with rows $\mathbf{w_i} = [w_{i, 1}, w_{i, 2}, \ldots, w_{i, n}]$ corresponding to the weights of the $i$-th neuron in the hidden layer.
The bias values are represented by $\mathbf{b_1} = [b_1, b_2, \ldots, b_m]$.</p>
<p>In the case of several layers, we work with multiple weight matrices and bias vectors, which we index as $\mathbf{W_j}$ and $\mathbf{b_j}$, respectively.</p>
<p>So, given an input $\mathbf{x}$, the output of the hidden layer (i.e figure 2) is given by,
$$
\mathbf{a} = f.(\mathbf{W_1} \mathbf{x} + \mathbf{b_1}),
$$</p>
<p>where the dot indicates that the activation function $f$ is applied element-wise.</p>
<p>So the final output is therefore,
$$
\mathbf{y} = f.(\mathbf{W_2} \mathbf{a} + \mathbf{b_2}).
$$</p>
<p>This is the basic structure of a neural network, for the sake of brevity I will not go into more detail about the <em>training</em> process, but I will mention that these weights and biases are learned through an optimization process called <em>backpropagation</em>.</p>
<p>There are a ton of resources to understand these concepts, even we tried to explain these concepts in our thesis.</p>
<h3 id="computer-vision">Computer Vision</h3>
<p>Now that we have a basic understanding of neural networks, we can move on to computer vision.</p>
<p>Computer vision is a field of computer science that focuses on replicating parts of the complexity of the human vision system and enabling computers to identify and process objects in images and videos in the same way that humans do.</p>
<p>The most important thing that we will cover here is how we represent images, which is crucial for understanding how we can apply neural networks to images.</p>
<h3 id="images">Images</h3>
<p>Images are represented as a grid of pixels, where each pixel needs to be represented in a numerical way.</p>
<p>For most images, we represent each pixel as a 3-dimensional vector, where each element corresponds to the intensity of the color channels red, green, and blue (RGB). This is called a <em>channel</em>.</p>
<p>So, a single pixel in an image is represented as a vector, therefore a whole image can be represented as a 3-dimensional tensor.</p>
<p>Remember this.</p>
<h3 id="entropy-based-uncertainty-quantification-framework">Entropy-based Uncertainty Quantification Framework</h3>
<p>I’ll try to keep this quite short and sweet, again, in our paper we go into more detail, but from what we have seen, we can view a neural network as a function.</p>
<p>In our case, our model, since we’re dealing with classification, will spit out a probablity vector, and we always choose the class with the highest probability, ergo,
$$
\hat{y} = \arg\max(\mathcal{F}(\mathbf{x}, \mathbf{W})),
$$</p>
<p>where $\mathcal{F}$ is our <strong>trained</strong> neural network, $\mathbf{x}$ is our input, and $\mathbf{W}$ are the <strong>trained</strong> weights of the network.</p>
<p>But this is deterministic, there is no uncertainty here, so how can we quantify this?</p>
<p>We make it stochastic.</p>
<p>By adding <em>noise</em> to the function, random noise, it makes the function stochastic.
If we do this multiple times, we can get a distribution of outputs, and from this distribution, we can calculate the entropy.</p>
<h3 id="entropy-and-uncertainty">Entropy and Uncertainty</h3>
<p>Now, when we are talking about entropy, we are talking about the information kind of entropy.
Thanks to the great work of Claude Shannon, we have a way to quantify the uncertainty of a random variable.</p>
<p>The entropy of a random variable $X$ is defined as,
$$
H(X) = -\sum_{x \in \chi} p(x) \log p(x),
$$</p>
<p>where $p(x) = P(X = x)$.</p>
<h3 id="entropy-in-neural-networks">Entropy in Neural Networks</h3>
<p>So, if we have a neural network that outputs a probability distribution, we can calculate the entropy of this distribution.
The entropy of a distribution is a measure of the uncertainty of the distribution, the higher the entropy, the more uncertain we are about the output.
So, by adding noise to the function, we can calculate the entropy of the distribution of outputs, and this can give us a measure of the uncertainty of the model!</p>
<h3 id="conclusion">Conclusion</h3>
<p>This is a very brief overview of what we did in our thesis, and I hope that I have motivated why we did what we did.
I thought it was a very challenging but fun project, and I learned a lot from it.</p>
<p>Read the paper as well, we basically died writing it.</p>  </div> </article>  </main> </div> </body></html> 