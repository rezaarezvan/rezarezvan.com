<!DOCTYPE html><html class="bg-background text-foreground" lang="en"> <head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"><meta name="generator" content="Astro v5.16.1"><meta name="robots" content="index, follow"><meta name="HandheldFriendly" content="True"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="format-detection" content="telephone=no,date=no,address=no,email=no,url=no"><meta name="theme-color" content="#121212" media="(prefers-color-scheme: dark)"><meta name="theme-color" content="#121212" media="(prefers-color-scheme: light)"><link rel="sitemap" href="/sitemap-index.xml"><link rel="manifest" href="/site.webmanifest"><link rel="alternate" type="application/rss+xml" title="rezarezvan.com" href="https://rezarezvan.com/rss.xml"><!-- PageFind --><link href="/pagefind/pagefind-ui.css" rel="stylesheet"><script src="/pagefind/pagefind-ui.js"></script><!-- KaTeX support --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><!-- inline KaTeX --><script>
    function renderKaTeX() {
      if (typeof renderMathInElement !== 'undefined') {
        renderMathInElement(document.body, {
          delimiters: [
            { left: '$$', right: '$$', display: true },
            { left: '$', right: '$', display: false },
          ],
        })
      }
    }

    document.addEventListener('DOMContentLoaded', renderKaTeX)
    document.addEventListener('astro:after-swap', renderKaTeX)
  </script><link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><link rel="shortcut icon" href="/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><meta name="apple-mobile-web-app-title" content="rezvan-blog"><link rel="manifest" href="/site.webmanifest"><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><script type="module" src="/_astro/ClientRouter.astro_astro_type_script_index_0_lang.QW52Ox2j.js"></script><script>
    function init() {
      setGiscusTheme()
    }

    const setGiscusTheme = () => {
      const giscus = document.querySelector('.giscus-frame')

      const isDark = document.documentElement.classList.contains('dark')

      if (giscus) {
        const url = new URL(giscus.src)
        url.searchParams.set('theme', isDark ? 'dark' : 'light')
        giscus.src = url.toString()
      }
    }

    document.addEventListener('DOMContentLoaded', () => init())
    document.addEventListener('astro:after-swap', () => init())
  </script><title>Part 6 - MCMC for Bayesian Inference | rezarezvan.com</title><meta name="title" content="Part 6 - MCMC for Bayesian Inference | rezarezvan.com"><meta name="description" content="Personal website and course notes repository"><link rel="canonical" href="https://rezarezvan.com"><meta name="robots" content="noindex"><meta property="og:title" content="Part 6 - MCMC for Bayesian Inference"><meta property="og:description" content="Personal website and course notes repository"><meta property="og:image" content="https://rezarezvan.com/static/1200x630.png"><meta property="og:image:alt" content="Part 6 - MCMC for Bayesian Inference"><meta property="og:type" content="website"><meta property="og:locale" content="en"><meta property="og:site_name" content="rezarezvan.com"><meta property="og:url" content="https://rezarezvan.com/notes/master/mve550/mve550_6/"><meta name="twitter:title" content="Part 6 - MCMC for Bayesian Inference"><meta name="twitter:description" content="Personal website and course notes repository"><meta property="twitter:image" content="https://rezarezvan.com/static/1200x630.png"><meta name="twitter:image:alt" content="Part 6 - MCMC for Bayesian Inference"><meta name="twitter:card" content="summary_large_image"><link rel="stylesheet" href="/_astro/_slug_.CEkEc1oo.css"></head><body> <div class="flex h-fit min-h-screen flex-col gap-y-6 font-sans"> <div class="bg-background/50 sticky top-0 z-50 divide-y backdrop-blur-sm xl:divide-none"> <header data-astro-transition-persist="astro-l7r54iwe-1"> <div class="mx-auto flex max-w-3xl items-center justify-between gap-4 px-4 py-3"> <a href="/" target="_self" class="transition-colors duration-300 ease-in-out flex shrink-0 items-center justify-center gap-3">  <span class="hidden h-full text-lg font-medium min-[300px]:block">rezarezvan.com</span>  </a> <div class="flex items-center sm:gap-4"> <nav class="hidden items-center gap-4 text-sm sm:flex sm:gap-6"> <a href="/blog" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> blog<span>/</span>  </a><a href="/notes" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> notes<span>/</span>  </a><a href="/dump" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> dump<span>/</span>  </a><a href="/research" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> research<span>/</span>  </a> </nav> <button id="magnifying-glass" aria-label="Search" class="flex items-center px-2 text-sm transition-colors duration-300 ease-in-out hover:rounded hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg height="16" stroke-linejoin="round" viewBox="0 0 16 16" width="16" style="color: currentcolor;"> <path fill-rule="evenodd" clip-rule="evenodd" d="M3.5 7C3.5 5.067 5.067 3.5 7 3.5C8.933 3.5 10.5 5.067 10.5 7C10.5 7.88461 10.1718 8.69256 9.63058 9.30876L9.30876 9.63058C8.69256 10.1718 7.88461 10.5 7 10.5C5.067 10.5 3.5 8.933 3.5 7ZM9.96544 11.0261C9.13578 11.6382 8.11014 12 7 12C4.23858 12 2 9.76142 2 7C2 4.23858 4.23858 2 7 2C9.76142 2 12 4.23858 12 7C12 8.11014 11.6382 9.13578 11.0261 9.96544L14.0303 12.9697L14.5607 13.5L13.5 14.5607L12.9697 14.0303L9.96544 11.0261Z" fill="currentColor"></path> </svg>
&nbsp;Search
</button> <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).load=e;window.dispatchEvent(new Event("astro:load"));})();</script><script>(()=>{var A=Object.defineProperty;var g=(i,o,a)=>o in i?A(i,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):i[o]=a;var d=(i,o,a)=>g(i,typeof o!="symbol"?o+"":o,a);{let i={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t),11:t=>1/0*t},o=t=>{let[l,e]=t;return l in i?i[l](e):void 0},a=t=>t.map(o),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([l,e])=>[l,o(e)]));class y extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var b;if(!this.hydrator||!this.isConnected)return;let e=(b=this.parentElement)==null?void 0:b.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let c=this.querySelectorAll("astro-slot"),n={},h=this.querySelectorAll("template[data-astro-template]");for(let r of h){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(let r of c){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("name")||"default"]=r.innerHTML)}let p;try{p=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(r){let s=this.getAttribute("component-url")||"<unknown>",v=this.getAttribute("component-export");throw v&&(s+=` (export ${v})`),console.error(`[hydrate] Error parsing props for component ${s}`,this.getAttribute("props"),r),r}let u;await this.hydrator(this)(this.Component,p,n,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),c.disconnect(),this.childrenConnectedCallback()},c=new MutationObserver(()=>{var n;((n=this.lastChild)==null?void 0:n.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});c.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),c=this.getAttribute("client");if(Astro[c]===void 0){window.addEventListener(`astro:${c}`,()=>this.start(),{once:!0});return}try{await Astro[c](async()=>{let n=this.getAttribute("renderer-url"),[h,{default:p}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),u=this.getAttribute("component-export")||"default";if(!u.includes("."))this.Component=h[u];else{this.Component=h;for(let f of u.split("."))this.Component=this.Component[f]}return this.hydrator=p,this.hydrate},e,this)}catch(n){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,n)}}attributeChangedCallback(){this.hydrate()}}d(y,"observedAttributes",["props"]),customElements.get("astro-island")||customElements.define("astro-island",y)}})();</script><astro-island uid="1f72gE" prefix="r18" component-url="/_astro/mobile-menu.DvFCA0M9.js" component-export="default" renderer-url="/_astro/client.DXIZDqlh.js" props="{&quot;data-astro-transition-persist&quot;:[0,&quot;astro-iq5tym4z-2&quot;]}" ssr client="load" opts="{&quot;name&quot;:&quot;MobileMenu&quot;,&quot;value&quot;:true}" data-astro-transition-persist="astro-iq5tym4z-2" await-children><button data-slot="dropdown-menu-trigger" class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 size-9 md:hidden" title="Menu" type="button" id="radix-:r18R0:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu h-5 w-5"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg><span class="sr-only">Toggle menu</span></button><!--astro:end--></astro-island> <button data-slot="button" class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 size-9" id="theme-toggle" title="Toggle theme"> <svg width="1em" height="1em" class="size-4 scale-100 rotate-0 transition-all dark:scale-0 dark:-rotate-90" data-icon="lucide:sun">   <symbol id="ai:lucide:sun" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><circle cx="12" cy="12" r="4"/><path d="M12 2v2m0 16v2M4.93 4.93l1.41 1.41m11.32 11.32l1.41 1.41M2 12h2m16 0h2M6.34 17.66l-1.41 1.41M19.07 4.93l-1.41 1.41"/></g></symbol><use href="#ai:lucide:sun"></use>  </svg> <svg width="1em" height="1em" class="absolute size-4 scale-0 rotate-90 transition-all dark:scale-100 dark:rotate-0" data-icon="lucide:moon">   <symbol id="ai:lucide:moon" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.985 12.486a9 9 0 1 1-9.473-9.472c.405-.022.617.46.402.803a6 6 0 0 0 8.268 8.268c.344-.215.825-.004.803.401"/></symbol><use href="#ai:lucide:moon"></use>  </svg> <span class="sr-only">Toggle theme</span> </button> <script data-astro-rerun>
  const theme = (() => {
    const localStorageTheme = localStorage?.getItem('theme') ?? ''
    if (['dark', 'light'].includes(localStorageTheme)) {
      return localStorageTheme
    }
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
      return 'dark'
    }
    return 'light'
  })()

  document.documentElement.setAttribute('data-theme', theme)
  document.documentElement.classList.add(
    theme === 'dark' ? 'scheme-dark' : 'scheme-light',
  )
  window.localStorage.setItem('theme', theme)
</script> <script type="module">function a(){const e=document.documentElement,n=e.getAttribute("data-theme")==="dark"?"light":"dark";e.classList.add("[&_*]:transition-none"),e.setAttribute("data-theme",n),e.classList.remove("scheme-dark","scheme-light"),e.classList.add(n==="dark"?"scheme-dark":"scheme-light"),window.getComputedStyle(e).getPropertyValue("opacity"),requestAnimationFrame(()=>{e.classList.remove("[&_*]:transition-none")}),localStorage.setItem("theme",n)}function s(){const e=document.getElementById("theme-toggle");e&&e.addEventListener("click",a)}s();document.addEventListener("astro:after-swap",()=>{const e=localStorage.getItem("theme")||"light",t=document.documentElement;t.classList.add("[&_*]:transition-none"),window.getComputedStyle(t).getPropertyValue("opacity"),t.setAttribute("data-theme",e),t.classList.remove("scheme-dark","scheme-light"),t.classList.add(e==="dark"?"scheme-dark":"scheme-light"),requestAnimationFrame(()=>{t.classList.remove("[&_*]:transition-none")}),s()});</script> </div> </div> </header> <div id="mobile-toc-container" class="w-full xl:hidden"><details class="group"><summary class="flex w-full cursor-pointer items-center justify-between"><div class="mx-auto flex w-full max-w-3xl items-center px-4 py-3"><div class="relative mr-2 size-4"><svg class="h-4 w-4" viewBox="0 0 24 24"><circle class="text-primary/20" cx="12" cy="12" r="10" fill="none" stroke="currentColor" stroke-width="2"></circle><circle id="mobile-toc-progress-circle" class="text-primary" cx="12" cy="12" r="10" fill="none" stroke="currentColor" stroke-width="2" stroke-dasharray="62.83" stroke-dashoffset="62.83" transform="rotate(-90 12 12)"></circle></svg></div><span id="mobile-toc-current-section" class="text-muted-foreground flex-grow truncate text-sm">
Overview
</span><span class="text-muted-foreground ml-2"><svg width="1em" height="1em" class="h-4 w-4 transition-transform duration-200 group-open:rotate-180" data-icon="lucide:chevron-down">   <symbol id="ai:lucide:chevron-down" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m6 9l6 6l6-6"/></symbol><use href="#ai:lucide:chevron-down"></use>  </svg></span></div></summary><astro-island uid="GL8zI" prefix="r25" component-url="/_astro/scroll-area.BdR3V4Yj.js" component-export="ScrollArea" renderer-url="/_astro/client.DXIZDqlh.js" props="{&quot;className&quot;:[0,&quot;mx-auto max-w-3xl&quot;],&quot;data-toc-header-scroll&quot;:[0,true]}" ssr client="load" opts="{&quot;name&quot;:&quot;ScrollArea&quot;,&quot;value&quot;:true}" await-children><div dir="ltr" data-slot="scroll-area" class="relative mx-auto max-w-3xl" data-toc-header-scroll="true" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px"><style>[data-radix-scroll-area-viewport]{scrollbar-width:none;-ms-overflow-style:none;-webkit-overflow-scrolling:touch;}[data-radix-scroll-area-viewport]::-webkit-scrollbar{display:none}</style><div data-radix-scroll-area-viewport="" data-slot="scroll-area-viewport" class="ring-ring/10 dark:ring-ring/20 dark:outline-ring/40 outline-ring/50 size-full rounded-[inherit] transition-[color,box-shadow] focus-visible:ring-4 focus-visible:outline-1" style="overflow-x:hidden;overflow-y:hidden"><div style="min-width:100%;display:table"><astro-slot><div class="max-h-[30vh]"><ul class="flex list-none flex-col gap-y-2 px-4 pb-4" id="mobile-table-of-contents"><li class="px-4 text-sm text-foreground/60"><a href="#introduction" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="introduction">Introduction</a></li><li class="px-4 text-sm text-foreground/60"><a href="#discrete-time-markov-chains-with-continuous-state-space" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="discrete-time-markov-chains-with-continuous-state-space">Discrete-Time Markov Chains with Continuous State Space</a></li><li class="px-4 text-sm ml-4 text-foreground/60"><a href="#bayesian-inference" class="mobile-toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="bayesian-inference">Bayesian Inference</a></li></ul></div></astro-slot></div></div></div><!--astro:end--></astro-island></details></div><script type="module" src="/_astro/TOCHeader.astro_astro_type_script_index_0_lang.CKMLAwWj.js"></script>   </div> <main class="grow"> <div class="mx-auto flex grow flex-col gap-y-6 px-4">   <section class="grid grid-cols-[minmax(0px,1fr)_min(calc(var(--breakpoint-md)-2rem),100%)_minmax(0px,1fr)] gap-y-6"> <div class="col-start-2"> <nav aria-label="breadcrumb" data-slot="breadcrumb"> <ol data-slot="breadcrumb-list" class="text-muted-foreground flex flex-wrap items-center gap-1.5 text-sm break-words sm:gap-2.5"> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"> <a data-slot="breadcrumb-link" class="hover:text-foreground transition-colors" href="/"> <svg width="1em" height="1em" class="size-4 shrink-0" data-icon="lucide:home">   <symbol id="ai:lucide:home" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 21v-8a1 1 0 0 0-1-1h-4a1 1 0 0 0-1 1v8"/><path d="M3 10a2 2 0 0 1 .709-1.528l7-6a2 2 0 0 1 2.582 0l7 6A2 2 0 0 1 21 10v9a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"/></g></symbol><use href="#ai:lucide:home"></use>  </svg> </a> </li>  <li data-slot="breadcrumb-separator" role="presentation" aria-hidden="true" class="[&amp;&gt;svg]:size-3.5"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></li> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"><a data-slot="breadcrumb-link" class="hover:text-foreground transition-colors" href="/notes/"> <span class="flex items-center gap-x-2"> <svg width="1em" height="1em" class="size-4" data-icon="lucide:graduation-cap">   <symbol id="ai:lucide:graduation-cap" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M21.42 10.922a1 1 0 0 0-.019-1.838L12.83 5.18a2 2 0 0 0-1.66 0L2.6 9.08a1 1 0 0 0 0 1.832l8.57 3.908a2 2 0 0 0 1.66 0zM22 10v6"/><path d="M6 12.5V16a6 3 0 0 0 12 0v-3.5"/></g></symbol><use href="#ai:lucide:graduation-cap"></use>  </svg> Master </span> </a></li>  <li data-slot="breadcrumb-separator" role="presentation" aria-hidden="true" class="[&amp;&gt;svg]:size-3.5"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></li> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"><a data-slot="breadcrumb-link" class="hover:text-foreground transition-colors" href="/notes/master/mve550"> <span class="flex items-center gap-x-2"> <svg width="1em" height="1em" class="size-4" data-icon="lucide:book-open">   <symbol id="ai:lucide:book-open" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 7v14m-9-3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4a4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3a3 3 0 0 0-3-3z"/></symbol><use href="#ai:lucide:book-open"></use>  </svg> Stochastic Processes And Bayesian Inference </span> </a></li>  <li data-slot="breadcrumb-separator" role="presentation" aria-hidden="true" class="[&amp;&gt;svg]:size-3.5"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></li> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"><span data-slot="breadcrumb-page" role="link" aria-disabled="true" aria-current="page" class="text-foreground font-normal"> <span class="flex items-center gap-x-2"> <svg width="1em" height="1em" class="size-4 shrink-0" data-icon="lucide:file-text">   <symbol id="ai:lucide:file-text" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M6 22a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h8a2.4 2.4 0 0 1 1.704.706l3.588 3.588A2.4 2.4 0 0 1 20 8v12a2 2 0 0 1-2 2z"/><path d="M14 2v5a1 1 0 0 0 1 1h5M10 9H8m8 4H8m8 4H8"/></g></symbol><use href="#ai:lucide:file-text"></use>  </svg> <span>Part 6 - MCMC for Bayesian Inference</span> </span> </span></li> </ol> </nav> </div>  <section class="col-start-2 flex flex-col gap-y-6 text-center"> <div class="flex flex-col"> <h1 class="mb-2 scroll-mt-31 text-4xl leading-tight font-medium text-pretty" id="post-title"> Part 6 - MCMC for Bayesian Inference </h1> <div class="text-muted-foreground mb-4 flex flex-wrap items-center justify-center gap-2 text-sm"> <div class="flex items-center gap-2"> <span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground">MVE550</span> <div data-orientation="vertical" role="none" data-slot="separator-root" class="bg-border red shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px h-4!"></div> <span>Date: November 24, 2025</span> <div data-orientation="vertical" role="none" data-slot="separator-root" class="bg-border red shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px h-4!"></div>  <div class="font-base text-sm">
Last modified: Invalid Date </div>  <div data-orientation="vertical" role="none" data-slot="separator-root" class="bg-border red shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px h-4!"></div> <span>19 min read</span> </div> </div> </div> <nav class="col-start-2 grid grid-cols-1 gap-4 sm:grid-cols-2"> <a href="/notes/master/mve550/mve550_5" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-lg group flex items-center justify-start size-full" aria-disabled="false">  <svg width="1em" height="1em" class="mr-2 size-4 transition-transform group-hover:-translate-x-1" data-icon="lucide:arrow-left">   <symbol id="ai:lucide:arrow-left" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m12 19l-7-7l7-7m7 7H5"/></symbol><use href="#ai:lucide:arrow-left"></use>  </svg> <div class="flex flex-col items-start overflow-hidden text-wrap"> <span class="text-muted-foreground text-left text-xs"> Previous Post </span> <span class="w-full text-left text-sm text-balance text-ellipsis"> Part 5 - Branching Processes and MCMC </span> </div>  </a>  <a href="#" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-lg group flex items-center justify-end size-full pointer-events-none opacity-50 cursor-not-allowed" aria-disabled="true">  <div class="flex flex-col items-end overflow-hidden text-wrap"> <span class="text-muted-foreground text-right text-xs"> Next Post </span> <span class="w-full text-right text-sm text-balance text-ellipsis"> You&#39;re at the newest post! </span> </div> <svg width="1em" height="1em" class="ml-2 size-4 transition-transform group-hover:translate-x-1" data-icon="lucide:arrow-right">   <symbol id="ai:lucide:arrow-right" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 12h14m-7-7l7 7l-7 7"/></symbol><use href="#ai:lucide:arrow-right"></use>  </svg>  </a> </nav> </section> <div id="toc-sidebar-container" class="sticky top-20 col-start-1 row-span-1 mr-8 ml-auto hidden h-[calc(100vh-5rem)] max-w-md xl:block"><astro-island uid="qgoq4" prefix="r26" component-url="/_astro/scroll-area.BdR3V4Yj.js" component-export="ScrollArea" renderer-url="/_astro/client.DXIZDqlh.js" props="{&quot;className&quot;:[0,&quot;flex max-h-[calc(100vh-8rem)] flex-col overflow-y-auto&quot;],&quot;type&quot;:[0,&quot;hover&quot;],&quot;data-toc-scroll-area&quot;:[0,true]}" ssr client="load" opts="{&quot;name&quot;:&quot;ScrollArea&quot;,&quot;value&quot;:true}" await-children><div dir="ltr" data-slot="scroll-area" class="relative flex max-h-[calc(100vh-8rem)] flex-col overflow-y-auto" data-toc-scroll-area="true" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px"><style>[data-radix-scroll-area-viewport]{scrollbar-width:none;-ms-overflow-style:none;-webkit-overflow-scrolling:touch;}[data-radix-scroll-area-viewport]::-webkit-scrollbar{display:none}</style><div data-radix-scroll-area-viewport="" data-slot="scroll-area-viewport" class="ring-ring/10 dark:ring-ring/20 dark:outline-ring/40 outline-ring/50 size-full rounded-[inherit] transition-[color,box-shadow] focus-visible:ring-4 focus-visible:outline-1" style="overflow-x:hidden;overflow-y:hidden"><div style="min-width:100%;display:table"><astro-slot><div class="flex flex-col gap-2 px-4"><span class="text-lg font-medium">Table of Contents</span><ul class="flex list-none flex-col gap-y-2"><li class="text-sm text-foreground/60"><a href="#introduction" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="introduction">Introduction</a></li><li class="text-sm text-foreground/60"><a href="#discrete-time-markov-chains-with-continuous-state-space" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="discrete-time-markov-chains-with-continuous-state-space">Discrete-Time Markov Chains with Continuous State Space</a></li><li class="text-sm ml-4 text-foreground/60"><a href="#bayesian-inference" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="bayesian-inference">Bayesian Inference</a></li></ul></div></astro-slot></div></div></div><!--astro:end--></astro-island></div><script type="module">class f{links=document.querySelectorAll("[data-heading-link]");activeIds=[];headings=[];regions=[];scrollArea=null;tocScrollArea=null;reset(){this.links=document.querySelectorAll("#toc-sidebar-container [data-heading-link]"),this.activeIds=[],this.headings=[],this.regions=[];const t=document.getElementById("toc-sidebar-container");this.scrollArea=t?.querySelector("[data-radix-scroll-area-viewport]")||null,this.tocScrollArea=t?.querySelector("[data-toc-scroll-area]")||null}}const e=new f;class c{static build(){if(e.headings=Array.from(document.querySelectorAll(".prose h2, .prose h3, .prose h4, .prose h5, .prose h6")),e.headings.length===0){e.regions=[];return}e.regions=e.headings.map((t,o)=>{const i=e.headings[o+1];return{id:t.id,start:t.offsetTop,end:i?i.offsetTop:document.body.scrollHeight}})}static getVisibleIds(){if(e.headings.length===0)return[];const t=window.scrollY+80,o=window.scrollY+window.innerHeight,i=new Set,l=(s,r)=>s>=t&&s<=o||r>=t&&r<=o||s<=t&&r>=o;return e.headings.forEach(s=>{const r=s.offsetTop+s.offsetHeight;l(s.offsetTop,r)&&i.add(s.id)}),e.regions.forEach(s=>{if(s.start<=o&&s.end>=t){const r=document.getElementById(s.id);if(r){const a=r.offsetTop+r.offsetHeight;s.end>a&&(a<o||t<s.end)&&i.add(s.id)}}}),Array.from(i)}}class h{static update(){if(!e.scrollArea||!e.tocScrollArea)return;const{scrollTop:t,scrollHeight:o,clientHeight:i}=e.scrollArea,l=5,s=t<=l,r=t>=o-i-l;e.tocScrollArea.classList.toggle("mask-t-from-90%",!s),e.tocScrollArea.classList.toggle("mask-b-from-90%",!r)}}class g{static update(t){e.links.forEach(o=>{o.classList.remove("text-foreground")}),t.forEach(o=>{if(o){const i=document.querySelector(`#toc-sidebar-container [data-heading-link="${o}"]`);i&&i.classList.add("text-foreground")}}),this.scrollToActive(t)}static scrollToActive(t){if(!e.scrollArea||!t.length)return;const o=document.querySelector(`#toc-sidebar-container [data-heading-link="${t[0]}"]`);if(!o)return;const{top:i,height:l}=e.scrollArea.getBoundingClientRect(),{top:s,height:r}=o.getBoundingClientRect(),a=s-i+e.scrollArea.scrollTop,u=Math.max(0,Math.min(a-(l-r)/2,e.scrollArea.scrollHeight-e.scrollArea.clientHeight));Math.abs(u-e.scrollArea.scrollTop)>5&&(e.scrollArea.scrollTop=u)}}class d{static handleScroll(){const t=c.getVisibleIds();JSON.stringify(t)!==JSON.stringify(e.activeIds)&&(e.activeIds=t,g.update(e.activeIds))}static handleTOCScroll=()=>h.update();static handleResize(){c.build();const t=c.getVisibleIds();JSON.stringify(t)!==JSON.stringify(e.activeIds)&&(e.activeIds=t,g.update(e.activeIds)),h.update()}static init(){if(e.reset(),c.build(),e.headings.length===0){g.update([]);return}this.handleScroll(),setTimeout(h.update,100);const t={passive:!0};window.addEventListener("scroll",this.handleScroll,t),window.addEventListener("resize",this.handleResize,t),e.scrollArea?.addEventListener("scroll",this.handleTOCScroll,t)}static cleanup(){window.removeEventListener("scroll",this.handleScroll),window.removeEventListener("resize",this.handleResize),e.scrollArea?.removeEventListener("scroll",this.handleTOCScroll),Object.assign(e,{activeIds:[],headings:[],regions:[],scrollArea:null,tocScrollArea:null})}}document.addEventListener("astro:page-load",()=>d.init());document.addEventListener("astro:after-swap",()=>{d.cleanup(),d.init()});document.addEventListener("astro:before-swap",()=>d.cleanup());</script> <article class="prose col-start-2 max-w-none"> <!doctype html><html lang="en"><head></head><body>


<meta charset="utf-8">
<title>MVE550_6</title>
<meta content="width=device-width, initial-scale=1" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" rel="stylesheet">

<svg xmlns="http://www.w3.org/2000/svg" style="display:none"><defs>
        <symbol id="info" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="10"></circle><path d="M12 16v-4"></path><path d="M12 8h.01"></path>
        </symbol>
        <symbol id="lightbulb" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M15 14c.2-1 .7-1.7 1.5-2.5 1-.9 1.5-2.2 1.5-3.5A6 6 0 0 0 6 8c0 1 .2 2.2 1.5 3.5.7.7 1.3 1.5 1.5 2.5"></path><path d="M9 18h6"></path><path d="M10 22h4"></path>
        </symbol>
        <symbol id="alert-triangle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="m21.73 18-8-14a2 2 0 0 0-3.48 0l-8 14A2 2 0 0 0 4 21h16a2 2 0 0 0 1.73-3"></path><path d="M12 9v4"></path><path d="m12 17h.01"></path>
        </symbol>
        <symbol id="shield-alert" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M20 13c0 5-3.5 7.5-7.66 8.95a1 1 0 0 1-.67-.01C7.5 20.5 4 18 4 13V6a1 1 0 0 1 1-1c2 0 4.5-1.2 6.24-2.72a1.17 1.17 0 0 1 1.52 0C14.51 3.81 17 5 19 5a1 1 0 0 1 1 1z"></path><path d="M12 8v4"></path><path d="M12 16h.01"></path>
        </symbol>
        <symbol id="message-square-warning" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M22 17a2 2 0 0 1-2 2H6.828a2 2 0 0 0-1.414.586l-2.202 2.202A.71.71 0 0 1 2 21.286V5a2 2 0 0 1 2-2h16a2 2 0 0 1 2 2z"></path><path d="M12 15h.01"></path><path d="m12 17v4"></path>
        </symbol>
        <symbol id="book-open" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 7v14"></path><path d="M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z"></path>
        </symbol>
        <symbol id="anchor" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 22V8"></path><path d="M5 12H2a10 10 0 0 0 20 0h-3"></path><circle cx="12" cy="5" r="3"></circle>
        </symbol>
        <symbol id="pen-tool" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M15.707 21.293a1 1 0 0 1-1.414 0l-1.586-1.586a1 1 0 0 1 0-1.414l5.586-5.586a1 1 0 0 1 1.414 0l1.586 1.586a1 1 0 0 1 0 1.414z"></path><path d="m18 13-1.375-6.874a1 1 0 0 0-.746-.776L3.235 2.028a1 1 0 0 0-1.207 1.207L5.35 15.879a1 1 0 0 0 .776.746L13 18"></path><path d="m2.3 2.3 7.286 7.286"></path><circle cx="11" cy="11" r="2"></circle>
        </symbol>
        <symbol id="check-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="10"></circle><path d="m9 12 2 2 4-4"></path>
        </symbol>
        <symbol id="puzzle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M15.39 4.39a1 1 0 0 0 1.68-.474 2.5 2.5 0 1 1 3.014 3.015 1 1 0 0 0-.474 1.68l1.683 1.682a2.414 2.414 0 0 1 0 3.414L19.61 15.39a1 1 0 0 1-1.68-.474 2.5 2.5 0 1 0-3.014 3.015 1 1 0 0 1 .474 1.68l-1.683 1.682a2.414 2.414 0 0 1-3.414 0L8.61 19.61a1 1 0 0 0-1.68.474 2.5 2.5 0 1 1-3.014-3.015 1 1 0 0 0 .474-1.68l-1.683-1.682a2.414 2.414 0 0 1 0-3.414L4.39 8.61a1 1 0 0 1 1.68.474 2.5 2.5 0 1 0 3.014-3.015 1 1 0 0 1-.474-1.68l1.683-1.682a2.414 2.414 0 0 1 3.414 0z"></path>
        </symbol>
        <symbol id="git-branch" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <line x1="6" x2="6" y1="3" y2="15"></line><circle cx="18" cy="6" r="3"></circle><circle cx="6" cy="18" r="3"></circle><path d="M18 9a9 9 0 0 1-9 9"></path>
        </symbol>
        <symbol id="file-text" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path>
        </symbol>
        <symbol id="help-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="10"></circle><path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3"></path><path d="M12 17h.01"></path>
        </symbol>
        <symbol id="check-square" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <rect width="18" height="18" x="3" y="3" rx="2"></rect><path d="m9 12 2 2 4-4"></path>
        </symbol>
        <symbol id="message-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M2.992 16.342a2 2 0 0 1 .094 1.167l-1.065 3.29a1 1 0 0 0 1.236 1.168l3.413-.998a2 2 0 0 1 1.099.092 10 10 0 1 0-4.777-4.719"></path>
        </symbol>
        <symbol id="rotate-ccw" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M3 12a9 9 0 1 0 9-9 9.75 9.75 0 0 0-6.74 2.74L3 8"></path><path d="M3 3v5h5"></path>
        </symbol>
        <symbol id="code" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="m16 18 6-6-6-6"></path><path d="m8 6-6 6 6 6"></path>
        </symbol>
        <symbol id="dumbbell" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M17.596 12.768a2 2 0 1 0 2.829-2.829l-1.768-1.767a2 2 0 0 0 2.828-2.829l-2.828-2.828a2 2 0 0 0-2.829 2.828l-1.767-1.768a2 2 0 1 0-2.829 2.829z"></path><path d="m2.5 21.5 1.4-1.4"></path><path d="m20.1 3.9 1.4-1.4"></path><path d="M5.343 21.485a2 2 0 1 0 2.829-2.828l1.767 1.768a2 2 0 1 0 2.829-2.829l-6.364-6.364a2 2 0 1 0-2.829 2.829l1.768 1.767a2 2 0 0 0-2.828 2.829z"></path><path d="m9.6 14.4 4.8-4.8"></path>
        </symbol>
        <symbol id="alert-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="12" cy="12" r="10"></circle><line x1="12" x2="12" y1="8" y2="12"></line><line x1="12" x2="12.01" y1="16" y2="16"></line>
        </symbol>
        <symbol id="check" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M20 6 9 17l-5-5"></path>
        </symbol>
        <symbol id="check-circle-2" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="m9 12 2 2 4-4"></path><circle cx="12" cy="12" r="9"></circle>
        </symbol>
        <symbol id="list" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M3 5h.01"></path><path d="M3 12h.01"></path><path d="M3 19h.01"></path><path d="M8 5h13"></path><path d="M8 12h13"></path><path d="M8 19h13"></path>
        </symbol>
        <symbol id="chevron-down" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="m6 9 6 6 6-6"></path>
        </symbol>
        <symbol id="cpu" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 20v2"></path><path d="M12 2v2"></path><path d="M17 20v2"></path><path d="M17 2v2"></path><path d="M2 12h2"></path><path d="M2 17h2"></path><path d="M2 7h2"></path><path d="M20 12h2"></path><path d="M20 17h2"></path><path d="M20 7h2"></path><path d="M7 20v2"></path><path d="M7 2v2"></path><rect x="4" y="4" width="16" height="16" rx="2"></rect><rect x="8" y="8" width="8" height="8" rx="1"></rect>
        </symbol></defs></svg>
<!-- ## Introduction -->
<!-- In this part, we will introduce branching processes, their properties, some useful theorems, and applications. -->
<!-- Then, we will introduce Markov Chain Monte Carlo (MCMC) and some algorithms related to it. -->
<!---->
<!-- ## Branching Processes -->
<!-- > [!INTUITION/Branching Processes] -->
<!-- > Many real-world phenomena can be described as developing with a tree-like structure, for example, -->
<!-- > - Growth of cells. -->
<!-- > -->
<!-- > - Spread of viruses or other pathogens in a population. -->
<!-- > -->
<!-- > - Nuclear chain reactions. -->
<!-- > -->
<!-- > - Spread of funny cat videos on the internet. -->
<!-- > -->
<!-- > - Spread of a surname over generations. [^margin: This was the original motivation for studying branching processes by Francis Galton and Henry William Watson in the 19th century.] -->
<!-- > -->
<!-- > The process with which one node gives rise to "children" can be described as random. -->
<!-- > We will assume the probabilistic properties of this process is the same for all nodes. -->
<!-- > -->
<!-- > Further, we will assume all nodes are organized into generatiomns, we are only concerned with the size of each generation. -->
<!-- > -->
<!-- > Lastly, the important questions we are interested in are; How large are the generations? How much does the size vary? Will the process die out eventually (i.e., reach a generation with size 0)? -->
<!---->
<!-- ### Definitions and Properties -->
<!-- > [!DEFINITION/Galton-Watson Branching Process] -->
<!-- > A (Galton-Watson) branching process is a discrete Markov chain $Z_0, Z_1, \ldots, Z_n, \ldots$ where, -->
<!-- > - The state space is the non-negative integers. -->
<!-- > -->
<!-- > - $Z_0 = 1$ (i.e., the process starts with one individual). -->
<!-- > -->
<!-- > - $Z_n$ is the sum $X_1 + X_2 + \ldots + X_{Z_{n - 1}}$, where $X_i$ are independent random non-negative integers all with the same offspring distribution, -->
<!-- $$ -->
<!-- Z_n = \sum_{i = 1}^{Z_{n - 1}} X_i -->
<!-- $$ -->
<!-- > Connecting each of the $Z_n$ individuals in generation $n$ with their offspring in generation $n + 1$ we get a tree illustrating the branching process. -->
<!-- > -->
<!-- > The offspring distribution is described by the probability vector $a = (a_0, a_1, \ldots)$ where $a_j = P(X_i = j)$. -->
<!-- > -->
<!-- > We will mainly focus on the interesting cases where we assume $a_0 > 0$ (i.e., there is a positive probability of having no offspring) and $a_0 + a_1 < 1$ (i.e., there is a positive probability of having more than one offspring). -->
<!---->
<!-- > [!DEFINITION/Expected Size of Generations in a Branching Process and Classification of Branching Processes] -->
<!-- > > [!NOTE] -->
<!-- > > Note that the state 0 is abosribing, this absorbtion is called extinction. -->
<!-- > > Also, as $a_0 > 0$, all nonzero states are transient. -->
<!-- > Let $\mu = \mathbb{E}[X_i] = \sum_{j=0}^{\infty} j \cdot a_j$, i.e., the expected number of offspring per individual. -->
<!-- > -->
<!-- > Then, we may compute that, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- \mathbb{E}[Z_n] & = \mathbb{E}\left(\sum_{i = 1}^{Z\_{n-1}}\right) \newline -->
<!-- & = \mathbb{E}\left(\mathbb{E}\left(\sum_{i = 1}^{Z\_{n-1}} X_i \mid Z\_{n - 1} \right)\right) -->
<!-- & = \ldots -->
<!-- & = \mathbb{E}[Z\_{n - 1}] \cdot \mu \newline -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > Thus, we directly get that, -->
<!-- $$ -->
<!-- \mathbb{E}[Z_n] = \mu^n \cdot \underbrace{\mathbb{E}[Z_0]}_{\coloneqq 1} = \mu^n -->
<!-- $$ -->
<!-- > Based on the value of $\mu$, we classify branching processes into three categories: -->
<!-- > 1. Subcritical branching processes: $\mu < 1$. The expected generation size decreases over time, $\lim_{n \to \infty} \mathbb{E}[Z_n] = 0$. -->
<!-- > -->
<!-- > 2. Critical branching processes: $\mu = 1$. The expected generation size remains constant over time, $\lim_{n \to \infty} \mathbb{E}[Z_n] = 1$. -->
<!-- > -->
<!-- > 3. Supercritical branching processes: $\mu > 1$. The expected generation size increases over time, $\lim_{n \to \infty} \mathbb{E}[Z_n] = \infty$. -->
<!-- > > [!NOTE] -->
<!-- > > One can prove that if $\lim_{n \to \infty} Z_n = 0$, then the probability of extinction is 1. -->
<!-- > > We will do this later. -->
<!---->
<!-- > [!DEFINITION/Variance of Generation Sizes in a Branching Process] -->
<!-- > Let $\mu$ be defined as above, and let $\sigma^2 = \mathrm{Var}(X_i)$ be the variance of the number of children. -->
<!-- > -->
<!-- > Using the law of total variance we get, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- \mathrm{Var}(Z_n) & = \mathrm{Var}(\mathbb{E}[Z_n \mid Z\_{n - 1}]) + \mathbb{E}[\mathrm{Var}(Z_n \mid Z\_{n - 1})] \newline -->
<!-- & = \mathrm{Var}\left(\mathbb{E}\left(\sum\_{i = 1}^{Z\_{n - 1}} X_i \mid Z\_{n - 1}\right)\right) + \mathbb{E}\left[\mathrm{Var}\left(\sum\_{i = 1}^{Z\_{n - 1}} X_i \mid Z\_{n - 1}\right)\right] \newline -->
<!-- & = \mathrm{Var}(\mu Z\_{n - 1}) + \mathbb{E}[\sigma^2 Z\_{n - 1}] \newline -->
<!-- & = \mu^2 \mathrm{Var}(Z\_{n - 1}) + \sigma^2 \mathbb{E}[Z\_{n - 1}] \newline -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > From this, we can prove by induction that, for $n \geq 1$, -->
<!-- $$ -->
<!-- \mathrm{Var}(Z_n) = \sigma^2 \mu^{n - 1} \sum_{k = 0}^{n - 1} \mu^k = -->
<!-- \begin{cases} -->
<!-- n \sigma^2 & \text{if } \mu = 1 \newline -->
<!-- \sigma^2 \mu^{n - 1} \frac{\mu^n - 1}{\mu - 1} & \text{if } \mu \neq 1 -->
<!-- \end{cases} -->
<!-- $$ -->
<!---->
<!-- > [!DEFINITION/Extinction Probability of a Branching Process] -->
<!-- > Let $X$ denote an offspring variable and $e$ the probability of extinction. -->
<!-- > -->
<!-- > Using first-step analysis, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- e & = \sum_{k = 0}^{\infty} P(X = k) P(\text{extinction of } k \text{ processes}) \newline -->
<!-- & = \sum_{k = 0}^{\infty} P(X = k) e^k = \mathbb{E}_X[e^X] = G_X(e) -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > where we define the probability generating function (PGF) $G_X(s)$ as, -->
<!-- $$ -->
<!-- G_X(s) = \mathbb{E}[s^X] -->
<!-- $$ -->
<!-- > -->
<!-- > Let $e_n$ be the probability of extinction by generation $n$. Using first-step analysis again, we get, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- e_n & = \sum\_{k = 0}^{\infty} P(X = k) P(\text{extinction by generation } n - 1 \text{ of } k \text{ processes}) \newline -->
<!-- & = \sum\_{k = 0}^{\infty} P(X = k) e\_{n - 1}^k = \mathbb{E}\_X[e_{n - 1}^X] = G_X(e_{n - 1}) -->
<!-- \end{align*} -->
<!-- $$ -->
<!---->
<!-- > [!THEOREM/Extinction Probability Theorem] -->
<!-- > Let $G_X$ be the probability generating function for the offspring distribution for a branching process. -->
<!-- > The probability of eventual extinction is the smallest positive root of the equation $G_X(s) = s$. -->
<!-- > > [!PROOF] -->
<!-- > > If $s = x$ is any positive solution to $G_X(s) = s$, then $e \leq x$. -->
<!-- > > We have $0 = e_0 < x$. We have that $G_X$ is an increasing function, as, -->
<!-- $$ -->
<!-- s_0 < s_1 \implies \sum_{k = 0}^{\infty} P(X = k) s_0^k < \sum_{k = 0}^{\infty} P(X = k) s_1^k \implies G_X(s_0) < G_X(s_1) -->
<!-- $$ -->
<!-- > Thus, applying $G_X$ repeatedly we get $e_n < x$ and thus $e = \lim_{n \to \infty} e_n \leq x$. -->
<!---->
<!-- ### Probability Generating Functions -->
<!-- > [!INTUITION/Probability Generating Functions] -->
<!-- > For any discrete random variable $X$ taking values in $\\{0, 1, 2, \ldots\\}$, define the probability function $G_X(s)$ as, -->
<!-- $$ -->
<!-- G_X(s) = \mathbb{E}[s^X] = \sum_{k = 0}^{\infty} P(X = k) s^k -->
<!-- $$ -->
<!-- > The series converges absolutely for $\Vert s \Vert \leq 1$. We assume $s$ is a real number in $[0, 1]$. -->
<!-- > -->
<!-- > We get a 1-1 correspondence between probability vectors on $\\{0, 1, 2, \ldots\\}$ and functions represented by a series where the coefficients sum to 1. -->
<!-- > -->
<!-- > Specifically, if $G_X(s) = G_Y(s)$ for all $s$ for random variables $X$ and $Y$, then $X$ and $Y$ have the same distribution. -->
<!-- > > [!NOTE/Properties of Probability Generating Functions] -->
<!-- > > - To go from $X$ to $G_X(s)$, we compute the (in)finite sum. -->
<!-- > > -->
<!-- > > - To go from $G_X(s)$ to $X$, we use that we have, -->
<!-- $$ -->
<!-- P(X = k) = \frac{G_X^{(k)}(0)}{k!} -->
<!-- $$ -->
<!-- > > - If $X$ and $Y$ are independent random variables, then, -->
<!-- $$ -->
<!-- G_{X + Y}(s) = \mathbf{E}[s^{X + Y}] = \mathbb{E}[s^X s^Y] = \mathbb{E}[s^X] \mathbb{E}[s^Y] \eqqcolon G_X(s) G_Y(s) -->
<!-- $$ -->
<!-- > > - $\mathbb{E}[X] = G_X^{\prime}(1)$ -->
<!-- > > -->
<!-- > > - $\mathbb{E}[X(X - 1)] = G_X^{\prime \prime}(1)$ -->
<!-- > > -->
<!-- > > - As a consequence, $\mathrm{Var}(X) = G_X^{\prime \prime}(1) + G_X^{\prime}(1) - (G_X^{\prime}(1))^2$ -->
<!-- > > -->
<!-- > > Further, assume we have a branching process $Z_0, Z_1, \ldots$ with independent offspring variables $X_i$ all with the same distribution as $X$. -->
<!-- > > By writing $G_n(s) = G_{Z_n}(s) = \mathbb{E}[s^{Z_n}]$ and $G_X(s) = \mathbb{E}[s^X]$, we get, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- G_n(s) & = \mathbb{E}[s^{\sum_{k = 1}^{Z_{n - 1}} X_k}] \newline -->
<!-- & = \mathbb{E}[\mathbb{E}[s^{\sum_{k = 1}^{Z_{n - 1}} X_k} \mid Z_{n - 1}]] \newline -->
<!-- & = \mathbb{E}\left[\mathbb{E}\left[\prod_{k = 1}^{Z_{n - 1}} s^{X_k} \mid Z_{n - 1}\right]\right] \newline -->
<!-- & = \mathbb{E}[G(s)^{Z_{n - 1}}] \newline -->
<!-- & = G_{n - 1}(G_X(s)) \newline -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > As $G_0(s) = \mathbb{E}[s^{Z_0}] = s$, it follows that, -->
<!-- $$ -->
<!-- G_n(s) = \underbrace{G_X(G_X(\ldots G_X(s) \ldots))}_{n \text{ times}} -->
<!-- $$ -->
<!---->
<!-- > [!THEOREM/Extinction Probability Theorem (Addition)] -->
<!-- > Let $G_X$ be the probability generating function for the offspring distribution for a branching process. -->
<!-- > The probability of eventual extinction is the smallest positive root of the equation $G_X(s) = s$. -->
<!-- > Also, if the process is critical ($\mu = 1$) then the extinction probability is 1. -->
<!-- > > [!PROOF/Of the last sentence] -->
<!-- > > In the critical case, -->
<!-- $$ -->
<!-- G_X^{\prime}(1) = \mathbb{E}[X] = \mu = 1 -->
<!-- $$ -->
<!-- > > As $G_X^{\prime \prime}(s) > 0$ for $s \in (0, 1)$, we get that $G_X^{\prime}(s) < 1$ for $s \in (0, 1)$, and $\frac{d}{ds}(G(s) - s) < 0$ for $s \in (0, 1)$. -->
<!-- > > As $G(1) - 1 = 0$ for any probability generating function, we get that $G(s) = s$ has its smallest positive root at $s = 1$. -->
<!---->
<!-- ## Markov Chain Monte Carlo (MCMC) -->
<!-- > [!INTUITION/Markov Chain Monte Carlo (MCMC)] -->
<!-- > So far we have used Markov chains in the following way -->
<!-- > 1. Define a Markov chain -->
<!-- > -->
<!-- > 2. Find its limiting distribution, if it exists -->
<!-- > -->
<!-- > Now, we want to, -->
<!-- > 1. Start with a desired distribution -->
<!-- > -->
<!-- > Construct a Markov chain having this limiting distribution -->
<!-- > -->
<!-- > In many contexts we want to compute the expecation $\mathbb{E}[r(X)]$ for some random variable $X$ and some function $r$. -->
<!-- > -->
<!-- > One way to approximate this is to generate samples $x_1, \ldots, x_N$ from $X$ and use, -->
<!-- $$ -->
<!-- \mathbb{E}[r(X)] = \lim_{N \to \infty} \frac{1}{N} \sum_{k = 1}^{N} r(x_k) \approx \frac{1}{N} \sum_{k = 1}^{N} r(x_k) -->
<!-- $$ -->
<!-- > Thus, the idea is simulating from a Markov chain having $X$ as its limiting distribution generates an approximate sample. Average over it as above. -->
<!---->
<!-- > [!INTUITION/Is an approximate sample good enough?] -->
<!-- > We know this works in classical statistics. -->
<!-- > -->
<!-- > Strong law of large numbers of samples: If $Y_1, Y_2, \ldots, Y_m$ and $Y$ are i.i.d random variables from a distribution with finite mean, and if $\mathbb{E}[r(Y)]$ exists, then, with probability 1, -->
<!-- $$ -->
<!-- \lim_{m \to \infty} \frac{r(Y_1) + r(Y_2) + \ldots + r(Y_m)}{m} = \mathbb{E}[r(Y)] -->
<!-- $$ -->
<!-- > There is a corresponding result for Markov chains. -->
<!-- > -->
<!-- > Strong law of large numbers for Markov chains: If $X_0, X_1, \ldots$ is an ergodic Markov chain with stationary distribution $\pi$, and if $\mathbb{E}_{X \sim \pi}[r(X)]$ exists, then, with probability 1, -->
<!-- $$ -->
<!-- \lim\_{m \to \infty} \frac{r(X_0) + r(X_1) + \ldots + r(X_m)}{m} = \mathbb{E}\_{X \sim \pi}[r(X)] -->
<!-- $$ -->
<!-- > where $X$ has distribution $\pi$. -->
<!-- > > [!NOTE] -->
<!-- > > When using this theorem in practice, one might improve accuracy by throwing away the first sequence $X_1, \ldots, X_s$ for $s < m$ before computing the average. The first sequence is then called the burn-in. -->
<!---->
<!-- ### Examples -->
<!-- > [!EXAMPLE/TOy Example of MCMC] -->
<!-- > Consider the Markov chain $X_0, X_1, \ldots$ with states $\\{0, 1, 2\\}$ and with, -->
<!-- $$ -->
<!-- P = -->
<!-- \begin{bmatrix} -->
<!-- 0.99 & 0.01 & 0 \newline -->
<!-- 0 & 0.9 & 0.1 \newline -->
<!-- 0.2 & 0 & 0.8 -->
<!-- \end{bmatrix} -->
<!-- $$ -->
<!-- > One can find that the limiting distribution is $v = (\frac{20}{23}, \frac{2}{23}, \frac{1}{23})$. -->
<!-- > -->
<!-- > Consider the function $r(x) = x^5$. If $X$ is a random variable with the limiting distribution, -->
<!-- $$ -->
<!-- \mathbb{E}[r(X)] = 0^5 \cdot \frac{20}{23} + 1^5 \cdot \frac{2}{23} + 2^5 \cdot \frac{1}{23} = \frac{34}{23} = 1.4783 -->
<!-- $$ -->
<!-- > If $Y_1, \ldots, Y_n$ are all i.i.d variables with the limiting distribution, we can check numerically that, -->
<!-- $$ -->
<!-- \lim_{n \to \infty} \frac{r(Y_1) + r(Y_2) + \ldots + r(Y_n)}{n} = 1.4783 -->
<!-- $$ -->
<!-- > We also get for $X_0, X_1, \ldots$ that, -->
<!-- $$ -->
<!-- \lim_{n \to \infty} \frac{r(X_1) + r(X_2) + \ldots + r(X_n)}{n} = 1.4783 -->
<!-- $$ -->
<!-- > but in this case the limit is approached more slowly. -->
<!---->
<!-- ### The Metropolis-Hastings Algorithm, Gibbs Sampling, and The Ising Model -->
<!-- > [!DEFINITION/The Metropolis-Hastings Algorithm] -->
<!-- > The Metropolis-Hastings algorithm is a method to construct a Markov chain having a desired limiting distribution. -->
<!-- > Let $\theta$ be a random variable with probability mass function $\pi( \theta)$. -->
<!-- > We also assume given a proposal distribution $T(\theta_{\text{new}} \mid \theta)$, which, for every given $\theta$, provides a PMF for a new $\theta_{\text{new}}$. -->
<!-- > -->
<!-- > Finally, we define, for $\theta$ and $\theta_{\text{new}}$, the acceptance probability, -->
<!-- $$ -->
<!-- a = \min\left(1, \frac{\pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}})}{\pi(\theta) T(\theta_{\text{new}} \mid \theta)}\right) -->
<!-- $$ -->
<!-- > The Metropolis-Hastings algorithm is: Starting with some initial value $\theta_0$, generate $\theta_1, \theta_2, \ldots$ by, at each step, proposing a new $\theta$ based on the old using the proposal function and accepting it with probability $a$. -->
<!-- > If it is not accepted, the old value is used again. -->
<!-- > -->
<!-- > If this defines an ergodic Markov chain, its unique stationary distribution is $\pi(\theta)$. -->
<!-- > > [!PROOF] -->
<!-- > > Let $P$ be the transition matrix of the Markov chain defined by the Metropolis-Hastings algorithm. -->
<!-- > > We need to show that $\pi P = \pi$. -->
<!-- > > It suffices to show that the detailed balance equations are satisfied, i.e., that, -->
<!-- $$ -->
<!-- \pi(\theta) P_{\theta \theta_{\text{new}}} = \pi(\theta_{\text{new}}) P_{\theta_{\text{new}} \theta} -->
<!-- $$ -->
<!-- > > for all states $\theta$ and $\theta_{\text{new}}$. -->
<!-- > > We have, -->
<!-- $$ -->
<!-- P_{\theta \theta_{\text{new}}} = T(\theta_{\text{new}} \mid \theta) \cdot a = -->
<!-- T(\theta_{\text{new}} \mid \theta) \cdot \min\left(1, \frac{\pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}})}{\pi(\theta) T(\theta_{\text{new}} \mid \theta)}\right) -->
<!-- $$ -->
<!-- > > and, -->
<!-- $$ -->
<!-- P_{\theta_{\text{new}} \theta} = T(\theta \mid \theta_{\text{new}}) \cdot a^{\prime} = -->
<!-- T(\theta \mid \theta_{\text{new}}) \cdot \min\left(1, \frac{\pi(\theta) T(\theta_{\text{new}} \mid \theta)}{\pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}})}\right) -->
<!-- $$ -->
<!-- > > If $\pi(\theta) T(\theta_{\text{new}} \mid \theta) \leq \pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}})$, then, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- \pi(\theta) P_{\theta \theta_{\text{new}}} & = \pi(\theta) T(\theta_{\text{new}} \mid \theta) \cdot 1 \newline -->
<!-- & = \pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}}) \cdot \frac{\pi(\theta) T(\theta_{\text{new}} \mid \theta)}{\pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}})} \newline -->
<!-- & = \pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}}) \cdot a^{\prime} \newline -->
<!-- & = \pi(\theta_{\text{new}}) P_{\theta_{\text{new}} \theta} -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > > If $\pi(\theta) T(\theta_{\text{new}} \mid \theta) > \pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}})$, then, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- \pi(\theta) P_{\theta \theta_{\text{new}}} & = \pi(\theta) T(\theta_{\text{new}} \mid \theta) \cdot \frac{\pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}})}{\pi(\theta) T(\theta_{\text{new}} \mid \theta)} \newline -->
<!-- & = \pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}}) \cdot 1 \newline -->
<!-- & = \pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}}) \cdot a^{\prime} \newline -->
<!-- & = \pi(\theta_{\text{new}}) P_{\theta_{\text{new}} \theta} -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > > Thus, in both cases, the detailed balance equations are satisfied, so $\pi$ is the stationary distribution of the chain. -->
<!---->
<!-- > [!DEFINITION/Gibbs Sampling] -->
<!-- > For any probability model over a vector $\theta = (\theta_1, \theta_2, \ldots, \theta_n)$, consider a Metropolis-Hastings proposal function changing only one coordinate, with the value of this coordinate simulated from the conditional distribution given the remaining coordinates. -->
<!-- > The acceptance probability is always 1, so the proposal is always accepted. -->
<!-- > > [!PROOF] -->
<!-- > > Let $\theta = (\theta_1, \theta_2, \ldots, \theta_n)$ and $\theta_{\text{new}} = (\theta_1, \ldots, \theta_{i - 1}, \theta_{i,\text{new}}, \theta_{i + 1}, \ldots, \theta_n)$ differ only in coordinate $i$. -->
<!-- > > The proposal distribution is, -->
<!-- $$ -->
<!-- T(\theta_{\text{new}} \mid \theta) = P(\theta_i = \theta_{i,\text{new}} \mid \theta_1, \ldots, \theta_{i - 1}, \theta_{i + 1}, \ldots, \theta_n) -->
<!-- $$ -->
<!-- > > and, -->
<!-- $$ -->
<!-- T(\theta \mid \theta_{\text{new}}) = P(\theta_i = \theta_i \mid \theta_1, \ldots, \theta_{i - 1}, \theta_{i + 1}, \ldots, \theta_n) -->
<!-- $$ -->
<!-- > > Using the definition of conditional probability, we have, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- T(\theta_{\text{new}} \mid \theta) -->
<!-- & = \frac{P(\theta_1, \ldots, \theta_{i - 1}, \theta_{i,\text{new}}, \theta_{i + 1}, \ldots, \theta_n)} -->
<!--          {P(\theta_1, \ldots, \theta_{i - 1}, \theta_{i + 1}, \ldots, \theta_n)} \\ -->
<!-- & = \frac{\pi(\theta_{\text{new}})} -->
<!--          {\sum_x \pi(\theta_1, \ldots, \theta_{i - 1}, x, \theta_{i + 1}, \ldots, \theta_n)} , -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > > and, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- T(\theta \mid \theta_{\text{new}}) -->
<!-- & = \frac{P(\theta_1, \ldots, \theta_{i - 1}, \theta_i, \theta_{i + 1}, \ldots, \theta_n)} -->
<!--          {P(\theta_1, \ldots, \theta_{i - 1}, \theta_{i + 1}, \ldots, \theta_n)} \\ -->
<!-- & = \frac{\pi(\theta)} -->
<!--          {\sum_x \pi(\theta_1, \ldots, \theta_{i - 1}, x, \theta_{i + 1}, \ldots, \theta_n)} , -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > > Thus, we have, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- \frac{\pi(\theta_{\text{new}}) T(\theta \mid \theta_{\text{new}})} -->
<!--      {\pi(\theta) T(\theta_{\text{new}} \mid \theta)} -->
<!-- & = \frac{ -->
<!-- \pi(\theta_{\text{new}}) \frac{\pi(\theta)}{\sum_x \pi(\theta_1,\ldots,\theta_{i-1},x,\theta_{i+1},\ldots,\theta_n)} -->
<!-- }{ -->
<!-- \pi(\theta) \frac{\pi(\theta_{\text{new}})}{\sum_x \pi(\theta_1,\ldots,\theta_{i-1},x,\theta_{i+1},\ldots,\theta_n)} -->
<!-- } \\ -->
<!-- & = \frac{\pi(\theta_{\text{new}})\pi(\theta)} -->
<!--          {\pi(\theta)\pi(\theta_{\text{new}})} = 1 , -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > > Thus, the proposal is always accepted. -->
<!-- > -->
<!-- > Putting together an algorithm updating different coordinates in different steps may create an ergodic Markov chain. -->
<!-- > -->
<!-- > This is called Gibbs sampling. -->
<!-- > Sometimes the conditional distributions are easy to derive. Then this is an easy-to-use version of Metropolis-Hastings. -->
<!---->
<!-- > [!DEFINITION/The Ising Model] -->
<!-- > Uses a grid of vertices; We will assume an $n \times n$ grid. -->
<!-- > Two vertices $v$ and $w$ are neighbours, denoted $v \sim w$, if they are next to each other in the grid. -->
<!-- > -->
<!-- > Each vertex $v$ can have value $+1$ or $-1$ (called its "spin"); We denote this by $\sigma_{v} = 1$ or $\sigma_{v} = -1$. -->
<!-- > -->
<!-- > A configuration $\sigma$ consists of a choice of $+1$ or $-1$ for each vertex. Thus, the set $\Omega$ of possible configurations has $2^(n^2)$ elements. -->
<!-- > We define the energy of a configuration as $E(\sigma) = - \sum_{v \sim w} \sigma_v \sigma_w$. -->
<!-- > -->
<!-- > The Gibbs distribution is the probability mass function on $\Omega$ defined by, -->
<!-- $$ -->
<!-- \pi(\sigma) \propto \exp(-\beta E(\sigma)) -->
<!-- $$ -->
<!-- > where $\beta$ is a parameter of the model; $\frac{1}{\beta}$ is called the temperature. -->
<!-- > -->
<!-- > It turns out that when the temperature is high, samples from the model will show a chaotic pattern of spins, but when the temperature sinks below the phase transition value, samples will show chunks of neighbouring vertices with the same spin, i.e., the system will be "magnetized". -->
<!---->
<!-- > [!INTUITION/Simulating from the Ising Model using Gibbs Sampling] -->
<!-- > For a vertex configuration $\sigma$ and a vertex $v$, let $\sigma_{-v}$ denote the part of $\sigma$ that does not involve $v$. -->
<!-- > Prpose a new configuration $\sigma^{\star}$ given an old configuration $\sigma$ by first choosing a vertex $v$, then, let $\sigma^{\star}$ be identical to $\sigma$ except possibly at $v$. -->
<!-- > Decide the spin at $v$ using the conditional distribution given $\sigma_{-v}$, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- \pi(\sigma^{\star} \mid \sigma_{-v}) & = \frac{\pi(\sigma_v = 1, \sigma_{-v})}{\pi(\sigma_{-v})} \newline -->
<!-- & = \frac{\pi(\sigma_v = 1, \sigma_{-v})}{\pi(\sigma_v = 1, \sigma_{-v}) + \pi(\sigma_v = -1, \sigma_{-v})} \newline -->
<!-- & = \frac{1}{1 + \frac{\pi(\sigma_v = -1, \sigma_{-v})}{\pi(\sigma_v = 1, \sigma_{-v})}} \newline -->
<!-- & = \frac{1}{1 + \exp(-\beta E(\sigma_v = -1, \sigma_{-v}) + \beta E(\sigma_v = 1, \sigma_{-v}))} \newline -->
<!-- & = \frac{1}{1 + \exp(\beta \sum_{v \sim w} \sigma_v \sigma_w \mid \sigma_v = -1 - \beta \sum_{v \sim w} \sigma_v \sigma_w \mid \sigma_v = 1)} \newline -->
<!-- & = \frac{1}{1 + \exp(-2 \beta \sum_{v \sim w} \sigma_w)} \newline -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > This works, but we will dicuss an even better approach "perfect sampling" later. -->
<!---->
<!-- ## Perfect Sampling -->
<!-- So far, we have seen that MCMC means, -->
<!---->
<!-- - We want to find the expected value of some expression under some distribution. -->
<!-- - The distribution is difficult to sample from, so instead we simulate from a Markov chain having the given distribution as its limiting distribution. -->
<!-- - Using the strong law of large numbers for Markov chains, we can approximate the expected value as an average over expressions computed on your simulated values. -->
<!---->
<!-- When we sample directly from a distribution, the rate of convergence of the averages is governed by the central limit theorem. -->
<!-- When using a Markov chain, it is generally very difficult to find the rate of convergence. -->
<!---->
<!-- There are some exceptions to this, we will now look at Perfect Sampling, where you prove that you have reached the limiting distribution exactly. -->
<!---->
<!-- > [!INTUITION/Perfect Sampling] -->
<!-- > Given an ergodic Markov chain with finite state space of size $k$ and limiting distribution $\pi$. -->
<!-- > The idea is that, given $n$ prove that $X_n$ actually has reached the limit distribution $\pi$. -->
<!-- > -->
<!-- > If we can rpove that the distribution at $X_n$ is independent of the starting state $X_0$, then we might be able to conclude that $X_n$ has the limiting distribution $\pi$. -->
<!-- > -->
<!-- > Construct $k$ Markov chains that are dependent ("coupled") but which are marginally Markov chains as above. -->
<!-- > If they start at the $k$ possible value at $X_0$ but have identical values at $X_n$, then we are done. -->
<!-- > -->
<!-- > > [!NOTE] -->
<!-- > > $n$ cannot be determined as the first value where the $k$ chains meet, it must be determined independently of such information. -->
<!-- > > Thus, usually one wants to generate chains $X_{-n}, X_{-n + 1}, \ldots, X_0$ where $X_0$ has the limiting distribution, and we stepwise increase $n$ to make all chains coalesce to one chain at time 0. -->
<!---->
<!-- > [!INTUITION/Using same source of randomness for all chains] -->
<!-- > Consider the chains $X^{(j)}\_{-n}, \ldots, X^{(j)}_0$ for $j = 1, 2, \ldots, k$. -->
<!-- > Instead of simulating $X^{(j)}\_{i + 1}$ based on $X^{(j)}_i$ independently for each j, we define a function $g$ so that $X^{(j)}\_{i + 1} = g(X^{(j)}_i, U_i)$ for all $j$ where $U_i \sim \mathrm{Uniform}(0, 1)$. -->
<!-- > -->
<!-- > Thus, if two chains have identical values in $X_i$, they will also be identical in $X_{i + 1}$. -->
<!-- > -->
<!-- > Thus, for a particular $n$, if all chains have not converged at $X_0$, we simulate $k$ chains from $X_{-2n}$ to $X_{-n}$. -->
<!-- > They might only hit a subset of the $k$ states at $X_{-n}$ and thus might coalesce to one state at $X_0$ using the old simulations. If not, double $n$ again. -->
<!---->
<!-- > [!DEFINITION/Monotonicity] -->
<!-- > One smart question is, do we need to keep track of all $k$ chains? -->
<!-- > We define a partial ordering on as et as a relation $x \leq y$ between some pairs $x$ and $y$ in the set, such that, -->
<!-- > - If $x \leq y$ and $y \leq x$ then $x = y$. -->
<!-- > -->
<!-- > - If $x \leq y$ and $y \leq z$ then $x \leq z$. -->
<!-- > -->
<!-- > We will need that our partial ordering has a minimal element (an $m$ such that $m \leq x$ for all $x$) and a maximal element (an $M$ such that $x \leq M$ for all $x$). -->
<!-- > -->
<!-- > If we have a partial ordering on the state space of the Markov chain and if $x \leq y$ implies $g(x, U) \leq g(y, U)$, then $g$ is monotone. -->
<!-- > -->
<!-- > We can then prove that we only need to keep track of the chain starting at $m$ and the chain starting at $M$. -->
<!-- > > [!PROOF] -->
<!-- > > If the chains starting at $m$ and $M$ coalesce at time 0, then for any other chain starting at $x$, we have $m \leq x \leq M$. -->
<!-- > > By monotonicity, we get that the chain starting at $m$ is less than or equal to the chain starting at $x$ at all times, and the chain starting at $M$ is greater than or equal to the chain starting at $x$ at all times. -->
<!-- > > Thus, if the chains starting at $m$ and $M$ are equal at time 0, the chain starting at $x$ must also be equal to them at time 0. -->
<!---->
<!-- > [!EXAMPLE/Simulating from the Ising Model using Perfect Sampling] -->
<!-- > We saw above how to use Gibbs sampling to simulate from the Ising model. We then simulated from the conditional distributions, using, -->
<!-- $$ -->
<!-- \pi(\sigma_v^{\star} = 1 \mid \sigma_{-v}) = \ldots = \frac{1}{1 + \exp(-2 \beta \sum_{v \sim w} \sigma_w)} -->
<!-- $$ -->
<!-- > Now, we extend this method into a perfect sampling. -->
<!-- > -->
<!-- > Define the partial ordering on configurations $\sigma, \tau$ be defining, -->
<!-- $$ -->
<!-- \sigma \leq \tau \iff \sigma_v \leq \tau_v \text{ for all vertices } v -->
<!-- $$ -->
<!-- > We have minimal and maximal elements $m = (-1, \ldots, -1)$ and $M = (1, \ldots, 1)$. -->
<!-- > > [!NOTE] -->
<!-- > > iF $\sigma \leq \tau$, then for all $v$ $\pi(\sigma_v^{\star} = 1 \mid \sigma_{-v}) \leq \pi(\tau_v^{\star} = 1 \mid \tau_{-v})$. -->
<!-- > -->
<!-- > Defining $g(\sigma, U)_v = 2 I (U \leq \pi(\sigma_v^{\star} = 1 \mid \sigma\_{-v}) ) - 1$, makes $g$ monotone. -->
<!-- > -->
<!-- > Given an Ising model with $\beta > 0$, start with choosing an integer $n > 0$ and setting configurations, -->
<!-- $$ -->
<!-- \begin{align*} -->
<!-- \sigma^{(-n)} & = m = (-1, \ldots, -1) \newline -->
<!-- \tau^{(-n)} & = M = (1, \ldots, 1) \newline -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- > > [!ALGORITHM/Perfect Sampling for the Ising Model] -->
<!-- > > - For $i = -n +1, \ldots, 0$ set $\sigma^{(i)} = \sigma^{(i - 1)}$, $\tau^{(i)} = \tau^{(i - 1)}$ and then, -->
<!-- > > -->
<!-- > >   - Looping through all vertices $v$: -->
<!-- > >     - Compute $\pi(\sigma_v^{\star} = 1 \mid \sigma_{-v}^{(i)})$ and $\pi(\tau_v^{\star} = 1 \mid \tau_{-v}^{(i)})$. -->
<!-- > >     - Simulate $U \sim \mathrm{Uniform}(0, 1)$. -->
<!-- > >     - Update $\sigma_v^{(i)} = g(\sigma^{(i)}, U)_v$ and $\tau_v^{(i)} = g(\tau^{(i)}, U)_v$. -->
<!-- > > - If $\sigma^{(0)} = \tau^{(0)}$, this is the result. Otherwise, double $n$ and repeat. -->
<h2 id="introduction">Introduction</h2>
<p>In this part we will introduce how MCMC can be used from a Bayesian perspective when doing inference.</p>
<h2 id="discrete-time-markov-chains-with-continuous-state-space">Discrete-Time Markov Chains with Continuous State Space</h2>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-teal-600 dark:bg-teal-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-teal-700 dark:text-teal-400" data-lucide="book-open" viewBox="0 0 24 24"><use href="#book-open"></use></svg>Definition: Discrete-Time Markov Chains with Continuous State Space<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-teal-700 dark:text-teal-400" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Our so far basic definitions are the same. $\{X_i\}<em>{i = 0, 1, \ldots}$ is a set of continuous random satisfying the Markov property,
$$
\pi(X</em>{n + 1} \mid X_0, X_1, \ldots, X_n) = \pi(X_{n + 1} \mid X_n)
$$
for all $n$.</p><p>The limiting distribution has the same definition. A stationary distribution is a density $f(x)$ satisfying,
$$
f(x_{n + 1}) = \int \pi(x_{n + 1} \mid x_n) f(x_n) \ dx_n.
$$
Ergodicity still means (with adjusted definitions) that the Markov chain is irreducible, aperiodic, and positive recurrent.</p><p>Ergodic Markov chains have a unique limiting distribution; Theorem same as before.</p><p>The Strong Law of Large Numbers for Markov Chains also holds in this setting.
Lastly, the theory for Metropolis-Hastings is also OK (although, some technical complications in the proof arise).</p></div>
</details>
<h3 id="bayesian-inference">Bayesian Inference</h3>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-sky-300 dark:bg-sky-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-sky-500 dark:text-sky-200" data-lucide="lightbulb" viewBox="0 0 24 24"><use href="#lightbulb"></use></svg>Intuition: MCMC for Bayesian Inference<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-sky-500 dark:text-sky-200" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Consider that we have some data $y_1, \ldots, y_n$, and we want to make a probability prediction for $y_{\text{new}}$.</p><p>We (often) define a parameter $\theta$, and a probabilistic model so that $y_1, \ldots, y_n, y_{\text{new}}$ are all conditionally independent given $\theta$,
$$
\pi(y_1, \ldots, y_n, y_{\text{new}}, \theta) = \left[\prod_{i=1}^{n} \pi(y_i \mid \theta) \right] \pi(y_{\text{new} \mid \theta} \pi(\theta).
$$
Then,
$$
\begin{align*}
\pi(y_{\text{new}} \mid y_1, \ldots, y_n) &#x26; = \int_{\theta} \pi(y_{\text{new}} \mid \theta) \pi(\theta \mid y_1, \ldots, y_n) \ d\theta \newline
&#x26; = \mathbb{E}_{\theta \mid y_1, \ldots, y_n}[\pi(y_{\text{new}} \mid \theta)] \newline
\end{align*}
$$
Using strong law of large numbers, we can make predictions by,</p><ul>
<li>Simulating $\theta_1, \ldots, \theta_k$ from the posterior $\pi(\theta \mid y_1, \ldots, y_n)$.</li>
<li>Averaging,
$$
\mathbb{E}_{\theta \mid y_1, \ldots, y_n}[\pi(y_{\text{new}} \mid \theta)] \approx \frac{1}{k} \sum_{i = 1}^{k} \pi(y_{\text{new}} \mid \theta_i)
$$
However, simulating from the posterior is often difficult. MCMC can help us here by simulating from a Markov chain having the posterior as its limiting distribution.</li>
</ul><p>By the strong law of large numbers for Markov chains, we still have (as $k \to \infty$),
$$
\mathbb{E}_{\theta \mid y_1, \ldots, y_n}[\pi(y_{\text{new}} \mid \theta)] = \lim_{k \to \infty} \frac{1}{k} \sum_{i = 1}^{k} \pi(y_{\text{new}} \mid \theta_i)
$$
when $\theta_1, \ldots, \theta_k$ is a realization from a Markov chain with the posterior $\pi(\theta \mid y_1, \ldots, y_n)$ as its limiting distribution.</p><p>As $\pi(\theta \mid y_1, \ldots, y_n) \propto \prod_{i = 1}^{n} \pi(y_i \mid \theta) \pi(\theta)$, we can use Metropolis-Hastings with some proposal distribution $q(\theta^{\star} \mid \theta)$ and acceptance probability,
$$
a = \min\left(1, \frac{\pi(\theta^{\star} \mid y_1, \ldots, y_n) q(\theta \mid \theta^{\star})}{\pi(\theta \mid y_1, \ldots, y_n) q(\theta^{\star} \mid \theta)}\right)
$$
To avoid underflow we generate $U \sim \mathrm{Uniform}(0, 1)$ and accept if,
$$
U > \exp\left(\sum_{i = 1}^{n} \left(\log \pi(y_i \mid \theta^{\star}) - \log \pi(y_i \mid \theta) \right) + \log \pi(\theta^{\star}) - \log \pi(\theta) + \log q(\theta \mid \theta^{\star}) - \log q(\theta^{\star} \mid \theta)\right)
$$</p></div>
</details>
<details class="group relative px-4 py-3 my-6 border-l-4 text-base border-sky-500 dark:bg-sky-950/10" open>
<summary class="flex items-center cursor-pointer font-medium text-base [&#x26;::-webkit-details-marker]:hidden"><svg class="mr-2 w-6 h-6 text-sky-600 dark:text-sky-300" data-lucide="code" viewBox="0 0 24 24"><use href="#code"></use></svg>Example: Toy Example<svg class="ml-auto w-5 h-5 transform transition-transform duration-200 group-open:rotate-180 text-sky-600 dark:text-sky-300" data-lucide="chevron-down" viewBox="0 0 24 24"><use href="#chevron-down"></use></svg></summary>
<div class="mt-2 text-base leading-relaxed prose dark:prose-invert"><p>Consider the following,
$$
\begin{align*}
y \mid p &#x26; \sim \mathrm{Binomial}(n = 17, p) \newline
p &#x26; \sim \mathrm{Beta}(\alpha = 2.3, \beta = 4.1) \newline
y_{\text{new}} \mid p &#x26; \sim \mathrm{Binomial}(n = 3, p) \newline
\end{align*}
$$
We would like to compute $P(y_{\text{new}} = 1 \mid y = 4)$.</p><p>We first note that,
$$
p \mid y \sim \mathrm{Beta}(\alpha = 2.3 + y, \beta = 4.1 + n - y)
$$
Thus, the predictive distribution,
$$
\begin{align*}
\pi(y \mid p) &#x26; = \frac{\pi(y \mid p) \pi(p)}{\pi(p \mid y)} \newline
&#x26; = \frac{\mathrm{Binomial}(y \mid n = 17, p) \mathrm{Beta}(p \mid \alpha = 2.3, \beta = 4.1)}{\mathrm{Beta}(p \mid \alpha = 2.3 + y, \beta = 4.1 + n - y)} \newline
&#x26; = \frac{\binom{n}{y} p^y (1 - p)^{n - y} \cdot \frac{p^{\alpha - 1} (1 - p)^{\beta - 1}}{B(\alpha, \beta)}}{\frac{p^{\alpha + y - 1} (1 - p)^{\beta + n - y - 1}}{B(\alpha + y, \beta + n - y)}} \newline
&#x26; = \binom{n}{y} \frac{B(\alpha + y, \beta + n - y)}{B(\alpha, \beta)} \newline
\end{align*}
$$
Since $y = 4$ and $n = 17$, the posterior is $\mathrm{Beta}(6.3, 17.1)$, predicting 1 success among 3 trials gives,
$$
\binom{3}{1} \frac{B(6.3 + 1, 17.1 + 2)}{B(6.3, 17.1)} = 0.3933
$$
Alternatively, we can use MCMC to estimate this value.</p></div>
</details>


</body></html> </article>  <nav class="col-start-2 grid grid-cols-1 gap-4 sm:grid-cols-2"> <a href="/notes/master/mve550/mve550_5" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-lg group flex items-center justify-start size-full" aria-disabled="false">  <svg width="1em" height="1em" viewBox="0 0 24 24" class="mr-2 size-4 transition-transform group-hover:-translate-x-1" data-icon="lucide:arrow-left">   <use href="#ai:lucide:arrow-left"></use>  </svg> <div class="flex flex-col items-start overflow-hidden text-wrap"> <span class="text-muted-foreground text-left text-xs"> Previous Post </span> <span class="w-full text-left text-sm text-balance text-ellipsis"> Part 5 - Branching Processes and MCMC </span> </div>  </a>  <a href="#" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-lg group flex items-center justify-end size-full pointer-events-none opacity-50 cursor-not-allowed" aria-disabled="true">  <div class="flex flex-col items-end overflow-hidden text-wrap"> <span class="text-muted-foreground text-right text-xs"> Next Post </span> <span class="w-full text-right text-sm text-balance text-ellipsis"> You&#39;re at the newest post! </span> </div> <svg width="1em" height="1em" viewBox="0 0 24 24" class="ml-2 size-4 transition-transform group-hover:translate-x-1" data-icon="lucide:arrow-right">   <use href="#ai:lucide:arrow-right"></use>  </svg>  </a> </nav> <div class="col-start-2"> <section class="mx-auto mt-12"> <script data-astro-rerun src="https://giscus.app/client.js" data-repo="rezaarezvan/rezarezvan.com" data-repo-id="R_kgDOHvQr3w" data-category="General" data-category-id="DIC_kwDOHvQr384CiWVC" data-mapping="og:title" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="en" data-loading="lazy" crossorigin="anonymous" async></script> </section> <script>
  function updateGiscusTheme() {
    const element = document.documentElement
    const theme = element.getAttribute('data-theme')
    const iframe = document.querySelector('iframe.giscus-frame')
    if (!iframe) return
    iframe.contentWindow.postMessage(
      { giscus: { setConfig: { theme } } },
      'https://giscus.app',
    )
  }

  const observer = new MutationObserver(updateGiscusTheme)
  observer.observe(document.documentElement, {
    attributes: true,
    attributeFilter: ['class'],
  })

  window.onload = () => {
    updateGiscusTheme()
  }
</script> </div> </section> <button data-slot="button" class="items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 size-9 group fixed right-8 bottom-8 z-50 hidden" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top"> <svg width="1em" height="1em" class="mx-auto size-4 transition-all group-hover:-translate-y-0.5" data-icon="lucide:arrow-up">   <symbol id="ai:lucide:arrow-up" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 12l7-7l7 7m-7 7V5"/></symbol><use href="#ai:lucide:arrow-up"></use>  </svg> </button> <script type="module">document.addEventListener("astro:page-load",()=>{const o=document.getElementById("scroll-to-top"),t=document.querySelector("footer");o&&t&&(o.addEventListener("click",()=>{window.scrollTo({top:0,behavior:"smooth"})}),window.addEventListener("scroll",()=>{const e=t.getBoundingClientRect().top<=window.innerHeight;o.classList.toggle("hidden",window.scrollY<=300||e)}))});</script>  </div> </main> <footer class="py-4"> <div class="mx-auto flex max-w-3xl flex-col items-center justify-center gap-y-2 px-4 sm:flex-row sm:justify-between"> <div class="flex flex-wrap items-center justify-center gap-x-2 text-center"> <span class="text-muted-foreground text-sm">
&copy; 2025  rezarezvan.com </span> </div> </div> </footer> <div id="backdrop" class="invisible fixed top-0 left-0 z-50 flex h-screen w-full justify-center bg-[rgba(0,0,0,0.5)] p-6 backdrop-blur-sm" data-astro-transition-persist="astro-t6dxx5el-4"> <div id="pagefind-container" class="m-0 flex h-fit max-h-[80%] w-full max-w-screen-sm flex-col overflow-auto rounded border border-black/15 bg-neutral-100 p-2 px-4 py-3 shadow-lg dark:border-white/20 dark:bg-neutral-900"> <div id="search" class="pagefind-ui pagefind-init" data-pagefind-ui data-bundle-path="/pagefind/" data-ui-options="{&#34;showImages&#34;:false,&#34;excerptLength&#34;:15,&#34;resetStyles&#34;:false}"></div> <script type="module" src="/_astro/Search.astro_astro_type_script_index_0_lang.tZYucdM2.js"></script> <div class="dark:prose-invert mr-2 pt-4 pb-1 text-right text-xs">
Press <span class="prose dark:prose-invert text-xs"><kbd class="">Esc</kbd></span> or click anywhere to close
</div> </div> </div> <script>
  document.addEventListener('DOMContentLoaded', () => {
    const magnifyingGlass = document.getElementById('magnifying-glass')
    const backdrop = document.getElementById('backdrop')

    function openPagefind() {
      const searchDiv = document.getElementById('search')
      const search = searchDiv.querySelector('input')
      setTimeout(() => {
        search.focus()
      }, 0)
      backdrop?.classList.remove('invisible')
      backdrop?.classList.add('visible')
    }

    function closePagefind() {
      const searchDiv = document.getElementById('search')
      const search = searchDiv.querySelector('input')
      if (search) {
        search.value = ''
      }
      backdrop?.classList.remove('visible')
      backdrop?.classList.add('invisible')
    }

    // open pagefind
    magnifyingGlass?.addEventListener('click', () => {
      openPagefind()
    })

    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape') {
        closePagefind()
      }
    })

    // close pagefind when searched result(link) clicked
    document.addEventListener('click', (event) => {
      if (event.target.classList.contains('pagefind-ui__result-link')) {
        closePagefind()
      }
    })

    backdrop?.addEventListener('click', (event) => {
      if (!event.target.closest('#pagefind-container')) {
        closePagefind()
      }
    })

    // prevent form submission
    const form = document.getElementById('form')
    form?.addEventListener('submit', (event) => {
      event.preventDefault()
    })
  })
</script>  </div> </body></html>