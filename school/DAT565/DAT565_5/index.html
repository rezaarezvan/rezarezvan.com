<!DOCTYPE html>
<html><head lang="en">
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge"><title>rezvan | Introduction to data science and AI: Part 5 - Statistical analysis</title><link rel="icon" type="image/png" href="https://rezvan.xyz/images/icon.png" /><meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
        content="Introduction In this part we&rsquo;ll cover some very central and fundamental concepts and theorems from statistics that are used.
The statistic paradigm The bread and butter of statistics is inferring the properties of the population from a random sample.
Properties of the random sample are determined by the underlying properties of the population and probability. The random sample can be described in terms of descriptive statistics, such as summary statistics e." />
    <meta property="og:image" content="https://raw.githubusercontent.com/rezaarezvan/rezvan.xyz/main/images/icon.png" />
    <meta property="og:title" content="Introduction to data science and AI: Part 5 - Statistical analysis" />
<meta property="og:description" content="Introduction In this part we&rsquo;ll cover some very central and fundamental concepts and theorems from statistics that are used.
The statistic paradigm The bread and butter of statistics is inferring the properties of the population from a random sample.
Properties of the random sample are determined by the underlying properties of the population and probability. The random sample can be described in terms of descriptive statistics, such as summary statistics e." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rezvan.xyz/school/DAT565/DAT565_5/" /><meta property="article:section" content="school" />
<meta property="article:published_time" content="2024-01-27T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-01-27T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Introduction to data science and AI: Part 5 - Statistical analysis"/>
<meta name="twitter:description" content="Introduction In this part we&rsquo;ll cover some very central and fundamental concepts and theorems from statistics that are used.
The statistic paradigm The bread and butter of statistics is inferring the properties of the population from a random sample.
Properties of the random sample are determined by the underlying properties of the population and probability. The random sample can be described in terms of descriptive statistics, such as summary statistics e."/>
<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@1,500&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Fira+Sans&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">

    
    
    <link rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/main.d5daa3df2e3279d931705520614088c3ae23adee342622cd4d903d1e1db73c7f.css" />
    <link id="lightSyntaxStyle" rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/light_syntax.65408cc3a5c02070b661c3e4e79306fc261cc63620f4adce9a30eafcba4ab79e.css" />
    
    <link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://rezvan.xyz/css/dark.43a635302a5e2609b6625cc16df70bbe0ae7f7f5dcae796685f45e2bf31c07c4.css"  disabled />
    <link id="darkSyntaxStyle" rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/dark_syntax.2b10cc1a2156b30874a063b7439a993bc3b43d476c5e1d8598d769c929c7b381.css" />
    

    
    
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

    
    <script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script>
    

    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>

    
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>
    

    
</head>
<body>
    <div class="content"><header>
    <div class="main">
        <a href="https://rezvan.xyz/">rezvan</a>
    </div>
    <nav id="site-navbar">
        
        <a href="/">home</a>
        
        <a href="/about">about</a>
        
        <a href="/principles">principles</a>
        
        <a href="/contact">contact</a>
        
        <a href="/cv">cv</a>
        
        <a href="/school">school</a>
        
        <a href="/tags">tags</a>
        
        | <span id="dark-mode-toggle" onclick="toggleTheme()"></span>
        <script src="https://rezvan.xyz/js/themetoggle.js"></script>
        
    </nav>
</header>

<main>
    <article>
        <div class="title">
            <h1 class="title">Introduction to data science and AI: Part 5 - Statistical analysis</h1>
            <div class="meta">Posted on Jan 27, 2024</div>
        </div>
        

        <section class="body">
            <h3 id="introduction">Introduction</h3>
<p>In this part we&rsquo;ll cover some very central and fundamental concepts and theorems from statistics that are used.</p>
<h3 id="the-statistic-paradigm">The statistic paradigm</h3>
<p>The bread and butter of statistics is <strong>inferring</strong> the properties of the <strong>population</strong> from a <strong>random sample</strong>.</p>
<p>Properties of the random sample are determined by the underlying properties of the population and <strong>probability</strong>.
The random sample can be described in terms of <strong>descriptive statistics</strong>, such as <strong>summary statistics</strong> e.g. count, mean, sample variance, order statistics</p>
<p>If the population is very large, we can treat the sample as being drawn from a probability distribution.</p>
<h3 id="fundamentals-of-probability">Fundamentals of probability</h3>
<p>Experiments yield exactly <strong>one</strong> of the possible outcomes.</p>
<p>The set of all possible outcomes is called the sample space, which we denote as $S$.</p>
<p>An event $E \subseteq S$ is a subset of outcomes.</p>
<p>If the outcome $s$ of the experiment is in the event, that is $s \in E$, we say the event occurs.</p>
<p>The probability distribution $p : S \to [0,1]$ is a function that maps each outcome $s \in S$ to a
probability, a number in the interval $[0,1]$, satisfying:
$$
\sum_{s \in S} p(s) = 1
$$</p>
<p>Probabilities of the sample space sum up to one, every experiment has an outcome, and the outcome has to be from the sample space
The probability of an event is the sum of probabilities of outcomes in the event:
$$
P(E) = \sum_{s \in E} p(s)
$$</p>
<p>The complement $\bar{E}$ of an event $E$ is the set of all outcomes that are not included in $E$, that is $\bar{E} = S \ \backslash \ E$.</p>
<p>The probability of the complement event satisfies $p(\bar{E}) = 1 - p(E)$.</p>
<p>Random variable $X$ is a function that maps outcomes to numerical values.
This means that the probability of random variable receiving a value $x$:
$$
P(X = x) = P(\{s \in S | X(s) = x\})
$$</p>
<p>The expected value of a random variable $X$ is:
$$
E[X] = \sum_{s \to S} p(s)V(s) \ | \ \text{where } V(s) \text{ is the value of outcome}
$$</p>
<p>Events are sets and there exists a universe $S$, we can apply set operations on events, such as</p>
<ul>
<li>Intersection $A \cap B = A\ \backslash \ (S\ \backslash\ B) \ | $ meaning the event that both $A$ and $B$ occur.</li>
<li>Union $A \cup B$ meaning that either $A$ or $B$ or both occur.</li>
</ul>
<p>If $A$ and $B$ satisfy $P(A \cap B)  = P(A)P(B)$, we say they are <strong>independent</strong>.
This means that knowing the event $A$ occurs gives no information about whether $B$ occurs and vice versa.</p>
<p>The conditional probability of $A$ given $B$, denoted $P(A | B)$ is defined as:
$$
P(A | B) = \frac{P(A \cap B)}{P(B)}
$$</p>
<h3 id="general-multiplication-rule">General multiplication rule</h3>
<p>It directly follows from the definition of conditional probability that:
$$
P(A \cap B) = P(A | B)P(B) = P(B | A)P(A)
$$</p>
<p>For $A$ and $B$ to occur simultaneously, $B$ has to occur, and then $A$ has to occur given $B$.</p>
<h3 id="bayes-theorem">Bayes&rsquo; theorem</h3>
<p>Conditional probability can be reversed; since we already observed that
$$
P(A \cap B) = P(A | B)P(B) = P(B | A)P(A)
$$</p>
<p>Solving for $P(B | A)$ we get:
$$
P(B | A) = \frac{P(A | B)P(B)}{P(A)}
$$</p>
<h3 id="random-variables-and-distributions">Random variables and distributions</h3>
<p>The distribution of a random variable $X$ is most importantly characterized by its <strong>expected value</strong> $E[x]$ and <strong>variance</strong> $Var[X] = E[X - E[X]^2]$
The expected value tells us about what kind of value is the most expected. Variance tells us about how the values are spread about the expectation.</p>
<p>If the codomain of the RV (the “values”) is finite or countably infinite, we say it is discrete.
If the codomain of the RV is uncountable, like $\mathbb{R}$, we say it is continuous.</p>
<h3 id="discrete-random-variables">Discrete random variables</h3>
<p>Discrete random variables take each value with a well-defined probability.</p>
<p>The function assigning this probability is called the <strong>probability mass function</strong> (PMF) $p\ ∶\ S \to [0,1]$
The expectation satisfies $E[X] = \sum_{x \in S} xp(x)$</p>
<h3 id="continuous-random-variables">Continuous random variables</h3>
<p>Continuous random variables take each individual value with probability zero.</p>
<p>Probability only becomes meaningful when an interval of values is considered
The probability of a single point is characterized by the <strong>probability density function</strong> (PDF) $f\ :\ S \to [0,1]$
Probability is obtained by integrating over the interval.
The probability that $X$ takes a value in the interval $[a, b]$ can be obtained by computing $P(a \leq X \leq B) = \int_a^b f(x)\ dx$</p>
<p>In particular, the probability that $X$ takes value at most $x$ is known as the <strong>cumulative distribution function</strong> (CDF) $F ∶ S \to [0,1]$, so $F(x) = P(X \leq x)$</p>
<p>PDF and CDF are related via $F(x) = \int_{-\infty}^{\infty} f(t)\ dt$ and conversely $f(x) = F&rsquo;(x)$.
The expectation satisfies, $E[X] = \int_{x \in S} x\ dx$</p>
<h3 id="exponential-distribution">Exponential distribution</h3>
<p>We denote $X \sim Exp(\lambda)$ that $X$ is distributed as the exponential distribution with parameter $\lambda &gt; 0$ if its PDF satisfies:
$$
f(x) =
\begin{cases}
\lambda e^{-\lambda x} &amp; \ | \ x \geq 0 \\
0 &amp; \ | \ \text{otherwise}
\end{cases}
$$</p>
<p>Which means:
$$
F(x) = \int_0^x f(t)\ dt =
\begin{cases}
1 - e^{-\lambda x} &amp; \ | \ x \geq 0 \\
0 &amp; \ | \ \text{otherwise}
\end{cases}
$$</p>
<p>$$
E[X] = \int_0^{\infty} tf(t)\ dt = \int_0^{\infty} t\lambda e^{-\lambda t}\ dt = \frac{1}{\lambda}
$$</p>
<p>$$
Var[X] = E[X - E[X]^2] = \int_0^{\infty} \left(t - \frac{1}{\lambda}\right)^2 f(t)\ dt = \int_0^{\infty} \left(t - \frac{1}{\lambda}\right)^2 \lambda e^{-\lambda t}\ dt = \frac{1}{\lambda^2}
$$</p>
<h3 id="normal-distribution">Normal distribution</h3>
<p>The normal distribution is one of the most important distributions as a lot of natural phenomena are by their very nature normally distributed.</p>
<p>The PDF of a normal distribution with mean $\mu$ (sometimes called location) and standard deviation $\sigma$ (sometimes called scale) is:
$$
f(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}
$$
The CDF $F(x) = \int_{-\infty}^{\infty} f(t)\ dt$ cannot be expressed in
closed form in terms of elementary functions.</p>
<p>Normal distribution with $\mu = 0$ and $\sigma = 1$ is called the standard normal distribution and its PDF and CDF are denoted $\varphi(x)$ and $\Phi(x)$, respectively.</p>
<p>We denote $X \sim \mathcal{N}(\mu, \sigma)$ that $X$ is normally distributed.</p>
<h3 id="z-scores">Z-scores</h3>
<p>Suppose we have some observations $x_1, x_2, \ldots, x_n$.</p>
<p>The sample mean is:
$$
\bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i
$$</p>
<p>This is an <strong>unbiased estimator</strong> for the population mean.</p>
<p>The sample variance is:
$$
s^2 = \frac{1}{n - 1} \sum_{i = 1}^{n} (x_i - \bar{x})^2
$$</p>
<p>The $\frac{1}{n - 1}$ factor is known as <strong>Bessel&rsquo;s correction</strong> and makes this an unbiased estimator for the population variance.</p>
<p>Sample standard deviation is computed as:
$$
s = \sqrt{s^2}
$$</p>
<p>This is not unbiased, but this is good enough in most cases.</p>
<p>Population mean and standard deviation are often denoted as $\mu$ and $\sigma$ respectively.</p>
<p>The <strong>Z-score</strong> for an observation is:
$$
z = \frac{x_i - \mu}{\sigma}
$$</p>
<p>Since these are usually unknown, we use their unbiased estimators:</p>
<p>$$
z = \frac{x_i - \bar{x}}{s}
$$</p>
<p>Computing the Z-score, brings the mean within the range $[0, 1]$ and the variance is 1. This makes observations insensitive to scaling and translation.</p>
<h3 id="concentration-of-measure">Concentration of measure</h3>
<p>Knowing just the expectation and the variance of a random variable tells us a lot because of a phenomenon called concentration of measure.
<a href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality">Chebyshev’s inequality</a> shows that, for any random variable with finite variance $\sigma^2$, the
random variable deviates from its expectation by more than $k\sigma$ (more than $k$ standard deviations) is at most $\frac{1}{k^2}$.</p>
<p>This holds for <strong>all random variables, regardless of the distribution</strong>.
This means that <strong>always</strong> at least 50% of probability mass is within $\sqrt{2}$ standard deviations from the mean, 75% within $2$ standard deviations, and so on.</p>
<p>This is what makes statistics work!</p>
<h3 id="law-of-large-numbers-and-central-limit-theorem">Law of large numbers and Central limit theorem</h3>
<p>The law of large numbers says that if we draw samples uniformly at random from an arbitrary probability distribution, then their mean converges to the expectation</p>
<p>Suppose $x_1, x_2, \ldots, x_n$ are <strong>independent and identically distributed</strong> samples from a probability
distribution with expectation $\mu$. Then:
$$
\bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i \to \mu \text{ as } n \to \infty
$$</p>
<p>So, we can <strong>estimate</strong> the expectation by computing the arithmetic mean from a sample.</p>
<p>The <strong>central limit theorem</strong> says that the mean of samples, sampled from an arbitrary
distribution, become normally distributed when there are a sufficiently large number of
samples. When we say standard error around the mean, we mean:
$$
SE = \frac{\sigma}{\sqrt{n}}
$$</p>
<p>Which is the standard deviation of the distribution as suggested by CLT.</p>
<h2 id="estimators-for-mean-and-variance">Estimators for mean and variance</h2>
<p>An <strong>estimator</strong> $\hat{\theta}(x)$ is a function that computes an <strong>estimate</strong> for a parameter $\theta$ of the underlying population (the distribution) from a sample.</p>
<p>The <strong>bias</strong> of an estimator is the systematic error of the estimate: $E[\hat{\theta}] - \theta$
If the bias is zero, we say the estimator is <strong>unbiased</strong>.
We already saw that the arithmetic mean is an unbiased estimator for the expectation by
the law of large numbers; for a sample $x_1, x_2, \ldots, x_n$ the <strong>sample mean</strong> is:
$$
\hat{\mu} = \bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i
$$</p>
<p>For variance, the unbiased sample variance is:
$$
\hat{\sigma^2} = \frac{1}{n - 1} \sum_{i = 1}^n \left(x_i - \hat{\mu}\right)^2
$$</p>
<p>Sample standard deviation is simply the square root of the sample variance, although this is slightly biased:
$$
\hat{\sigma} = \sqrt{\frac{1}{n - 1} \sum_{i = 1}^n \left(x_i - \hat{\mu}\right)^2}
$$</p>
<h3 id="confidence-interval">Confidence interval</h3>
<p>A point estimate is often too coarse, as there is a sizable variation around the mean, so instead it is better to give an interval than an individual point.</p>
<p>A confidence level sets the relative proportion of sampled confidence intervals that contain the true value of the parameter in question</p>
<p>The most commonly used confidence level is 95%;
this means that out of all possible confidence intervals, the sampled interval should contain the correct parameter value in 95% of the cases.</p>
<p>Higher confidence level yields wider confidence intervals
If, by CLT, we assume the estimator is normally distributed, then a confidence interval at
95% confidence level corresponds to the interval $[\hat{\mu} - 1.96SE, \hat{\mu} + 1.96SE]$
As $\sigma$ is often unknown, we usually estimate $SE = \frac{\hat{\sigma}}{\sqrt{n}}$.</p>
<h3 id="bernoulli-distribution">Bernoulli distribution</h3>
<p>Bernoulli distribution is a fundamental distribution for binary-valued questions: YES/NO, did an event occur? Etc.</p>
<p>$X \sim Bernoulli(p)$ for a parameter $p \in [0,1]$ if its PMF satisfies:
$$
P(X = 1) = p \\
P(X = 0) = 1 - p
$$</p>
<p>$$
E[X] = p \\
Var[X] = p(1 − p)
$$</p>
<h3 id="binomial-distribution">Binomial distribution</h3>
<p>The Binomial distribution models the number of positive outcomes of independent Bernoulli trials.
If $X_1, X_2, \ldots, X_n \sim Bernoulli(p)$ are independent and identically distributed, then $X = \sum_{i = 1}^n X_i \sim Bin(n, p)$.
So, $X$ counts how many of the Bernoulli variables were 1.
$$
E[X] = np \\
Var[X] = np(p - 1) \\
$$</p>
<p>PMF:
$$
P(X = k) = {n\choose k} p^k (1 - p){n - k}
$$</p>
<h3 id="multinomial-distribution">Multinomial distribution</h3>
<p>What if we have more than two categories of objects?
$$
X \sim Mul(n; p_1, p_2, \ldots, p_k)
$$</p>
<p>The PMF satisfies:
$$
P(X = (x_1, x_2, \ldots, x_k)) = {n \choose {x_1, x_2, \ldots, x_k}} p_1^{x_1} p_2^{x_2} \dots p_k^{x_k}
$$</p>
<p>$$
E[X_i] = np_i \\
Var[X_i] = np_i(1 - p_i)
$$</p>
<h3 id="bag-of-words">Bag of words</h3>
<p>Consider a text document of natural language; one way to represent the content of documents that is useful for classification tasks is the bag of words model.
A bag is a multiset; a bag of words maps each word of a vocabulary to the number of times it occurred in a document</p>
<h4 id="classifying-bags-of-words">Classifying bags of words</h4>
<p>In the bag of words model, each document $x$ is a vector of length $n$ where $n$ is the
size of vocabulary. The elements of a document vector $x_i$ record the multiplicity of the word in that document.</p>
<p>Typically the vectors are sparse: most words never occur in a given document. Suppose we try to classify documents in to classes $C_1, C_2, \ldots, C_k$.
Given an observation $x$, we can try to estimate the likelihood that a document from a class
$j$ would generate a vector that matches the observation, $P(x | C_j)$.</p>
<p>We can combine this with the prior probability that we know how probable different classes are to occur, $P(C_j)$
By Bayes’ theorem, the posterior probability of the class, given evidence, is:
$$
P(C_j | x) = \frac{P(x | C_j)P(C_j)}{P(x)}
$$</p>
<h3 id="naïve-bayesian-classifier">Naïve Bayesian classifier</h3>
<p>It would thus make sense to classify the document to the class that has the largest posterior probability.
Unfortunately, we often have insufficient information about the dependence between features to make full Bayesian inference feasible.
Instead, we can make the Naïve Bayesian assumption: we assume that all features are independent.</p>
<h3 id="zero-values">Zero-values</h3>
<p>If one of the values in some of the features is not observed for some class in the training set, the estimated value $P(X | C)$ = 0, which means that the posterior probability becomes
necessarily zero
This might not be what we wanted: some values might be exceedingly rare but still occur in practice.</p>
<p>We can deal with this by Laplace Smoothing, we choose a parameter $\alpha &gt; 0$.
If we use $d$ values for the feature and our empirical proportion is $p_i = \frac{x_i}{n}$,
and we have $d$ different values then we change this to:
$$
\hat{p_i} = \frac{x_i + \alpha}{n + \alpha d}
$$</p>
<p>This corresponds to adding $\alpha$ imaginary observations of each value</p>
<h3 id="bernoulli-naïve-bayes">Bernoulli Naïve Bayes</h3>
<p>Suppose $x$ is a bag of words over a vocabulary of $n$ words.</p>
<p>We can then treat each $x_i$ as a Bernoulli variable: we don’t care about the count, only if the
word is present in a document; that is, $x \in \{0, 1\}^n$.</p>
<p>The estimate $p_{i,j} = P(x_i | C_j)$ is then simply the fraction of documents of class $C_j$ that
contain the word $i$.</p>
<p>Likelihood of the document $x$ being generated from class $C_j$ would thus be:
$$
P(x | C_j) = \prod_{i = 1}^n p_{i,j}^{x_i} (1 - p_{i, j})^{1 - x_i}
$$</p>
<h3 id="multinomial-naïve-bayes">Multinomial Naïve Bayes</h3>
<p>Suppose $x$ is a bag of words over a vocabulary of $n$ words.</p>
<p>We can then treat each $x_i$ as a binomial variable: this time we care about the count, so $x \in \mathbb{N}^n$.
The estimate $p_{i,j} = P(x_i | C_j)$ is then simply the fraction of words in total of class $C_j$ that are equal to the word $i$.</p>
<p>Likelihood of the document $x$ being generated from class $C_j$ would thus be determined by a multinomial distribution:
$$
P(x | C_j) = {{\sum_{i =1}^n x_i} \choose {x_1, x_2, \ldots, x_k}} \prod_{i = 1}^n p_{i,j}^{x_i}
$$</p>
<h3 id="logit-function">Logit function</h3>
<p>The logit function is defined as:
$$
f : (-\infty, \infty) \to [0, 1]
$$</p>
<p>$$
f(x) = \dfrac{1}{1 + e^{-cx}} \ | \ c &gt; 0
$$</p>
<p>The logit function turns arbitrary values into probabilities.
$$
\lim_{x \to \infty} f(x) = 1 \\
\lim_{x \to -\infty} f(x) = 0 \\
\lim_{x \to 0} f(x) = \frac{1}{2}
$$</p>

        </section>

        <div class="post-tags">
            
            
            <nav class="nav tags">
                <ul class="tags">
                    
                    <li><a href="/%20tags/Introduction-to-data-science-and-AI">Introduction to data science and AI</a></li>
                    
                </ul>
            </nav>
            
            
        </div>
    </article>
</main>
<footer>
    <div style="display:flex"><a class="soc" href="https://github.com/rezaarezvan" rel="me" title="GitHub"><i data-feather="github"></i></a>
        <a class="border"></a><a class="soc" href="https://twitter.com/rzvan__/" rel="me" title="Twitter"><i data-feather="twitter"></i></a>
        <a class="border"></a></div><p class="footer_msg">memento mori</p></footer><script>
    feather.replace()
</script></div>
</body>

</html>
