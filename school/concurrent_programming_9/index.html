<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8"><meta property="og:site_name" content="rezvan"><title>Concurrent Programming: Part 9 - Parallelizing computations | rezvan</title>
  <meta property="og:title" content="Concurrent Programming: Part 9 - Parallelizing computations | rezvan"><meta property="og:description" content="">
  <meta property="og:type" content="blog">
  <meta property="og:link" content="https://rezvan.xyz/school/concurrent_programming_9/"><link rel="shortcut icon" type="image/png" href=https://rezvan.xyz//images/icon.png />
  <meta property="og:image" content="https://rezvan.xyz//images/icon.png" /><meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz//css/main.css" />    
</head>

<body>
  <div class="wrapper">
	<div class="content">
		<div class="header_main">
	<a href="https://rezvan.xyz/"><p class="header_title">rezvan</p><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

	</a>
  <br>
  <nav id="main">
    
      <a href="/About/">About</a>
    
      <a href="/CV/">CV</a>
    
      <a href="/school/">School</a>
    
  </nav></div>

  <article><div class="title_wrapper">
			<h1 class="title">Concurrent Programming: Part 9 - Parallelizing computations</h1><p class="single_time">Feb 13, 2023</p></div>
		<section class="post">
			<p>Concurrent programming introduces:</p>
<ul>
<li>The potential for parallel execution (faster, better resource usage)</li>
<li>The risk of race conditions (incorrect, unpredictable computations)</li>
</ul>
<p>The challenge of concurrent programming is introducing parallelism without affecting correctness.</p>
<p>Let&rsquo;s define how to parallelize:</p>
<p>A task $(F, D)$ consists in computing the result
F $(D)$ of applying function $F$ to input data $D$</p>
<p>A parallelization of $(F, D)$ is a collection $(F_1 , D_1 ),\ (F_2 , D_2),\ \dots$ of tasks such that
$F (D)$ equals the <em>composition</em> of $F_1 (D_1),\ F_2 (D_2),\ \dots $</p>
<h3 id="synchronization">Synchronization</h3>
<p>Synchronization is required to ensure correctness, but it also introduces mental overhead.</p>
<p>In shared-memory concurrency:</p>
<ul>
<li>
<p>Synchronization is based on locking.</p>
</li>
<li>
<p>Locking synchronizes data from cache to main memory, which may involve a 100x overhead.</p>
</li>
<li>
<p>Other costs with locking may include context switching (wait/signal)
and system calls (mutual exclusion primitives).</p>
</li>
</ul>
<p>In message-passing concurrency:</p>
<ul>
<li>
<p>Synchronization is based on messages</p>
</li>
<li>
<p>Exchanging small messages is efficient, but sending around large data is quite expensive (still
goes through main memory).</p>
</li>
<li>
<p>Other costs associated with message passing may include extra acknowledgment messages
and mailbox management (removing unprocessed messages).</p>
</li>
</ul>
<p>Also, an important note is about processes, creating a new process is generally expensive compared to sequential
function calls within the same process, since it involves:</p>
<ul>
<li>
<p>Reserving memory</p>
</li>
<li>
<p>Registering the new process with runtime system</p>
</li>
<li>
<p>Setting up the process’s local memory (stack and mailbox)</p>
</li>
</ul>
<p>Even if process creation is optimized, the cost of spawning
should be weighted against the speedup that can be obtained by
additional parallelism.</p>
<p>In particular, when the processes become way more than the available
processors, there will be diminishing returns with more spawning</p>
<p>Now let&rsquo;s cover the solutions</p>
<h3 id="forkjoin-parallelism">Fork/join parallelism</h3>
<ul>
<li>
<p>Forking: spawning child processes and assigning them smaller tasks</p>
</li>
<li>
<p>Joining: waiting for the child processes to complete and combining their results</p>
</li>
</ul>
<p>Note, the order in which we wait at a join node for forked children does not affect the total
waiting time.</p>
<p>In order to obtain good performance using fork/join parallelism:</p>
<ul>
<li>
<p>After forking children tasks, keep some work for the parent task before it joins the
children.</p>
</li>
<li>
<p>For the same reason, use <code>invoke</code> and <code>invokeAll</code> only at the top level as a norm.</p>
</li>
<li>
<p>Perform small enough tasks sequentially in the parent task, and fork children tasks
only when there is a substantial chunk of work left</p>
</li>
<li>
<p>Make sure different tasks can proceed independently – minimize data dependencies</p>
</li>
</ul>
<p>The advantages of parallelism may only be visible with several physical processors, and
on very large inputs.</p>
<h3 id="pools-and-work-stealing">Pools and work stealing</h3>
<p>Process pools are a technique to address the problem of using an appropriate
number of processes.</p>
<p>A pool creates a number of worker processes upon initialization.
The number of workers is chosen according to the actual available resources to
run them in parallel – a detail which pool users need not know about:</p>
<ul>
<li>
<p>As long as more work is available, the pool deals a work assignment to
a worker that is available.</p>
</li>
<li>
<p>The pool collects the results of the workers’ computations.</p>
</li>
<li>
<p>When all work is completed, the pool terminates and returns the overall result
This kind of pool is called a dealing pool: it actively deals work to workers.</p>
</li>
</ul>
<p>Workers are servers that run as long as the pool that created them does
A worker can be in one of two states:</p>
<ul>
<li>
<p>Idle: waiting for work assignments from the pool.</p>
</li>
<li>
<p>Busy: computing a work assignment.</p>
</li>
</ul>
<p>As soon as a worker completes its work assignments,
it sends the result to the pool and goes back to being idle.</p>
<p>A pool keeps track of:</p>
<ul>
<li>
<p>The remaining work – not assigned yet</p>
</li>
<li>
<p>The busy workers</p>
</li>
<li>
<p>The idle workers</p>
</li>
</ul>
<p>The pool also stores:</p>
<ul>
<li>
<p>A split function, used to extract a single work item</p>
</li>
<li>
<p>A join function, used to combine partial results</p>
</li>
<li>
<p>The overall result of the computation that is underway</p>
</li>
</ul>
<p>The pool terminates and returns the result of the computation when there are no
pending work items, and all workers are idle (thus all work has been done).
As long as there is some pending work and some idle workers, the pool deals work
to some of those idle workers.</p>
<p>When there are no pending work items or all workers are busy, the pool can only wait
for workers to send back results</p>
<p>Dealing pools work well if:</p>
<ul>
<li>
<p>The workload can be split in even chunks.</p>
</li>
<li>
<p>The workload does not change over time (for example if users send new
tasks or cancel tasks dynamically).</p>
</li>
</ul>
<p>Under these conditions, the workload is balanced evenly between workers, so
as to maximize the amount of parallel computation
In realistic applications, however, these conditions are not met:</p>
<ul>
<li>It may be hard to predict reliably which tasks take more time to compute the
workload is highly dynamic</li>
</ul>
<p>Stealing pools use a different approach to allocating tasks to workers that better
addresses these challenging conditions</p>
<p>A stealing pool associates a queue to every worker process
The pool distributes new tasks by adding them to the workers’ queues
When a worker becomes idle:</p>
<ul>
<li>
<p>First, it gets the next task from its own queue</p>
</li>
<li>
<p>If its queue is empty, it can directly steal tasks from the queue of another
worker that is currently busy.</p>
</li>
</ul>
<p>With this approach, workers adjust dynamically to the current working conditions
without requiring a supervisor that can reliably predict the workload required by
each task</p>
<p>With stealing, the pool may even send all tasks to one default thread, letting other
idle threads steal directly from it, simplifying the pool and reducing the
synchronization costs it incurs</p>

		</section>
  </article>
	</div>

	<footer><p class="footer_msg">Memento mori</p></footer>

  </div>
</body>
</html>
