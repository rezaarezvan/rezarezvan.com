<!DOCTYPE html>
<html><head lang="en"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge"><title>rezvan | DSA: Part 10 - Complexity (2)</title><link rel="icon" type="image/png" href="http://localhost:1313/images/icon.png" /><meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
        content="We have actually covered everything in this course - in this part we&rsquo;ll do some exercises!
Order of growth of Functions Let&rsquo;s find out the complexity ($\mathcal{O}$) of: $$ T(n) = 5(3n^2 &#43; 2n &#43; 6)(4\ log_{10}(n) &#43; 1) $$
Since we seek the growth rate - we can use our rules about complexity. We can remove all constants: $$ T(n) = (n^2 &#43; n)(log_{10}(n)) $$
The next rule we can apply is, &ldquo;the most dominating factor &lsquo;wins&rsquo;&rdquo; as I like to call it." />
    <meta property="og:image" content="https://raw.githubusercontent.com/rezaarezvan/rezvan.xyz/main/images/icon.png" />
    <meta property="og:title" content="DSA: Part 10 - Complexity (2)" />
<meta property="og:description" content="We have actually covered everything in this course - in this part we&rsquo;ll do some exercises!
Order of growth of Functions Let&rsquo;s find out the complexity ($\mathcal{O}$) of: $$ T(n) = 5(3n^2 &#43; 2n &#43; 6)(4\ log_{10}(n) &#43; 1) $$
Since we seek the growth rate - we can use our rules about complexity. We can remove all constants: $$ T(n) = (n^2 &#43; n)(log_{10}(n)) $$
The next rule we can apply is, &ldquo;the most dominating factor &lsquo;wins&rsquo;&rdquo; as I like to call it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/school/DAT038/DAT038_10/" /><meta property="article:section" content="school" />
<meta property="article:published_time" content="2022-12-09T18:49:03+01:00" />
<meta property="article:modified_time" content="2022-12-09T18:49:03+01:00" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="DSA: Part 10 - Complexity (2)"/>
<meta name="twitter:description" content="We have actually covered everything in this course - in this part we&rsquo;ll do some exercises!
Order of growth of Functions Let&rsquo;s find out the complexity ($\mathcal{O}$) of: $$ T(n) = 5(3n^2 &#43; 2n &#43; 6)(4\ log_{10}(n) &#43; 1) $$
Since we seek the growth rate - we can use our rules about complexity. We can remove all constants: $$ T(n) = (n^2 &#43; n)(log_{10}(n)) $$
The next rule we can apply is, &ldquo;the most dominating factor &lsquo;wins&rsquo;&rdquo; as I like to call it."/>
<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@1,500&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Fira+Sans&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">

    
    
    <link rel="stylesheet" type="text/css" media="screen" href="http://localhost:1313/css/main.f21696f99283540e6a453bb6dbca948ad3a30656fa420a23835a6a2bc9dee469.css" />
    <link id="lightSyntaxStyle" rel="stylesheet" type="text/css" media="screen" href="http://localhost:1313/css/light_syntax.65408cc3a5c02070b661c3e4e79306fc261cc63620f4adce9a30eafcba4ab79e.css" />
    
    <link id="darkModeStyle" rel="stylesheet" type="text/css" href="http://localhost:1313/css/dark.f21f1f72304fd4373b0a82b421dac836220c43b1e924ed3811cbe84f5c8a1b3f.css"  disabled />
    <link id="darkSyntaxStyle" rel="stylesheet" type="text/css" media="screen" href="http://localhost:1313/css/dark_syntax.2b10cc1a2156b30874a063b7439a993bc3b43d476c5e1d8598d769c929c7b381.css" />
    

    
    
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

    
    <script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script>
    

    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>

    
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>
    

    
</head>
<body>
    <div class="content"><header>
    <nav id="site-navbar">
        
        <a href="/">home</a>
        
        <a href="/about">about</a>
        
        <a href="/principles">principles</a>
        
        <a href="/contact">contact</a>
        
        <a href="/cv">cv</a>
        
        <a href="/school">school</a>
        
        | <span id="dark-mode-toggle" onclick="toggleTheme()"></span>
        <script src="http://localhost:1313/js/themetoggle.js"></script>
        
    </nav>
</header>

<main>
    <article>
        <div class="title">
            <h1 class="title">DSA: Part 10 - Complexity (2)</h1>
            <div class="meta">Posted on Dec 9, 2022</div>
        </div>
        

        <section class="body">
            <p>We have actually covered everything in this course - in this part we&rsquo;ll do some exercises!</p>
<h3 id="order-of-growth-of-functions">Order of growth of Functions</h3>
<p>Let&rsquo;s find out the complexity ($\mathcal{O}$) of:
$$
T(n) = 5(3n^2 + 2n + 6)(4\ log_{10}(n) + 1)
$$</p>
<p>Since we seek the growth rate - we can use our rules about complexity. We can remove all constants:
$$
T(n) = (n^2 + n)(log_{10}(n))
$$</p>
<p>The next rule we can apply is, &ldquo;the most dominating factor &lsquo;wins&rsquo;&rdquo; as I like to call it. Therefore:
$$
T(n) = (n^2)(log_{10}(n))
$$</p>
<p>Then we just multiply!
$$
T(n) = n^2\ log_{10}(n)
$$</p>
<p>And since we usually write $log$ when using Big-O notation:
$$
T(n) =  n^2\ log(n)
$$</p>
<p>Now we can say that $T(n)$ has a $\mathcal{O}(n^2\ log(n))$ complexity!. Since we are talking about $\mathcal{O}$, this means this function has a <strong>lower</strong> bound of this. This means that $T(n)$ also has a complexity of $\mathcal{O}(n^3)$ for example.</p>
<p>So what we <em>really</em> mean is that $T(n)$ has a $\Theta(n^2\ log(n))$ complexity.</p>
<p><strong>Suppose an algorithm takes time <em>t</em> on an input of size <em>n</em>. How many times longer does it take on an input of size 10n if&hellip;</strong></p>
<ul>
<li>If the algorithm is $\Theta(n)$?</li>
<li>If the algorithm is $\Theta(n^2)$?</li>
<li>If the algorithm is $\Theta(n^3)$?</li>
<li>If the algorithm is $\Theta(n\ log(n))$?</li>
<li>If the algorithm is $\Theta(log(n))$?</li>
</ul>
<p>This is quite easy! We just plug in our new $n$, and see how much $t$ grows!</p>
<ul>
<li>If the algorithm is $\Theta(n)$?
<ul>
<li>We get $10n \rightarrow 10t$!</li>
</ul>
</li>
<li>If the algorithm is $\Theta(n^2)$?
<ul>
<li>We get $100n \rightarrow 100t$!</li>
</ul>
</li>
<li>If the algorithm is $\Theta(n^3)$?
<ul>
<li>We get $1000n \rightarrow 1000t$!</li>
</ul>
</li>
<li>If the algorithm is $\Theta(n\ log(n))$?
<ul>
<li>We get $10n\ \cdot log(10n)$!</li>
<li>This isn&rsquo;t just $10 log(10)&quot; times more - it&rsquo;s a <em>little</em> bit longer, or so called &ldquo;logarithmic linear&rdquo;.</li>
</ul>
</li>
<li>If the algorithm is $\Theta(log(n))$?
<ul>
<li>We get $log(10n)$!</li>
<li>This means just a constant <strong>more</strong> time!</li>
</ul>
</li>
</ul>
<h3 id="complexity-analysis">Complexity Analysis</h3>
<p>Let&rsquo;s analyze the following snippet of code and, it&rsquo;s complexity.</p>
<pre tabindex="0"><code>found = false
for x in list:
    for y in list:
        if x + y == 0:
            found = true
</code></pre><p>If we say that the length of <code>list</code> is $n$. In the first loop we will have a complexity of $\mathcal{O}(n)$.
The inner loop will follow, using our previous rule of &rsquo;nested loops means multiplication&rsquo;.
This means our final program will have the complexity of $\mathcal{O}(n^2)$.</p>
<p>Now let&rsquo;s see over this code snippet:</p>
<pre tabindex="0"><code>found = false
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = list[i]
        y = list[j]
        if x + y == 0:
            found = true
</code></pre><p>In this snippet - we&rsquo;ll have the first loop iterating $n$ times -
however, the second loop, it will iterate $(0, \dots ,n -1), (1, \dots , n - 2), \dots$</p>
<p>This means that the number of times the second loop will run is between 1 and $n$ times - using our definition of complexity.
Let&rsquo;s call the number of times our loop runs $m$, $m \leq n$ which means m has a complexity of $\mathcal{O}(n)$.</p>
<p>This finally means we have a total complexity of $\mathcal{O}(n^2)$</p>
<p>Now let&rsquo;s do the same, but for three numbers!</p>
<pre tabindex="0"><code>found = false
for i in 0 .. n - 1:
    for j in i .. n - 1:
        for k in j .. n - 1:
            x = list[i]
            y = list[j]
            z = list[k]

            if x + y + z == 0:
                found = true
</code></pre><p>Exactly the same logic goes as from the last question to this, we can prove that each loop has a complexity of $\mathcal{O}(n)$.</p>
<p>Which gives the total complexity of $\mathcal{O}(n^3)$.</p>
<p>Now let&rsquo;s look at a similar program:</p>
<pre tabindex="0"><code>pairs_list = []
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = list[i]
        y = list[j]
        pairs_list.add(x + y)

found = false
for xplusy in pairs_list:
    for zplusw in pairs_list:
        if xplusy + zplusw == 0:
            found = true
</code></pre><p>As we&rsquo;ve stated above, the first part of the program will have a complexity of $\mathcal{O}(n^2)$.
Note that the <code>add()</code> function takes $\mathcal{O}(1)$ for dynamic arrays.</p>
<p>However, in the next block, the new array length is $n^2$, since we have added all possible permutations of pairs.
So the loops will now through $n^2$ elements. Which in total results a complexity of $\mathcal{O}(n^4)$.</p>
<p>From our earlier rules, we &lsquo;add&rsquo; blocks of codes, so the complexity is $\mathcal{O}(n^2) + \mathcal{O}(n^4)$.
Which means the resulting complexity becomes $\mathcal{O}(n^4)$.</p>
<h3 id="data-structure-complexities">Data Structure Complexities</h3>
<p>Let&rsquo;s refresh our memory and state all the complexities for our data structures and their functions.</p>
<ul>
<li>Dynamic Arrays:
<ul>
<li>Get/Set:
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
<li>Add/Remove <strong>at end</strong>:
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
<li>Add/remove <strong>elsewhere</strong>:
<ul>
<li>$\mathcal{O}(n)$</li>
</ul>
</li>
</ul>
</li>
<li>Stacks/queues:
<ul>
<li>Push/Pop:
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
<li>Enqueue/Dequeue:
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
</ul>
</li>
<li>Binary Heaps:
<ul>
<li>Add/RemoveMin (or Max):
<ul>
<li>$\mathcal{O}(log(n))$</li>
</ul>
</li>
<li>getMin (or Max):
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
</ul>
</li>
<li>BSTs:
<ul>
<li>Add/Remove/Search (worst case, meaning it&rsquo;s already sorted):
<ul>
<li>$\mathcal{O}(n)$</li>
</ul>
</li>
<li>Otherwise:
<ul>
<li>$\mathcal{O}(log(n))$</li>
</ul>
</li>
</ul>
</li>
<li>Stacks/queues:
<ul>
<li>Add/Remove/Search (Always!):
<ul>
<li>$\mathcal{O}(log(n))$</li>
</ul>
</li>
</ul>
</li>
<li>Hash Tables:
<ul>
<li>Add/Remove/Search (Given that the hash function is &lsquo;good&rsquo;):
<ul>
<li>$\mathcal{O}(1)$</li>
</ul>
</li>
</ul>
</li>
<li>General Tree:
<ul>
<li>If you <strong>down</strong> in a tree, you&rsquo;ll visit:
<ul>
<li>$\mathcal{O}(height)$ nodes</li>
</ul>
</li>
<li>If you explore every node, you&rsquo;ll visit:
<ul>
<li>$\mathcal{O}(n)$ nodes</li>
</ul>
</li>
<li>A tree has the <strong>worst</strong> case $\mathcal{O}(n)$ height.</li>
<li>A <strong>balanced</strong> tree is <strong>always</strong> $\mathcal{O}(log(n))$ height.</li>
</ul>
</li>
</ul>
<h3 id="analyzing-more-complexities">Analyzing more complexities</h3>
<p>Let&rsquo;s take a look at program which utilizes different data structures now:</p>
<pre tabindex="0"><code>pairs_list = []
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = list[i]
        y = list[j]
        pairs_list.add(x + y)

merge_sort(pairs_list)

found = false
for xplusy in pairs_list:
    if binary_search(pairs_list, -xplusy):
        found = true
</code></pre><p>So, we&rsquo;ve seen that first part, we know it&rsquo;s $\mathcal{O}(n^2)$.
But now we see a <code>merge_sort()</code> - this has a complexity of $\mathcal{O}(n\ log(n))$.
As we stated before, the list after the first block has a length of $n^2$.
Which means <code>merge_sort()</code> will have a complexity of $\mathcal{O}(n^2\ log(n^2))$</p>
<p>This will just sort it so, no length is added.</p>
<p>Then the next block, the for loop will have a complexity of $\mathcal{O}(n^2)$.
The binary search algorithm, has a complexity of $\mathcal{O}(log(n^2))$.</p>
<p>So this block will in total have a complexity of $\mathcal{O}(n^2\ log(n^2))$.</p>
<p>So if we add these blocks together and apply our rules we will get a total complexity of:
$\mathcal{O}(n^2\ log(n^2))$. We can apply some log rules to this:
$$
\mathcal{O}(n^2\ log(n^2))
\newline
\mathcal{O}(n^2\ 2\ log(n))
\newline
\mathcal{O}(n^2\ log(n))
$$</p>
<p>So finally our answer is, $\mathcal{O}(n^2 log(n))$</p>
<p>Let&rsquo;s now look at a case using a tree:</p>
<pre tabindex="0"><code>pairs_set = empty AVL_tree
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = pairs_set[i]
        y = pairs_set[j]

        if pairs_set.contains(-(x+y)):
            return true

        pairs_set.add(x+y)
</code></pre><p>As we&rsquo;ve seen before, the loops are $\mathcal{O}(n^2)$ - now the interesting part is the <code>contains()</code> to check if an element is present.
To check whether an element is present in a tree, has a complexity of $\mathcal{O}(log(n))$.
The rest of the operations are constant so, we can ignore them (including the <code>add()</code> for the AVL tree).</p>
<p>So-therefore the final complexity is $\mathcal{O}(n^2\ log(n))$. In the absolute worst case the <code>contains()</code> will be $\mathcal{O}(n)$, but let&rsquo;s ignore that :).</p>
<p>Now let&rsquo;s look at a hash table:</p>
<pre tabindex="0"><code>pairs_set = empty Hash_table
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = pairs.set[i]
        y = pairs_set[j]

        if pairs_set.contains(-(x + y)):
            return true

        pairs_set.add(x + y)
</code></pre><p>As per usual, the loops together create a complexity of $\mathcal{O}(n^2)$, now, a search in a hash table is $\mathcal{O}(1)$, if our hash function is &lsquo;good&rsquo;.</p>
<p>The rest of the operations are constant. Therefore, the overall complexity is $\mathcal{O}(n^2)$.</p>
<p>Now let&rsquo;s look at a BST example:</p>
<pre tabindex="0"><code>pairs_set = empty BST
for i in 0 .. n - 1:
    for j in i .. n - 1:
        x = pairs_set[i]
        y = pairs_set[j]

        if pairs_set.contains(-(x + y)):
            return true

        pairs_set.add(x + y)
</code></pre><p>The usual $\mathcal{O}(n^2)$ loops :). Now the <code>contains()</code> is the interesting part.
Since this is a BST, the <code>contains()</code> will have a complexity of $\mathcal{O}(n)$ in the worst case, since the BST can become unbalanced.</p>
<p>The same applies for <code>add()</code>. Since the rest of the operations are constant therefore it will be, $\mathcal{O}(n^4)$.</p>
<h3 id="different-kinds-of-complexities">Different kinds of complexities</h3>
<p>We also need to consider the different cases</p>
<ul>
<li>Best-case
<ul>
<li>This is not useful.</li>
</ul>
</li>
<li>Worst-case
<ul>
<li>This is the most useful.</li>
</ul>
</li>
<li>Average-case
<ul>
<li>Can be useful sometimes, mostly gives us a &lsquo;indicator&rsquo;.</li>
</ul>
</li>
</ul>
<p>Let&rsquo;s now talk about <strong>expected</strong> and <strong>amortised</strong> complexity.</p>
<h4 id="expected-complexity">Expected Complexity</h4>
<p>This is useful for randomized algorithms! It&rsquo;s the average over all possible random choice for a particular input.</p>
<p>For example, if we choose a random pivot, we turn quicksort from average-case $\mathcal{O}(n\ log(n))$ to expected $\mathcal{O}(n\ log(n))$</p>
<h4 id="amortised-complexity">Amortised Complexity:</h4>
<p>Amortised complexity is, the average over any sequence of operations, this is super useful!</p>
<p>For example, we use this to make dynamic arrays have an amortised complexity of $\mathcal{O}(1)$.</p>
<p>However, when we&rsquo;re calculating the total runtime of a program, it&rsquo;s safe to forget about this amortised bit and just treat each operation as costing $\mathcal{O}(1)$.</p>
<h3 id="conclusion">Conclusion</h3>
<p>This was it for this part - and the final part in this series. I really enjoyed this DSA course, super fun :).</p>

        </section>

        <div class="post-tags">
            
            
            <nav class="nav tags">
                <ul class="tags">
                    
                    
                    

                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                </ul>
            </nav>
            
            
        </div>
    </article>
</main>
<footer id="site-footer">
    
    <a href="https://github.com/rezaarezvan" title="">github</a>
    
    <a href="https://x.com/rzvan__/" title="">x</a>
    <p class="footer_msg">memento mori</p></footer><script>
    feather.replace()
</script></div>
</body>

</html>
