<!DOCTYPE html>
<html><head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge"><title>rezvan | Computer System Engineering: Part 5 - Cache memory</title><link rel="icon" type="image/png" href="https://rezvan.xyz/images/icon.png" /><meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
        content="In this part we will cover one of the most important solutions in computer science, the cache memory, but also memory in general.
Memory types Before we dive in - let&rsquo;s quickly recap the different types of memory there are:
Static RAM (SRAM) 0.5 ns - 2.5 ns | $2000 - $5000 per GB Dynamic RAM (DRAM) 50 ns - 70 ns | $20 - $75 per GB Magnetic storage 5 ms - 20 ms | $0." />
    <meta property="og:image" content="https://raw.githubusercontent.com/rezaarezvan/rezvan.xyz/main/images/icon.png" />
    <meta property="og:title" content="Computer System Engineering: Part 5 - Cache memory" />
<meta property="og:description" content="In this part we will cover one of the most important solutions in computer science, the cache memory, but also memory in general.
Memory types Before we dive in - let&rsquo;s quickly recap the different types of memory there are:
Static RAM (SRAM) 0.5 ns - 2.5 ns | $2000 - $5000 per GB Dynamic RAM (DRAM) 50 ns - 70 ns | $20 - $75 per GB Magnetic storage 5 ms - 20 ms | $0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rezvan.xyz/school/eda333/eda333_5/" /><meta property="article:section" content="school" />
<meta property="article:published_time" content="2023-04-13T20:04:58+02:00" />
<meta property="article:modified_time" content="2023-04-13T20:04:58+02:00" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Computer System Engineering: Part 5 - Cache memory"/>
<meta name="twitter:description" content="In this part we will cover one of the most important solutions in computer science, the cache memory, but also memory in general.
Memory types Before we dive in - let&rsquo;s quickly recap the different types of memory there are:
Static RAM (SRAM) 0.5 ns - 2.5 ns | $2000 - $5000 per GB Dynamic RAM (DRAM) 50 ns - 70 ns | $20 - $75 per GB Magnetic storage 5 ms - 20 ms | $0."/>
<script src="https://rezvan.xyz/js/feather.min.js"></script>
    

    
    
    <link rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/main.e78c3f2ddb05c4f5f0aa7553861677149e3602644857702209900dcba4ebbdf7.css" />
    <link id="lightSyntaxStyle" rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/light_syntax.65408cc3a5c02070b661c3e4e79306fc261cc63620f4adce9a30eafcba4ab79e.css" />
    
    <link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://rezvan.xyz/css/dark.2b7c17d8b1c965837e6d0b727b269c478440b4aff7f6aa57b84e0dc8ddfd15dc.css"  disabled />
    <link id="darkSyntaxStyle" rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/dark_syntax.2b10cc1a2156b30874a063b7439a993bc3b43d476c5e1d8598d769c929c7b381.css" />
    

    
    
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

    
    <script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script>
    

    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>

    
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>
    
</head>
<body>
    <div class="content"><header>
    <nav id="site-navbar">
        
        <a href="/">home</a>
        
        <a href="/about">about</a>
        
        <a href="/principles">principles</a>
        
        <a href="/contact">contact</a>
        
        <a href="/cv">cv</a>
        
        <a href="/school">school</a>
        
        | <span id="dark-mode-toggle" onclick="toggleTheme()"></span>
        <script src="https://rezvan.xyz/js/themetoggle.js"></script>
        
    </nav>
</header>

<main>
    <article>
        <div class="title">
            <h1>Computer System Engineering: Part 5 - Cache memory</h1>
            <div class="meta">Posted on Apr 13, 2023</div>
        </div>
        
        <section class="body">
            <p>In this part we will cover one of the most important solutions in computer science, the cache memory, but also memory in general.</p>
<h3 id="memory-types">Memory types</h3>
<p>Before we dive in - let&rsquo;s quickly recap the different types of memory there are:</p>
<ul>
<li>Static RAM (SRAM)
<ul>
<li>0.5 ns - 2.5 ns | $2000 - $5000 per GB</li>
</ul>
</li>
<li>Dynamic RAM (DRAM)
<ul>
<li>50 ns - 70 ns | $20 - $75 per GB</li>
</ul>
</li>
<li>Magnetic storage
<ul>
<li>5 ms - 20 ms | $0.20 - $2 per GB</li>
</ul>
</li>
</ul>
<p>So, therefore, an ideal memory would be one of the speed of an SRAM but at the cost of a hard drive.</p>
<h3 id="locality-of-reference">Locality of reference</h3>
<p>The locality of reference is the tendency that programs, when accessing memory, usually access memory addresses that is the same or very close to each other.</p>
<p>This is a very crucial tendency - if we only keep a subset of the data, memory and instructions, that are probable to appear, in a small memory - we can use that instead.</p>
<p>This is exactly what we can define as a <em>cache memory</em>. Just to clearly define the two different localities:</p>
<ul>
<li>Spatial Locality
<ul>
<li>A memory address that is <em>adjacent</em></li>
</ul>
</li>
<li>Temporal Locality
<ul>
<li>The <em>exact same</em> address.</li>
</ul>
</li>
</ul>
<h3 id="cache-memory">Cache memory</h3>
<p>So, as we defined, the cache memory, is a small but fast memory that stores data you want to access often (and adjacent)</p>
<p>Let&rsquo;s define some terminology:
$$
h_x = \textbf{Hit rate} = \text{Percentage of hits in memory x} \newline
(1 - h_x) = m_x = \textbf{Miss rate} = \text{Percentage of misses in memory x}
$$</p>
<p>$$
T_x = \textbf{Hit time} = \text{Acces time for memory x} \newline
(T_2 - T_1) = \textbf{Miss penalty} = \text{Miss penalty for a miss in memory 1}
$$</p>
<p>With these defined we can finally define what we will use the most, <em>Average Memory Access Time</em> (AMAT):
$$
\text{AMAT} = h_1\ T_1 + m_1\ T_2 = h_1\ T_1 + (1 - h_1)T_2
$$</p>
<h3 id="memory-hierarchy">Memory hierarchy</h3>
<p>When dealing with memory, we&rsquo;ll hear the word hierarchy - what this means is the different &ldquo;levels&rdquo;.</p>
<p>Our cache memory is the closest and fastest, after that we have our primary memory - and, in some cases we&rsquo;ll even have a &ldquo;secondary&rdquo; memory (hard drive).</p>
<p>You may also see the word level been thrown around. Our cache memory is the L1 memory. The primary memory can be divided into bigger L2-Lx caches.</p>
<p>We&rsquo;ll cover this in more detail later. But why this is the case - is just as before, due to the locality of reference.</p>
<p>If we keep our memory in &ldquo;blocks&rdquo; we can easily keep track of where &ldquo;groups&rdquo; of memory are located.</p>
<h3 id="cache-implementation">Cache implementation</h3>
<p>So, in reality, we keep track of memory blocks in our caches.</p>
<p>Let&rsquo;s say we have 128 bytes of memory, we divide our memory into block of 16 bytes, meaning 8 total blocks.</p>
<p>In our cache memory we only have space for <strong>four</strong> blocks. How can we store all 8?</p>
<p>Well, the most obvious answer is that we keep block 0 - 3 at block &ldquo;place&rdquo; 0 - 3. But what about 4 - 7?</p>
<p>We use modulo for that!</p>
<p>But then we need a way to differentiate block 0 with block 4 and so on?</p>
<p>We&rsquo;ll keep a &ldquo;tag block&rdquo; to indicate what block that is currently stored!</p>
<p>A rule of thumb is that:
$$
\frac{\text{Memory size}}{\text{Cache size}} = N \newline
$$</p>
<p>$$
\text{Tag bits} = log_2(N)
$$</p>
<p>So, therefore we have to always keep track of these four questions in mind:</p>
<ul>
<li><strong>Where</strong> shall the block be place?</li>
<li><strong>How do we find</strong> a specific block</li>
<li><strong>Which block</strong> shall be written when the place is needed</li>
<li>How do we handle writes (&quot;<strong>store</strong>&quot;)?</li>
</ul>
<p>So to solve these problems we have to:</p>
<ul>
<li>Have some <em>index</em> bits that indicates what <em>index</em> we have to look at, along with the tag and offset bits.</li>
<li>If we miss:
<ul>
<li>A block that already is stored at that position gets <strong>thrown out</strong>. If it is different from when it was read (dirty) - we need to write it back to the next level.</li>
<li>The searched block gets read into the cache and gets stored at the address <strong>index</strong>, and the required word gets delivered to the CPU.</li>
<li>This, not surprisingly, takes a lot of time.</li>
</ul>
</li>
</ul>
<h3 id="mapping-types">Mapping types</h3>
<p>We&rsquo;ve lightly covered one type of mapping for caches - the direct mapping.</p>
<p>Index in cache = (Block address) mod (Cache size)</p>
<p>But we also have <em>associative caches</em>.</p>
<ul>
<li>Each index points at a <em>set</em> of blocks spaces (setindex).</li>
<li>A block can be placed anywhere within a set.</li>
<li>More blocks with the same index can now be in the cache simultaneously.</li>
<li>Set size = <strong>associativity</strong></li>
</ul>
<p>Note: This is more costly than direct mapping!</p>
<p>So, a two-way set associative cache is equivalent to two parallel direct mapped caches.</p>
<h3 id="handle-writes-in-caches">Handle writes in caches</h3>
<p>We have two main strageties for this:</p>
<ul>
<li><strong>Write back</strong> (aka copy back)
<ul>
<li>Write only in cache</li>
<li>When a block needs to be thrown out, we write it (copy) to primary memory *only if it has been updated (this requires one extra &ldquo;dirty&rdquo; bit).</li>
</ul>
</li>
<li><strong>Write through</strong>
<ul>
<li>The block writes to PM as well (normally via a so-called &ldquo;Store buffer&rdquo;).</li>
</ul>
</li>
</ul>
<p>But what do we do when we have a so-called &ldquo;store miss&rdquo;?</p>
<ul>
<li>For a write-back cache:
<ul>
<li>Read in the block to the cache.</li>
</ul>
</li>
<li>For write-through cache:
<ul>
<li>Two strategies:
<ul>
<li>&ldquo;Allocate on miss&rdquo;: Read in the block to the cache.</li>
<li>&ldquo;Write around&rdquo;: Do not read in the block (Since, programs often write whole blocks before they get read, for example when initializing a program).</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="cache-misses-affect-cpi">Cache misses affect CPI</h3>
<p>Let&rsquo;s look at an example:</p>
<p>Given:</p>
<ul>
<li>Instruction cache miss rate = 2%</li>
<li>Data cache miss rate = 4%</li>
<li>Miss penalty = 100 clock cycles</li>
<li>Base CPI = 2</li>
<li>Loads &amp; stores take 36% of total instructions</li>
</ul>
<p>We can calculate:</p>
<ul>
<li>Miss cycles per instruction ($\Delta CPI_{cache}$)
<ul>
<li>I-cache misses: $\Delta CPI_{instruction} = 0.02 * 100 = 2$</li>
<li>D-cache misses: $\Delta CPI_{data} = 0.36 * 0.04 * 100 = 1.44$</li>
</ul>
</li>
</ul>
<p>Therefore, our total CPI is = 2 + 2 + 1.44 = 5.44!</p>
<h3 id="multiple-level-cache">Multiple-level cache</h3>
<ul>
<li>Primary cache (L1)
<ul>
<li>Focuses on <strong>minimal hit time</strong>.</li>
</ul>
</li>
<li>L2 Cache
<ul>
<li>Focuses on <strong>low miss rate</strong> to avoid accessing PM.</li>
</ul>
</li>
<li>Therefore:
<ul>
<li>L1 caches are often very small when dealing with multiple levels of caches, compared to having single caches.</li>
<li>L1&rsquo;s block size is often smaller than L2&rsquo;s.</li>
</ul>
</li>
</ul>
<h3 id="floating-point">Floating-point</h3>
<p>$$
x = (-1)^S \cdot\ (1 + Fraction) \cdot\ 2^{Exponent - Bias}
$$</p>

        </section>
    </article>

    <nav class="navigation"
        style="display: flex; justify-content: space-between; align-items: center; margin-top: 20px;">
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

        
        
        <a class="previous" href="/school/eda333/eda333_4/" title="Previous: Computer System Engineering: Part 4 - Pipeline hardware"
            style="flex-grow: 0; text-align: left;">&larr; Previous</a>
        
        

        
        
        <a class="next" href="/school/eda333/eda333_6/" title="Next: Computer System Engineering: Part 6 - Virtual Memory"
            style="flex-grow: 0; text-align: right;">Next &rarr;</a>
        
        
    </nav>
</main>
<footer id="site-footer">
    
    <a href="https://github.com/rezaarezvan" title="">github</a>
    
    <a href="https://x.com/rzvan__/" title="">x</a>
    <p class="footer_msg">memento mori</p></footer></div>
</body>

</html>
