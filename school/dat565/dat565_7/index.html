<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Astro v4.11.5"><link rel="icon" type="image" href="/favicon.ico"><title>Part 7 - Linear and logistic regression</title><!-- KaTeX support --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script><!-- inline KaTeX --><link rel="stylesheet" href="/_astro/index.D8eNQxos.css">
<link rel="stylesheet" href="/_astro/_slug_.YHQGI-k7.css">
<style>article[data-astro-cid-v5ro3oot]{max-width:80ch;margin:0 auto}.nav-button[data-astro-cid-v5ro3oot]{display:flex;align-items:center;padding:.5rem;border-radius:.5rem;transition:background-color .3s ease;text-decoration:none;color:var(--text-color);background-color:var(--bg-color);border:1px solid var(--border-color)}.nav-button[data-astro-cid-v5ro3oot]:hover{background-color:var(--hover-color)}.nav-button[data-astro-cid-v5ro3oot] .arrow[data-astro-cid-v5ro3oot]{font-size:1.5rem;line-height:1}.nav-button[data-astro-cid-v5ro3oot] .text[data-astro-cid-v5ro3oot]{display:flex;flex-direction:column;margin:0 .5rem}.nav-button[data-astro-cid-v5ro3oot] .label[data-astro-cid-v5ro3oot]{font-size:.8rem;text-transform:uppercase;letter-spacing:.05em;color:var(--muted-color)}.nav-button[data-astro-cid-v5ro3oot] .title[data-astro-cid-v5ro3oot]{font-weight:500}.prev-button[data-astro-cid-v5ro3oot]{justify-content:flex-start}.next-button[data-astro-cid-v5ro3oot]{justify-content:flex-end;text-align:right}@media (max-width: 640px){.nav-button[data-astro-cid-v5ro3oot]{width:100%}.next-button[data-astro-cid-v5ro3oot]{justify-content:flex-start;text-align:left}.next-button[data-astro-cid-v5ro3oot] .text[data-astro-cid-v5ro3oot]{order:2;margin-left:.5rem}.next-button[data-astro-cid-v5ro3oot] .arrow[data-astro-cid-v5ro3oot]{order:1}}
</style><script type="module">document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})});
</script></head> <body> <div class="container mx-auto px-4 flex flex-col md:flex-row min-h-screen"> <aside class="w-full md:w-64 border-b md:border-r md:border-b-0 border-[var(--border-color)] border-dashed pt-8"> <header class="flex flex-col h-full"> <div class="flex items-center mb-4"> <script>
  function setTheme(mode) {
    localStorage.setItem("theme-storage", mode);
    document.documentElement.setAttribute('data-theme', mode);
  }
  function toggleTheme() {
    const currentTheme = localStorage.getItem("theme-storage") || "light";
    const newTheme = currentTheme === "light" ? "dark" : "light";
    setTheme(newTheme);
  }
  const savedTheme = localStorage.getItem("theme-storage") || "light";
  setTheme(savedTheme);
  window.toggleTheme = toggleTheme;
</script> <button id="theme-toggle" onclick="toggleTheme()" aria-label="Toggle theme" class="w-6 h-6 cursor-pointer"> <div class="w-5 h-5 border-2 border-primary rounded-full transition-colors duration-300 ease-in-out hover:bg-primary"></div> </button> <a href="/" class="text-2xl font-semibold ml-3 h-10 pr-3">rezvan.xyz</a> </div> <nav class="flex flex-wrap gap-2 md:flex-col md:gap-2"> <a href="/principles" class="hover:text-orange-500 dark:hover:text-orange-500 transition-colors">
[principles]
</a><a href="/cv" class="hover:text-orange-500 dark:hover:text-orange-500 transition-colors">
[cv]
</a><a href="/posts" class="hover:text-orange-500 dark:hover:text-orange-500 transition-colors">
[posts]
</a><a href="/school" class="hover:text-orange-500 dark:hover:text-orange-500 transition-colors">
[school]
</a> </nav> </header> </aside> <main class="flex-grow px-4 md:px-8 py-8 overflow-y-auto">  <article class="prose prose-sm sm:prose lg:prose-lg xl:prose-xl max-w-none" data-astro-cid-v5ro3oot> <h1 class="text-3xl sm:text-4xl font-bold mb-4" data-astro-cid-v5ro3oot>Part 7 - Linear and logistic regression</h1> <p class="text-sm text-muted-foreground mb-4" data-astro-cid-v5ro3oot>
Date: 2/6/2024 </p> <div class="markdown-content" data-astro-cid-v5ro3oot>  <h3 id="introduction">Introduction</h3>
<p>In this part we’ll cover linear and logistic regression.</p>
<h3 id="core-data-science-tasks">Core data science tasks</h3>
<p>Regression is one of the core data science tasks:</p>
<ul>
<li>Regression
<ul>
<li>Predicting a numerical quantity</li>
</ul>
</li>
<li>Classification
<ul>
<li>Assigning a label from a discrete set of possibilities</li>
</ul>
</li>
<li>Clustering
<ul>
<li>Grouping items by similarity</li>
</ul>
</li>
</ul>
<p>We will cover clustering in a few parts.</p>
<h3 id="linear-regression">Linear regression</h3>
<p>Regression line is useful for visualization and a method for forecasting numerical values.
Residual error of a regression line is the difference between the predicted and actual values.</p>
<p>The objective is to find the line $y = f(x)$ which minimizes the residual error, we often we this as an optimization problem.</p>
<p>We want to fit a line $f(x) = \beta x + \epsilon$ to our data.
We want to select $\beta$ and $\epsilon$ so that the total error is minimal, but how do we define the error?</p>
<h3 id="least-squares-linear-regression">Least squares linear regression</h3>
<p>We can define the error of the line as the sum of the squared errors of the datapoints.
This is a somewhat arbitrary choice that is easy to work with, but we’ve seen that this is usually a good idea.
We want to find the line that minimizes this sum.</p>
<p>$$
\begin{align*}
error &#x26; = \sum_{i = 1}^n (y_i - f(x_i))^2 \newline
&#x26; = (y_i - (\beta x_i + \epsilon))^2 \newline
&#x26; = y_i^2 - 2y_i(\beta x_i + \epsilon) + (\beta x_i + \epsilon)^2 \newline
&#x26; = a \beta^2 + b \beta \epsilon + c \epsilon^2 + d
\end{align*}
$$</p>
<p>So we want to minimize this with respect to $\beta$ and $\epsilon$, let us use a bit of calculus:
$$
\begin{cases}
2\beta a + b \epsilon &#x26; = 0 \newline
2\epsilon c + b \beta &#x26; = 0
\end{cases}
$$</p>
<p>We solve for $\beta$ and $\epsilon$. In reality, all minimization of errors leads to an equation system.</p>
<h3 id="coefficient-of-determination">Coefficient of determination</h3>
<p>$$
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
$$</p>
<p>Where:
$$
SS_{res} = \sum_i (y_i - f_i)^2 = \sum_i e_i^2 \newline
SS_{tot} = \sum_i (y_i - \bar{y})^2
$$</p>
<h3 id="correlation">Correlation</h3>
<p>Pearson correlation:
$$
R = \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}} = \sum_{i = 1}^n \frac{x_i - \bar{x}}{s_x} \frac{y_i - \bar{y}}{s_y}
$$</p>
<p>Where $s_x$ and $s_y$ is the standard deviation of $x$ and $y$ respectively</p>
<p>Spearman correlation:
$$
\rho = 1 - \frac{6 \sum d_{i}^2}{n(n^2 - 1)}
$$</p>
<p>Where $n$ is the number of observations and $d_i$ is the difference between the two ranks of each observation.</p>
<h3 id="correlation-and-causation">Correlation and causation</h3>
<p>Correlation does not imply causation, simple.</p>
<h3 id="ridge-regression">Ridge regression</h3>
<p>Regularization is the trick of adding secondary terms to the objective function to favor models that keep coefficients small.
Suppose we generalize our loss function with a second set of terms that are a function of the coefficients, not the training data:
$$
J(w) = \frac{1}{2n} \sum_{i = 1}^n (y_i - f(x_i))^2 + \lambda \sum_{j = 1}^m w_{j}^2
$$</p>
<h3 id="lasso-regression">LASSO Regression</h3>
<p><strong>L</strong>east <strong>A</strong>bsolute <strong>S</strong>hrinkage and <strong>S</strong>election <strong>O</strong>perator.
Minimize the sum of the absolute values of the coefficients, which is just as happy to drive down the smallest coefficients as the big ones.</p>
<p>$$
J(w, t) = \frac{1}{2n} \sum_{i = 1}^n (y_i - f(x_i))^2 \text{ subject to } \sum_{j = 1}^m |w_j| \leq t
$$</p>
<h3 id="logistic-regression">Logistic regression</h3>
<p>Logistic regression is a powerful tool for modeling the probability of a binary outcome.
It is particularly useful when the dependent variable is categorical and the relationship between the independent variables and the probability of the outcome needs to be understood.</p>
<h3 id="logistic-function">Logistic Function</h3>
<p>The logistic function, also known as the sigmoid function, is used in logistic regression to map input values to a probability between 0 and 1.
The formula for the logistic function is:
$$
\sigma = \frac{1}{1 + e^{-x}}
$$</p>
<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<p>In logistic regression, the model parameters are estimated using maximum likelihood estimation.
The goal is to find the parameter values that maximize the likelihood of observing the data given the model.
This involves optimizing the log-likelihood function, which quantifies how well the model fits the data.</p>
<h3 id="model-evaluation">Model Evaluation</h3>
<p>Once the logistic regression model is trained, it is essential to evaluate its performance.
Common metrics for evaluating classification models include accuracy, precision, recall, F1 score, and the receiver operating characteristic (ROC) curve.</p>  </div> <nav class="flex flex-col sm:flex-row justify-between mt-8 pt-4 border-t border-border" data-astro-cid-v5ro3oot> <a href="/school/dat565/dat565_6" class="nav-button prev-button mb-4 sm:mb-0" data-astro-cid-v5ro3oot> <span class="arrow" data-astro-cid-v5ro3oot>←</span> <span class="text" data-astro-cid-v5ro3oot> <span class="label" data-astro-cid-v5ro3oot>Previous</span> <span class="title" data-astro-cid-v5ro3oot>Part 6 - Mathematical models</span> </span> </a> <a href="/school/dat565/dat565_8" class="nav-button next-button" data-astro-cid-v5ro3oot> <span class="text" data-astro-cid-v5ro3oot> <span class="label" data-astro-cid-v5ro3oot>Next</span> <span class="title" data-astro-cid-v5ro3oot>Part 8 - Distance and network methods</span> </span> <span class="arrow" data-astro-cid-v5ro3oot>→</span> </a> </nav> </article>  </main> </div> </body></html> 