<!DOCTYPE html>
<html><head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge"><title>rezvan | Part 7 - Linear and logistic regression</title><link rel="icon" type="image/png" href="https://rezvan.xyz/images/icon.png" /><meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
        content="Introduction In this part we&rsquo;ll cover linear and logistic regression.
Core data science tasks Regression is one of the core data science tasks:
Regression Predicting a numerical quantity Classification Assigning a label from a discrete set of possibilities Clustering Grouping items by similarity We will cover clustering in a few parts.
Linear regression Regression line is useful for visualization and a method for forecasting numerical values. Residual error of a regression line is the difference between the predicted and actual values." />
    <meta property="og:image" content="https://raw.githubusercontent.com/rezaarezvan/rezvan.xyz/main/images/icon.png" />
    <meta property="og:url" content="https://rezvan.xyz/school/dat565/dat565_7/">
  <meta property="og:site_name" content="rezvan">
  <meta property="og:title" content="Part 7 - Linear and logistic regression">
  <meta property="og:description" content="Introduction In this part we’ll cover linear and logistic regression.
Core data science tasks Regression is one of the core data science tasks:
Regression Predicting a numerical quantity Classification Assigning a label from a discrete set of possibilities Clustering Grouping items by similarity We will cover clustering in a few parts.
Linear regression Regression line is useful for visualization and a method for forecasting numerical values. Residual error of a regression line is the difference between the predicted and actual values.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="school">
    <meta property="article:published_time" content="2024-02-06T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-05-26T11:09:10+02:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Part 7 - Linear and logistic regression">
  <meta name="twitter:description" content="Introduction In this part we’ll cover linear and logistic regression.
Core data science tasks Regression is one of the core data science tasks:
Regression Predicting a numerical quantity Classification Assigning a label from a discrete set of possibilities Clustering Grouping items by similarity We will cover clustering in a few parts.
Linear regression Regression line is useful for visualization and a method for forecasting numerical values. Residual error of a regression line is the difference between the predicted and actual values.">
<script src="https://rezvan.xyz/js/feather.min.js"></script>
    

    
    
    <link rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/main.73f3431f38ac72fc2890b3747cc9c6d55d5756f3ba40e590c77627ae6c16e46b.css" />
    <link id="lightSyntaxStyle" rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/light_syntax.65408cc3a5c02070b661c3e4e79306fc261cc63620f4adce9a30eafcba4ab79e.css" />
    
    <link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://rezvan.xyz/css/dark.13c6bfb747b76fdd2dc198b1172afdd77e7ce604de43f343ce643378bafc58af.css"  disabled />
    <link id="darkSyntaxStyle" rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/dark_syntax.2b10cc1a2156b30874a063b7439a993bc3b43d476c5e1d8598d769c929c7b381.css" />
    

    
    
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

    
    <script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script>
    

    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>

    
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>
    
</head>
<body>
    <div class="content"><header>
    <nav id="site-navbar">
        <div class="menu-items">
            
            <a href="/">home</a>
            
            <a href="/principles">principles</a>
            
            <a href="/cv">cv</a>
            
            <a href="/posts">posts</a>
            
            <a href="/school">school</a>
            
        </div>
        <div class="theme-toggle">
            <span id="dark-mode-toggle" onclick="toggleTheme()"><i data-feather="moon"></i></span>
            <script src="https://rezvan.xyz/js/themetoggle.js"></script>
        </div>
        
    </nav>
</header>

<main>
    <article>
        <div class="title">
            <h1>Part 7 - Linear and logistic regression</h1>
            <div class="meta">Posted on Feb 6, 2024
            </div>
            <div class="meta">
                (Last updated: May 26, 2024)
            </div>

        </div>
        
        <section class="body">
            <h3 id="introduction">Introduction</h3>
<p>In this part we&rsquo;ll cover linear and logistic regression.</p>
<h3 id="core-data-science-tasks">Core data science tasks</h3>
<p>Regression is one of the core data science tasks:</p>
<ul>
<li>Regression
<ul>
<li>Predicting a numerical quantity</li>
</ul>
</li>
<li>Classification
<ul>
<li>Assigning a label from a discrete set of possibilities</li>
</ul>
</li>
<li>Clustering
<ul>
<li>Grouping items by similarity</li>
</ul>
</li>
</ul>
<p>We will cover clustering in a few parts.</p>
<h3 id="linear-regression">Linear regression</h3>
<p>Regression line is useful for visualization and a method for forecasting numerical values.
Residual error of a regression line is the difference between the predicted and actual values.</p>
<p>The objective is to find the line $y = f(x)$ which minimizes the residual error, we often we this as an optimization problem.</p>
<p>We want to fit a line $f(x) = \beta x + \epsilon$ to our data.
We want to select $\beta$ and $\epsilon$ so that the total error is minimal, but how do we define the error?</p>
<h3 id="least-squares-linear-regression">Least squares linear regression</h3>
<p>We can define the error of the line as the sum of the squared errors of the datapoints.
This is a somewhat arbitrary choice that is easy to work with, but we&rsquo;ve seen that this is usually a good idea.
We want to find the line that minimizes this sum.</p>
<p>$$
\begin{align*}
error &amp; = \sum_{i = 1}^n (y_i - f(x_i))^2 \\
&amp; = (y_i - (\beta x_i + \epsilon))^2 \\
&amp; = y_i^2 - 2y_i(\beta x_i + \epsilon) + (\beta x_i + \epsilon)^2 \\
&amp; = a \beta^2 + b \beta \epsilon + c \epsilon^2 + d
\end{align*}
$$</p>
<p>So we want to minimize this with respect to $\beta$ and $\epsilon$, let us use a bit of calculus:
$$
\begin{cases}
2\beta a + b \epsilon &amp; = 0 \\
2\epsilon c + b \beta &amp; = 0
\end{cases}
$$</p>
<p>We solve for $\beta$ and $\epsilon$. In reality, all minimization of errors leads to an equation system.</p>
<h3 id="coefficient-of-determination">Coefficient of determination</h3>
<p>$$
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
$$</p>
<p>Where:
$$
SS_{res} = \sum_i (y_i - f_i)^2 = \sum_i e_i^2 \\
SS_{tot} = \sum_i (y_i - \bar{y})^2
$$</p>
<h3 id="correlation">Correlation</h3>
<p>Pearson correlation:
$$
R = \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}} = \sum_{i = 1}^n \frac{x_i - \bar{x}}{s_x} \frac{y_i - \bar{y}}{s_y}
$$</p>
<p>Where $s_x$ and $s_y$ is the standard deviation of $x$ and $y$ respectively</p>
<p>Spearman correlation:
$$
\rho = 1 - \frac{6 \sum d_{i}^2}{n(n^2 - 1)}
$$</p>
<p>Where $n$ is the number of observations and $d_i$ is the difference between the two ranks of each observation.</p>
<h3 id="correlation-and-causation">Correlation and causation</h3>
<p>Correlation does not imply causation, simple.</p>
<h3 id="ridge-regression">Ridge regression</h3>
<p>Regularization is the trick of adding secondary terms to the objective function to favor models that keep coefficients small.
Suppose we generalize our loss function with a second set of terms that are a function of the coefficients, not the training data:
$$
J(w) = \frac{1}{2n} \sum_{i = 1}^n (y_i - f(x_i))^2 + \lambda \sum_{j = 1}^m w_{j}^2
$$</p>
<h3 id="lasso-regression">LASSO Regression</h3>
<p><strong>L</strong>east <strong>A</strong>bsolute <strong>S</strong>hrinkage and <strong>S</strong>election <strong>O</strong>perator.
Minimize the sum of the absolute values of the coefficients, which is just as happy to drive down the smallest coefficients as the big ones.</p>
<p>$$
J(w, t) = \frac{1}{2n} \sum_{i = 1}^n (y_i - f(x_i))^2 \text{ subject to } \sum_{j = 1}^m |w_j| \leq t
$$</p>
<h3 id="logistic-regression">Logistic regression</h3>
<p>Logistic regression is a powerful tool for modeling the probability of a binary outcome.
It is particularly useful when the dependent variable is categorical and the relationship between the independent variables and the probability of the outcome needs to be understood.</p>
<h3 id="logistic-function">Logistic Function</h3>
<p>The logistic function, also known as the sigmoid function, is used in logistic regression to map input values to a probability between 0 and 1.
The formula for the logistic function is:
$$
\sigma = \frac{1}{1 + e^{-x}}
$$</p>
<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<p>In logistic regression, the model parameters are estimated using maximum likelihood estimation.
The goal is to find the parameter values that maximize the likelihood of observing the data given the model.
This involves optimizing the log-likelihood function, which quantifies how well the model fits the data.</p>
<h3 id="model-evaluation">Model Evaluation</h3>
<p>Once the logistic regression model is trained, it is essential to evaluate its performance.
Common metrics for evaluating classification models include accuracy, precision, recall, F1 score, and the receiver operating characteristic (ROC) curve.</p>

        </section>
    </article>

    <nav class="navigation">
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

        
        

        <div class="nav-item previous">
            
            
            <a href="/school/dat565/dat565_6/" title="Previous: Part 6 - Mathematical models">&larr; Previous</a>
            
            
        </div>

        <div class="nav-item next">
            
            
            <a href="/school/dat565/dat565_8/" title="Next: Part 8 - Distance and network methods">Next &rarr;</a>
            
            
        </div>
    </nav>
</main>
<footer id="site-footer">
    <div class="social-links">
        
        <a href="https://github.com/rezaarezvan" title="">github</a>
        
        <a href="https://x.com/rzvan__/" title="">x</a>
        
    </div><p class="footer-msg">memento mori</p>
</footer></div>
</body>

</html>
