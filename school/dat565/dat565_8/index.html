<!DOCTYPE html>
<html><head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge"><title>rezvan | Introduction to data science and AI: Part 8 - Distance and network methods</title><link rel="icon" type="image/png" href="https://rezvan.xyz/images/icon.png" /><meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
        content="Introduction In data science, classification stands out as a fundamental technique. It involves the art of categorizing data into distinct classes based on various features and attributes.
Different type of classifiers Within the arsenal of classification algorithms, we encounter a diverse array of tools. From the simplicity of Logistic Regression to the complexity of Support Vector Machines, each classifier brings its unique strengths to the table.
Measuring Distances The most common metric for distance is euclidean distance: $$ d(p, q) = \sqrt{\sum_{i=1}^{n} |q_i - p_i|^2} $$" />
    <meta property="og:image" content="https://raw.githubusercontent.com/rezaarezvan/rezvan.xyz/main/images/icon.png" />
    <meta property="og:title" content="Introduction to data science and AI: Part 8 - Distance and network methods" />
<meta property="og:description" content="Introduction In data science, classification stands out as a fundamental technique. It involves the art of categorizing data into distinct classes based on various features and attributes.
Different type of classifiers Within the arsenal of classification algorithms, we encounter a diverse array of tools. From the simplicity of Logistic Regression to the complexity of Support Vector Machines, each classifier brings its unique strengths to the table.
Measuring Distances The most common metric for distance is euclidean distance: $$ d(p, q) = \sqrt{\sum_{i=1}^{n} |q_i - p_i|^2} $$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rezvan.xyz/school/dat565/dat565_8/" /><meta property="article:section" content="school" />
<meta property="article:published_time" content="2024-02-13T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-02-13T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Introduction to data science and AI: Part 8 - Distance and network methods"/>
<meta name="twitter:description" content="Introduction In data science, classification stands out as a fundamental technique. It involves the art of categorizing data into distinct classes based on various features and attributes.
Different type of classifiers Within the arsenal of classification algorithms, we encounter a diverse array of tools. From the simplicity of Logistic Regression to the complexity of Support Vector Machines, each classifier brings its unique strengths to the table.
Measuring Distances The most common metric for distance is euclidean distance: $$ d(p, q) = \sqrt{\sum_{i=1}^{n} |q_i - p_i|^2} $$"/>
<script src="https://rezvan.xyz/js/feather.min.js"></script>
    

    
    
    <link rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/main.e78c3f2ddb05c4f5f0aa7553861677149e3602644857702209900dcba4ebbdf7.css" />
    <link id="lightSyntaxStyle" rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/light_syntax.65408cc3a5c02070b661c3e4e79306fc261cc63620f4adce9a30eafcba4ab79e.css" />
    
    <link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://rezvan.xyz/css/dark.2b7c17d8b1c965837e6d0b727b269c478440b4aff7f6aa57b84e0dc8ddfd15dc.css"  disabled />
    <link id="darkSyntaxStyle" rel="stylesheet" type="text/css" media="screen" href="https://rezvan.xyz/css/dark_syntax.2b10cc1a2156b30874a063b7439a993bc3b43d476c5e1d8598d769c929c7b381.css" />
    

    
    
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

    
    <script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script>
    

    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>

    
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>
    
</head>
<body>
    <div class="content"><header>
    <nav id="site-navbar">
        
        <a href="/">home</a>
        
        <a href="/about">about</a>
        
        <a href="/principles">principles</a>
        
        <a href="/contact">contact</a>
        
        <a href="/cv">cv</a>
        
        <a href="/school">school</a>
        
        | <span id="dark-mode-toggle" onclick="toggleTheme()"></span>
        <script src="https://rezvan.xyz/js/themetoggle.js"></script>
        
    </nav>
</header>

<main>
    <article>
        <div class="title">
            <h1>Introduction to data science and AI: Part 8 - Distance and network methods</h1>
            <div class="meta">Posted on Feb 13, 2024</div>
        </div>
        
        <section class="body">
            <h3 id="introduction">Introduction</h3>
<p>In data science, classification stands out as a fundamental technique.
It involves the art of categorizing data into distinct classes based on various features and attributes.</p>
<h3 id="different-type-of-classifiers">Different type of classifiers</h3>
<p>Within the arsenal of classification algorithms, we encounter a diverse array of tools.
From the simplicity of Logistic Regression to the complexity of Support Vector Machines, each classifier brings its unique strengths to the table.</p>
<h3 id="measuring-distances">Measuring Distances</h3>
<p>The most common metric for distance is euclidean distance:
$$
d(p, q) = \sqrt{\sum_{i=1}^{n} |q_i - p_i|^2}
$$</p>
<p>Where $p$ and $q$ are two points in the dataset and $n$ is the number of features.</p>
<p>But let&rsquo;s define what a distance metric is, a distance metric must satisfy the following properties:</p>
<ul>
<li>Positivity: $d(p, q) \geq 0$ for all $p$ and $q$.</li>
<li>Identity: $d(p, q) = 0$ if and only if $p = q$.</li>
<li>Symmetry: $d(p, q) = d(q, p)$ for all $p$ and $q$.</li>
<li>Triangle inequality: $d(p, q) + d(q, r) \geq d(p, r)$ for all $p$, $q$ and $r$.</li>
</ul>
<p>The euclidean distance is a special case of a more general family, the $L_k$ distance:
$$
d(p, q) = \left(\sum_{i=1}^{n} |q_i - p_i|^k\right)^{1/k}
$$</p>
<h3 id="k-nearest-neighbours">$K$-nearest neighbours</h3>
<p>Among these classifiers we find, the $k$-nearest neighbours algorithm.
By seeking the consensus among the $k$ closest neighbours, this method navigates the data points to assign each point a class label.</p>
<h3 id="determining-the-optimal-k">Determining the optimal $k$</h3>
<p>If there are two classes, choosing an odd value for $k$ ensures no ties.
If $k$ is small, noise can have a greater influence.
Compute and compare <em>confusion matrices</em> for different values of $k$.</p>
<h3 id="data-clustering">Data Clustering</h3>
<p>Beyond classification lies the realm of data clustering, where patterns and structures emerge from the data&rsquo;s depths.
Through clustering, we gain insights into the underlying relationships and groupings that shape our understanding of the data landscape.</p>
<h3 id="clustering-methods">Clustering methods</h3>
<p>The $k$-means algorithm partitions the data into $k$ clusters.
It iteratively assigns each data point to the nearest cluster and recalculates the cluster centroids.</p>
<p>Hierarchical clustering, on the other hand, builds a tree of clusters.
It starts with each data point as a cluster and merges the closest clusters until only one cluster remains.</p>
<h4 id="k-means-clustering">$K$-means clustering</h4>
<p>The $k$-means algorithm is a simple yet powerful tool for clustering.</p>
<p>The algorithm works as follows:</p>
<ul>
<li>Choose value of $k$.</li>
<li>Choose initial positions of the $k$ cluster centres.</li>
<li>Choose a distance metric.</li>
<li>Assign each data point to the nearest cluster.</li>
<li>Recalculate the cluster centres as the mean of the data points in the cluster.</li>
<li>Repeat steps 4 and 5 until convergence.</li>
</ul>
<h5 id="the-elbow-method">The elbow method</h5>
<p>How do we pick the optimal value of $k$?</p>
<p>Let the diameter of a clustering be the longest intra-cluster distance.
The elbow method involves plotting the diameter of the clustering as a function of $k$.
The optimal value of $k$ is the value at the &ldquo;elbow&rdquo; of the curve.</p>
<h5 id="limitations-of-k-means">Limitations of $k$-means</h5>
<p>The $k$-means algorithm is sensitive to the initial choice of cluster centres.
It is also sensitive to the choice of $k$.
The algorithm is not suitable for clusters of different sizes and densities.</p>
<p>In summary, $k$-means works best for:</p>
<ul>
<li>Spherical clusters.</li>
<li>Equal diameter clusters.</li>
<li>Equal cluster size.</li>
</ul>
<h4 id="hierarchical-clustering">Hierarchical clustering</h4>
<p>Somtimes called agglomerative clustering, hierarchical clustering builds a tree of clusters.</p>
<p>The algorithm works as follows:</p>
<ul>
<li>Start with each data point as a cluster.</li>
<li>Merge the two closest clusters.</li>
<li>Repeat step 2 until only one cluster remains.</li>
</ul>
<h3 id="rand-index">Rand index</h3>
<p>Similarity measure between two clusters by considering all pairs of
samples and counting pairs that are assigned in the same or different
clusters in the predicted and true clusterings.</p>
<p>$$
RI = \frac{\text{Number of agreeing pairs}}{\text{Total number of pairs}}
$$</p>

        </section>
    </article>

    <nav class="navigation"
        style="display: flex; justify-content: space-between; align-items: center; margin-top: 20px;">
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

        
        
        <a class="previous" href="/school/dat565/dat565_7/" title="Previous: Introduction to data science and AI: Part 7 - Linear and logistic regression"
            style="flex-grow: 0; text-align: left;">&larr; Previous</a>
        
        

        
    </nav>
</main>
<footer id="site-footer">
    
    <a href="https://github.com/rezaarezvan" title="">github</a>
    
    <a href="https://x.com/rzvan__/" title="">x</a>
    <p class="footer_msg">memento mori</p></footer></div>
</body>

</html>
