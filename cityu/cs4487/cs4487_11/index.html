<!DOCTYPE html><html lang="en"> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/icon" href="/favicon.ico"><meta name="generator" content="Astro v5.4.2"><!-- Canonical URL --><link rel="canonical" href="https://rezvan.xyz/cityu/cs4487/cs4487_11/"><!-- Primary Meta Tags --><title>Part 11 - High-Level and Low-Level Vision Applications | machine learning | rezarezvan.com</title><meta name="title" content="Part 11 - High-Level and Low-Level Vision Applications | machine learning | rezarezvan.com"><meta name="description"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://rezvan.xyz/cityu/cs4487/cs4487_11/"><meta property="og:title" content="Part 11 - High-Level and Low-Level Vision Applications | machine learning | rezarezvan.com"><meta property="og:description"><meta property="og:image" content="https://rezvan.xyz/favicon.ico"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://rezvan.xyz/cityu/cs4487/cs4487_11/"><meta property="twitter:title" content="Part 11 - High-Level and Low-Level Vision Applications | machine learning | rezarezvan.com"><meta property="twitter:description"><meta property="twitter:image" content="https://rezvan.xyz/favicon.ico"><!-- PageFind --><link href="/pagefind/pagefind-ui.css" rel="stylesheet"><script src="/pagefind/pagefind-ui.js"></script><!-- KaTeX support --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.15/dist/katex.min.css" integrity="sha384-Htz9HMhiwV8GuQ28Xr9pEs1B4qJiYu/nYLLwlDklR53QibDfmQzi7rYxXhMH/5/u" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.15/dist/katex.min.js" integrity="sha384-bxmi2jLGCvnsEqMuYLKE/KsVCxV3PqmKeK6Y6+lmNXBry6+luFkEOsmp5vD9I/7+" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.15/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><!-- inline KaTeX --><script>
    function renderKaTeX() {
        if (typeof renderMathInElement !== "undefined") {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false },
                ],
            });
        }
    }

    document.addEventListener("DOMContentLoaded", renderKaTeX);
    document.addEventListener("astro:after-swap", renderKaTeX);
</script><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><script type="module" src="/_astro/ClientRouter.astro_astro_type_script_index_0_lang.DksKCMWR.js"></script><script>
    function init() {
        preloadTheme();
        onScroll();
        animate();
        updateThemeButtons();
        addCopyCodeButtons();
        setGiscusTheme();

        const backToTop = document.getElementById("back-to-top");
        backToTop?.addEventListener("click", (event) => scrollToTop(event));

        const backToPrev = document.getElementById("back-to-prev");
        backToPrev?.addEventListener("click", () => window.history.back());

        const lightThemeButton = document.getElementById("light-theme-button");
        lightThemeButton?.addEventListener("click", () => {
            localStorage.setItem("theme", "light");
            toggleTheme(false);
            updateThemeButtons();
        });

        const darkThemeButton = document.getElementById("dark-theme-button");
        darkThemeButton?.addEventListener("click", () => {
            localStorage.setItem("theme", "dark");
            toggleTheme(true);
            updateThemeButtons();
        });

        const systemThemeButton = document.getElementById(
            "system-theme-button",
        );
        systemThemeButton?.addEventListener("click", () => {
            localStorage.setItem("theme", "system");
            toggleTheme(
                window.matchMedia("(prefers-color-scheme: dark)").matches,
            );
            updateThemeButtons();
        });

        window
            .matchMedia("(prefers-color-scheme: dark)")
            .addEventListener("change", (event) => {
                if (localStorage.theme === "system") {
                    toggleTheme(event.matches);
                }
            });

        document.addEventListener("scroll", onScroll);
    }

    function updateThemeButtons() {
        const theme = localStorage.getItem("theme");
        const lightThemeButton = document.getElementById("light-theme-button");
        const darkThemeButton = document.getElementById("dark-theme-button");
        const systemThemeButton = document.getElementById(
            "system-theme-button",
        );

        function removeActiveButtonTheme(button) {
            button?.classList.remove("bg-black/5");
            button?.classList.remove("dark:bg-white/5");
        }

        function addActiveButtonTheme(button) {
            button?.classList.add("bg-black/5");
            button?.classList.add("dark:bg-white/5");
        }

        removeActiveButtonTheme(lightThemeButton);
        removeActiveButtonTheme(darkThemeButton);
        removeActiveButtonTheme(systemThemeButton);

        if (theme === "light") {
            addActiveButtonTheme(lightThemeButton);
        } else if (theme === "dark") {
            addActiveButtonTheme(darkThemeButton);
        } else {
            addActiveButtonTheme(systemThemeButton);
        }
    }

    function animate() {
        const animateElements = document.querySelectorAll(".animate");

        animateElements.forEach((element, index) => {
            setTimeout(() => {
                element.classList.add("show");
            }, index * 100);
        });
    }

    function onScroll() {
        if (window.scrollY > 0) {
            document.documentElement.classList.add("scrolled");
        } else {
            document.documentElement.classList.remove("scrolled");
        }
    }

    function scrollToTop(event) {
        event.preventDefault();
        window.scrollTo({
            top: 0,
            behavior: "smooth",
        });
    }

    function toggleTheme(dark) {
        const css = document.createElement("style");

        css.appendChild(
            document.createTextNode(
                `* {
             -webkit-transition: none !important;
             -moz-transition: none !important;
             -o-transition: none !important;
             -ms-transition: none !important;
             transition: none !important;
          }
        `,
            ),
        );

        document.head.appendChild(css);

        if (dark) {
            document.documentElement.classList.add("dark");
        } else {
            document.documentElement.classList.remove("dark");
        }

        window.getComputedStyle(css).opacity;
        document.head.removeChild(css);

        setGiscusTheme();
    }

    function preloadTheme() {
        const userTheme = localStorage.theme;

        if (userTheme === "light" || userTheme === "dark") {
            toggleTheme(userTheme === "dark");
        } else {
            toggleTheme(
                window.matchMedia("(prefers-color-scheme: dark)").matches,
            );
        }
    }

    function addCopyCodeButtons() {
        let copyButtonLabel = "📋";
        let codeBlocks = Array.from(document.querySelectorAll("pre"));

        async function copyCode(codeBlock, copyButton) {
            const codeText = codeBlock.innerText;
            const buttonText = copyButton.innerText;
            const textToCopy = codeText.replace(buttonText, "");

            await navigator.clipboard.writeText(textToCopy);
            copyButton.innerText = "✅";

            setTimeout(() => {
                copyButton.innerText = copyButtonLabel;
            }, 2000);
        }

        for (let codeBlock of codeBlocks) {
            const wrapper = document.createElement("div");
            wrapper.style.position = "relative";

            const copyButton = document.createElement("button");
            copyButton.innerText = copyButtonLabel;
            copyButton.classList = "copy-code";

            codeBlock.setAttribute("tabindex", "0");
            codeBlock.appendChild(copyButton);

            codeBlock.parentNode.insertBefore(wrapper, codeBlock);
            wrapper.appendChild(codeBlock);

            copyButton?.addEventListener("click", async () => {
                await copyCode(codeBlock, copyButton);
            });
        }
    }

    const setGiscusTheme = () => {
        const giscus = document.querySelector(".giscus-frame");

        const isDark = document.documentElement.classList.contains("dark");

        if (giscus) {
            const url = new URL(giscus.src);
            url.searchParams.set("theme", isDark ? "dark" : "light");
            giscus.src = url.toString();
        }
    };

    document.addEventListener("DOMContentLoaded", () => init());
    document.addEventListener("astro:after-swap", () => init());
    preloadTheme();
</script><link rel="stylesheet" href="/_astro/_subject_.WmXrNcmP.css">
<link rel="stylesheet" href="/_astro/index.C6eUsQXi.css">
<style>summary[data-astro-cid-xvrfupwn]{cursor:pointer;border-top-left-radius:.5rem;border-top-right-radius:.5rem;padding:.375rem .75rem;font-weight:500;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}summary[data-astro-cid-xvrfupwn]:hover{background-color:#0000000d}summary[data-astro-cid-xvrfupwn]:hover:is(.dark *){background-color:#ffffff0d}details[data-astro-cid-xvrfupwn][open] summary[data-astro-cid-xvrfupwn]{background-color:#0000000d}details[data-astro-cid-xvrfupwn][open] summary[data-astro-cid-xvrfupwn]:is(.dark *){background-color:#ffffff0d}
</style></head> <body> <header data-astro-transition-persist="astro-l7r54iwe-1"> <div class="mx-auto max-w-screen-sm px-3"> <div class="flex flex-wrap justify-between gap-y-4"> <div class="flex flex-col gap-y-2"> <a href="/" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out">  <div class="font-semibold"> rezarezvan.com </div>  </a> <div class="flex gap-x-2"> <button id="light-theme-button" aria-label="Light theme" class="group flex size-6 items-center justify-center rounded border border-black/15 hover:bg-black/5 focus-visible:bg-black/5 dark:border-white/20 dark:hover:bg-white/5 dark:focus-visible:bg-white/5"> <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="transition-colors duration-300 ease-in-out group-hover:animate-pulse group-hover:stroke-black group-focus-visible:animate-pulse group-focus-visible:stroke-black group-hover:dark:stroke-white dark:group-focus-visible:stroke-white"> <circle cx="12" cy="12" r="5"></circle> <line x1="12" y1="1" x2="12" y2="3"></line> <line x1="12" y1="21" x2="12" y2="23"></line> <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line> <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line> <line x1="1" y1="12" x2="3" y2="12"></line> <line x1="21" y1="12" x2="23" y2="12"></line> <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line> <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line> </svg> </button> <button id="dark-theme-button" aria-label="Dark theme" class="group flex size-6 items-center justify-center rounded border border-black/15 hover:bg-black/5 focus-visible:bg-black/5 dark:border-white/20 dark:hover:bg-white/5 dark:focus-visible:bg-white/5"> <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="transition-colors duration-300 ease-in-out group-hover:animate-pulse group-hover:stroke-black group-focus-visible:animate-pulse group-focus-visible:stroke-black group-hover:dark:stroke-white dark:group-focus-visible:stroke-white"> <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path> </svg> </button> <button id="system-theme-button" aria-label="System theme" class="group flex size-6 items-center justify-center rounded border border-black/15 hover:bg-black/5 focus-visible:bg-black/5 dark:border-white/20 dark:hover:bg-white/5 dark:focus-visible:bg-white/5"> <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="transition-colors duration-300 ease-in-out group-hover:animate-pulse group-hover:stroke-black group-focus-visible:animate-pulse group-focus-visible:stroke-black group-hover:dark:stroke-white dark:group-focus-visible:stroke-white"> <rect x="2" y="3" width="20" height="14" rx="2" ry="2"></rect> <line x1="8" y1="21" x2="16" y2="21"></line> <line x1="12" y1="17" x2="12" y2="21"></line> </svg> </button> </div> </div> <nav class="flex items-center gap-1 text-sm"> <a href="/posts" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> posts </a> <span>/</span> <a href="/chalmers" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> chalmers </a> <span>/</span> <a href="/cityu" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> cityu </a> <span>/</span> <a href="/pdf/cv/cv.pdf" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> cv </a> <span>/</span> <button id="magnifying-glass" aria-label="Search" class="flex items-center rounded border border-black/15 bg-neutral-100 px-2 py-1 text-xs transition-colors duration-300 ease-in-out hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:border-white/20 dark:bg-neutral-900 dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg height="16" stroke-linejoin="round" viewBox="0 0 16 16" width="16" style="color: currentcolor;"> <path fill-rule="evenodd" clip-rule="evenodd" d="M3.5 7C3.5 5.067 5.067 3.5 7 3.5C8.933 3.5 10.5 5.067 10.5 7C10.5 7.88461 10.1718 8.69256 9.63058 9.30876L9.30876 9.63058C8.69256 10.1718 7.88461 10.5 7 10.5C5.067 10.5 3.5 8.933 3.5 7ZM9.96544 11.0261C9.13578 11.6382 8.11014 12 7 12C4.23858 12 2 9.76142 2 7C2 4.23858 4.23858 2 7 2C9.76142 2 12 4.23858 12 7C12 8.11014 11.6382 9.13578 11.0261 9.96544L14.0303 12.9697L14.5607 13.5L13.5 14.5607L12.9697 14.0303L9.96544 11.0261Z" fill="currentColor"></path> </svg>
&nbsp;Search
</button> </nav> </div> </div> </header> <main>  <div class="mx-auto max-w-screen-sm px-3"> <div class="animate grid gap-4"> <a href="/cityu/cs4487" class="not-prose group relative flex w-fit flex-nowrap rounded border border-black/15 py-1.5 pl-7 pr-3 transition-colors duration-300 ease-in-out hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:border-white/20 dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="absolute left-2 top-1/2 size-4 -translate-y-1/2 fill-none stroke-current stroke-2"> <line x1="5" y1="12" x2="19" y2="12" class="translate-x-2 scale-x-0 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-hover:scale-x-100 group-focus-visible:translate-x-0 group-focus-visible:scale-x-100"></line> <polyline points="12 5 5 12 12 19" class="translate-x-1 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-focus-visible:translate-x-0"></polyline> </svg> <div class="text-sm"> 
Back to machine learning </div> </a> <div class="grid grid-cols-2 gap-1.5 sm:gap-3"> <a href="/cityu/cs4487/cs4487_10" class="group relative flex flex-nowrap rounded-lg border border-black/15 px-4 py-3 pl-10 no-underline transition-colors duration-300 ease-in-out hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:border-white/20 dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="absolute left-2 top-1/2 size-5 -translate-y-1/2 fill-none stroke-current stroke-2"> <line x1="5" y1="12" x2="19" y2="12" class="translate-x-3 scale-x-0 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-hover:scale-x-100 group-focus-visible:translate-x-0 group-focus-visible:scale-x-100"></line> <polyline points="12 5 5 12 12 19" class="translate-x-1 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-focus-visible:translate-x-0"></polyline> </svg> <div class="flex items-center text-sm"> Part 10 - Neural Networks and Deep Learning Part 2 </div> </a> <div class="invisible"></div> </div> </div> <div class="my-10 space-y-1"> <div class="animate flex items-center gap-1.5"> <div class="font-base text-sm">CS4487</div>
&bull;
<div class="font-base text-sm"> <time datetime="2024-11-20T00:00:00.000Z"> November 20, 2024 </time> </div> 
&bull;
<div class="font-base text-sm">
Last modified:  <time datetime="2024-11-26T14:28:49.000Z"> November 26, 2024 </time> </div> 
&bull;
<div class="font-base text-sm">16 min read</div> </div> <h1 class="animate text-3xl font-semibold text-black dark:text-white"> Part 11 - High-Level and Low-Level Vision Applications </h1> </div> <details open class="animate rounded-lg border border-black/15 dark:border-white/20" data-astro-cid-xvrfupwn> <summary data-astro-cid-xvrfupwn>Table of Contents</summary> <nav class="" data-astro-cid-xvrfupwn> <ul class="py-3" data-astro-cid-xvrfupwn> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#image-classification" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Image Classification </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#imagenet" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> ImageNet </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#performance-of-deep-learning" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Performance of Deep Learning </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#lenet-5-1998" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> LeNet-5 (1998) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#alexnet" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> AlexNet </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#vggnet" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> VGGNet </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#inception-module" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Inception Module </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#inceptionnet-v1" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> InceptionNet (V1) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#residual-learning" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Residual Learning </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#residual-network-resnet" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Residual Network (ResNet) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#resnext" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> ResNeXt </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#squueze-and-excitation-networks-senets" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Squueze-and-Excitation Networks (SENets) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#vison-transformers-vits" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Vison Transformers (ViTs) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#contrastive-language-image-pretraining-clip" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Contrastive Language-Image Pretraining (CLIP) </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#object-detection" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Object Detection </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#ms-coco" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> MS COCO </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#region-based-cnn-r-cnn" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Region-based CNN (R-CNN) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#fast-r-cnn" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Fast R-CNN </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#faster-r-cnn" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Faster R-CNN </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#object-detection-lots-of-variables" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Object Detection: Lots of Variables </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#semantic-segmentation" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Semantic Segmentation </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#pascal-voc" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> PASCAL VOC </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#fully-convolutional-networks-fcn" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Fully Convolutional Networks (FCN) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#deconv" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> DeConv </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#instance-segmentation" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Instance Segmentation </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#mask-r-cnn" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Mask R-CNN </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#model-comparison" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Model Comparison </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#maximum-discrepancy-mad-competition" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> MAximum Discrepancy (MAD) Competition </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#quantify-the-discrepancy" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Quantify the Discrepancy </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#low-level-vision" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Low-Level Vision </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#general-formulation" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> General Formulation </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#maximum-a-posteriori-map-estimation-for-image-enhancement" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Maximum a Posteriori (MAP) Estimation for Image Enhancement </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#denoising" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Denoising </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#denoising-by-residual-learning-dncnn" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Denoising by Residual Learning (DnCNN) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#bias-free-cnn-for-denoising" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Bias-Free CNN for Denoising </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#deblurring" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Deblurring </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#deblurring-by-estimating-blurring-kernel" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Deblurring by Estimating Blurring Kernel </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#super-resolution" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Super-Resolution </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#super-resolution-by-srcnn" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Super-Resolution by SRCNN </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#compression" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Compression </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#end-to-end-optimized-image-compression" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> End-to-end Optimized Image Compression </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#quantizer" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Quantizer </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#network-architecture" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Network Architecture </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#colorization" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Colorization </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#image-colorization-by-cnns" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Image Colorization by CNNs </a>  </li> </ul> </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#multi-exposure-image-fusion-mef" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Multi-Exposure Image Fusion (MEF) </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#multi-exposure-image-fusion-by-mef-net" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Multi-Exposure Image Fusion by MEF-Net </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#loss-functions" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Loss Functions </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#mean-squared-error-mse" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Mean Squared Error (MSE) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#why-do-we-love-mse" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Why Do We Love MSE? </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#what-is-wrong-with-mse" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> What is Wrong with MSE? </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#structural-similarity-ssim" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Structural Similarity (SSIM) </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#what-is-wrong-with-ssim" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> What is Wrong with SSIM? </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#learned-perceptual-image-patch-similarity-lpips" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Learned Perceptual Image Patch Similarity (LPIPS) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#deep-image-structure-and-texture-similarity-dists" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Deep Image Structure and Texture Similarity (DISTS) </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#summary" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Summary </a>  </li> </ul> </nav> </details> <article class="animate"> <h3 id="image-classification">Image Classification</h3>
<p>Goal: Given a photographic image, predict the object class (a.k.a., object recognition).
Typically, we only have one main object present, but we can have more (multi-label classification).</p>
<p>If enough classes are considered, then it is a generic high-level vision task.</p>
<p><img src="/images/cityu/CS4487/IC.jpg" alt="">
<strong>Figure 1:</strong> Image Classification</p>
<h4 id="imagenet">ImageNet</h4>
<p>ImageNet is a large-scale dataset for image classification. It has 1.2 million images and 1000 classes. It is used for the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).</p>
<p><img src="/images/cityu/CS4487/IN.jpg" alt="">
<strong>Figure 2:</strong> ImageNet</p>
<h4 id="performance-of-deep-learning">Performance of Deep Learning</h4>
<p>The introduction of ILSVRC coincided with the emergence of deep learning.</p>
<p>The top-5 error rates decreased as deeper neural networks were developed (note, not just deeper, but the architecture designs were smarter).</p>
<p><img src="/images/cityu/CS4487/ILSVRC.jpg" alt="">
<strong>Figure 3:</strong> History of ILSVRC</p>
<h4 id="lenet-5-1998">LeNet-5 (1998)</h4>
<p>LeNet-5 was one of the first convolutional neural networks (CNNs) for image classification. It was developed by Yann LeCun.</p>
<p>It has a total of 7 layers, it includes convolutions &#x26; pooling and a final fully-connected layer.
The architecture was designed for hand-written digit recongition (MNIST).</p>
<p><img src="/images/cityu/CS4487/LN.jpg" alt="">
<strong>Figure 4:</strong> LeNet-5 Architecture</p>
<h4 id="alexnet">AlexNet</h4>
<p>AlexNet was the first deep CNN to win the ILSVRC in 2012. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.</p>
<p>It has a similiar architecture to LeNet, but deeper (14 layers). It has $11 \times 11$, $5 \times 5$, and $3 \times 3$ convolutions.</p>
<p>But the architecture is not the only new thing, they used some clever new tricks as well.
For example, ReLU is used, they use so called ‘local response normalization’, dropout is utilized, as well as max pooling, some data augmentation techniques and SGD with momentum.</p>
<p>Many consider AlexNet as the start of the deep learning revolution.
Partly because of the deep(er) architecture and the tricks used along with it, but also since it was one of the first networks to be trained on GPUs.</p>
<p><img src="/images/cityu/CS4487/AN.jpg" alt="">
<strong>Figure 5:</strong> AlexNet Architecture</p>
<p>We can see that in Figure 5, it is split in two pipelines because it was trained on 2 GPUs simultaneously.</p>
<h4 id="vggnet">VGGNet</h4>
<p>VGGNet was the runner-up in ILSVRC 2014. It was developed by the Visual Geometry Group at the University of Oxford.</p>
<p>It has the same style as LeNet and AlexNet, but made some other design choices.</p>
<p>The VGGNet only uses $3 \times 3$ convolutional filters (since it is fewer parameters).</p>
<p>They stack several convolutional layers on top of each other before pooling.</p>
<p>But the main difference is that the number of feature channels doubles after each stage.
This ensures they can capture more higher-level features, VGG features are very effective in modeling aspects of human perception because of this.</p>
<p><img src="/images/cityu/CS4487/VGG.jpg" alt="">
<strong>Figure 6:</strong> VGGNet Architecture</p>
<h4 id="inception-module">Inception Module</h4>
<p>The Inception Module was introduced in the GoogLeNet architecture, which won ILSVRC 2014.</p>
<p>It is called a “Network-in-Network” architecture, because it has several convolutional layers in parallel.
The idea is that they extract features at different scales ($1 \times 1$, $3 \times 3$, $5 \times 5$).</p>
<p>Then they pool these features with a $3 \times 3$ max pooling layer.
Features are also concatenated and passed to the next block (which we will become more important later).</p>
<h4 id="inceptionnet-v1">InceptionNet (V1)</h4>
<p>The InceptionNet (V1) architecture is a combination of several Inception Modules.</p>
<p>It has 9 inception modules, 22 layers and 50 convolutional blocks.</p>
<p>The InceptionNet were made for auxiliary classification tasks.
It uses features in the middle of the network to perform classification.</p>
<h4 id="residual-learning">Residual Learning</h4>
<p>The network is learning a function (image to class).
We can think of this as we are building the function block-by-block.</p>
<p>We can allow blocks to learn a <em>residual</em>, which is added to the previous block.
This allows us to keep all the previous information and make small changes with the residual.</p>
<p>A novel intuition here is that this behaves like ensembles of relatively shallow networks.</p>
<h4 id="residual-network-resnet">Residual Network (ResNet)</h4>
<p>The ResNet architecture was introduced in 2015 and won ILSVRC 2015.</p>
<p>There are different sizes of ResNet, but the most famous one is the ResNet-50 (50 layers).
ResNet uses $3 \times 3$ filters and have residual connections every two layers.</p>
<h4 id="resnext">ResNeXt</h4>
<p>ResNeXt combines the split-transform-aggregate strategy in the Inception network and the residual learning in ResNet.</p>
<p>The number of paths inside the ResNeXt block is defined as <strong>cardinality</strong>.
All the paths contain the same topology.</p>
<p>Instead of having high depth and width, having high cardinality helps in decreasing validation error.</p>
<h4 id="squueze-and-excitation-networks-senets">Squueze-and-Excitation Networks (SENets)</h4>
<p>A block for CNNs that improves channel interdependencies.
Adds a parameter to each channel of a convolutional block so that the network adaptively adjusts the weighting of each feature map.</p>
<h4 id="vison-transformers-vits">Vison Transformers (ViTs)</h4>
<p>Vison Transformers (ViTs) emerge as a competetive alternative to CNNs that are currently the state-of-the-art (SOTA) in computer vision and widely used for various image recognition tasks.</p>
<p>Three major processing elements in transformer encoder: Layer normalization, multi-head attention, and multi-layer perceptron (MLP).</p>
<p><img src="/images/cityu/CS4487/VIT.jpg" alt="">
<strong>Figure 7:</strong> Vision Transformer</p>
<h4 id="contrastive-language-image-pretraining-clip">Contrastive Language-Image Pretraining (CLIP)</h4>
<p>CLIP is an open-source, multi-modal, zero-shot model.</p>
<p>It uses both a text encoder (transformers) and a image encoder (ViT or ResNet architectures).</p>
<p>These encoders are trained to maximize the similarity of a dataset of 400 million (image, text) pairs.</p>
<h3 id="object-detection">Object Detection</h3>
<p>Goal: Identify and locate objects within an image or video.
Not only does this involve recognizing the object categories within an image, but also accurately detemining their locations in it.</p>
<p><img src="/images/cityu/CS4487/OD.jpg" alt="">
<strong>Figure 8:</strong> Object Detection</p>
<p>Typically, represented as bounding boxes, as seen in Figure 8.</p>
<h4 id="ms-coco">MS COCO</h4>
<p>Microsoft Common Objects in Context (MS COCO) is a large-scale dataset for object detection, segmentation, and captioning.</p>
<p>It consists of over a million images, encompassing 80 different object categories.</p>
<h4 id="region-based-cnn-r-cnn">Region-based CNN (R-CNN)</h4>
<p>R-CNN was the first deep learning model for object detection.</p>
<p>It consists of four steps:</p>
<ol>
<li>Region Proposal</li>
<li>Feature Extraction</li>
<li>Classification</li>
<li>Bounding Box Regression</li>
</ol>
<p>But R-CNN has some limitations.
We can get bad candidate region proposals, since we are using a selective search algorithm.</p>
<p>It is also very time-consuming, 2000 forward propagations for each image.</p>
<h4 id="fast-r-cnn">Fast R-CNN</h4>
<p>Fast R-CNN was introduced to address the limitations of R-CNN.</p>
<p>The four steps are instead:</p>
<ol>
<li>Input and Convolutional Feature Extraction</li>
<li>Region Proposal</li>
<li>Region of Interest (RoI) Pooling</li>
<li>Classification and Bounding Box Regression</li>
</ol>
<p>The RoI pooling layer is used to extract a fixed-size feature map from the feature map of the CNN.</p>
<p>However, Fast R-CNN still has some limitations.
Since the RoI are fixed in size, this will reduce accuracy for objects of different scales or aspect ratios.</p>
<p>Also the region proposal is a non-learnable step.</p>
<h4 id="faster-r-cnn">Faster R-CNN</h4>
<p>This is the third iteration of the R-CNN family.</p>
<ol>
<li>Input and Convolutional Feature Extraction</li>
<li>Region Proposal Network</li>
<li>Region of Interest (RoI) Pooling</li>
<li>Classification and Bounding Box Regression</li>
</ol>
<p>The Region Proposal Network (RPN) is a learnable network that predicts the region proposals.</p>
<h4 id="object-detection-lots-of-variables">Object Detection: Lots of Variables</h4>
<p>One-stage, two-stage, and Transformer series are three main frameworks in object detection.</p>
<ul>
<li>One-stage focus on speed and may sacrifice some accuracy.</li>
<li>Two-stage has high accuracy, but the speed is relatively slow.</li>
<li>Transformer series is based on Vision Transformers.</li>
</ul>
<p>Which framework to choose depends on the application and performance requirements.</p>






























<table><thead><tr><th><strong>One-Stage</strong></th><th><strong>Two-Stage</strong></th><th><strong>Transformer Series</strong></th></tr></thead><tbody><tr><td>YOLOv1</td><td>R-CNN</td><td>DETR</td></tr><tr><td>YOLOv2</td><td>Fast R-CNN</td><td>DN-DETR</td></tr><tr><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td></tr><tr><td>YOLOv8</td><td>Faster R-CNN</td><td>Focus-DETR</td></tr></tbody></table>
<h3 id="semantic-segmentation">Semantic Segmentation</h3>
<p>Goal: Assign a semantic class label to each pixel in an image.</p>
<p><img src="/images/cityu/CS4487/SS.jpg" alt="">
<strong>Figure 9:</strong> Semantic Segmentation</p>
<h4 id="pascal-voc">PASCAL VOC</h4>
<p>PASCAL visual object classes (VOC) is a dataset for object detection, segmentation, and classification.</p>
<p>It consists of 20 object classes, 9.9k (VOC2007) and 23k (VOC2012) images, and 24.6k (VOC2007) and 54.9k (VOC2012) objects.</p>
<h4 id="fully-convolutional-networks-fcn">Fully Convolutional Networks (FCN)</h4>
<p>Fully Convolutional Networks (FCN) is a framework for image semantic segmentation.
The core idea is:</p>
<ul>
<li>A fully convolutional network without fully connected layers, capable of adapting to inputs of arbitrary sizes.</li>
<li>A skip architecture that combines results from different depth layers while ensuring both robustness and precision.</li>
</ul>
<p><img src="/images/cityu/CS4487/FCN.jpg" alt="">
<strong>Figure 10:</strong> Fully Convolutional Network</p>
<p>However, the limitation of FCNs are that it directly upsamples the compact features.</p>
<h4 id="deconv">DeConv</h4>
<p>DeConv proposes a deconvolution network with unpooling operation to predict the segmentation map.</p>
<p>First, the convolution network downsamples the feature representations.
Then, the deconvolution network upsamples the compact features maps and refine the dense predictions.</p>
<h3 id="instance-segmentation">Instance Segmentation</h3>
<p>Goal: Detect objects in an image and label each pixel at the same time.</p>
<p><img src="/images/cityu/CS4487/IS.jpg" alt="">
<strong>Figure 11:</strong> Instance Segmentation</p>
<p>Compared with object detection, it outputs a mask instead of a bounding box.
Compared with semantic segmentation, it distinguishes between different instances in the same class.</p>
<h4 id="mask-r-cnn">Mask R-CNN</h4>
<p>Mask R-CNN is a framework for instance segmentation.</p>
<ol>
<li>Input and Convolutional Feature Extraction</li>
<li>Region Proposal Network (RPN)</li>
<li>Region of Interest (RoI) Align</li>
<li>Classification, Bounding Box Regression, and Mask Prediction</li>
</ol>
<p>The RoI Align layer is used to extract a fixed-size feature map from the feature map of the CNN.</p>
<h3 id="model-comparison">Model Comparison</h3>
<p>The conventional model comparison methodology is,</p>
<ul>
<li>Pre-select a number of images from the space of all possible natural images (i.e., natural images manifold) to form the test set.</li>
<li>Collect the human label for each image in the test set to identify its ground-truth category.</li>
<li>Rank the competing classifiers according to their goodness of fit (e.g., accuracy) on the test set.</li>
</ul>
<p>However, as always we will have limitations.</p>
<ul>
<li>The test sets are small, fixed, and extensively reused.</li>
<li>More fundamentally, the underlying philosophy is to <strong>prove</strong> a classifier to be correct, which is impossible to achieve.</li>
</ul>
<h4 id="maximum-discrepancy-mad-competition">MAximum Discrepancy (MAD) Competition</h4>
<p>MAximum Discrepancy (MAD) attempts to <strong>falsify</strong> two classifiers by maximizing their prediction discrepancy.</p>
<p>A classifier that is harder to be falsified in MAD is considered to be better.</p>
<h4 id="quantify-the-discrepancy">Quantify the Discrepancy</h4>
<p>We can not use the zero-one loss to quantify the discrepancy between two classifiers.</p>
<ul>
<li>The zero-one loss is not continuous and not differentiable.</li>
<li>Lack of sensitivity to the prediction confidence.</li>
<li>Instability and lack of robustness.</li>
</ul>
<h3 id="low-level-vision">Low-Level Vision</h3>
<p>Low-level vision tasks are the fundamental building blocks of high-level vision tasks.
For example, image denoising, image deblurring, image super-resolution, and image inpainting.</p>
<h4 id="general-formulation">General Formulation</h4>
<p>$$
\mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{e}.
$$</p>
<p>where $\mathbf{x}$ is the signal to be recovered.</p>
<p>$\mathbf{e}$ is the measurement noise.</p>
<p>$\mathbf{A}$ is the Sensing Matrix.</p>
<p>$\mathbf{y}$ is the measurements.</p>
<p><em>Fidelity</em> is the closeness between the recovered signal and the original signal, i.e.,</p>
<p>$$
\underset{\mathbf{x}}{\min} \Vert \mathbf{y} - \mathbf{A} \mathbf{x} \Vert_2^2
$$</p>
<p>However, we can’t just rely on fidelity, we can obtain the same fidelity with different solutions.
Where one solution is better than the other. Thus, prior knowledge is needed.</p>
<h4 id="maximum-a-posteriori-map-estimation-for-image-enhancement">Maximum a Posteriori (MAP) Estimation for Image Enhancement</h4>
<p>$$
\underset{\mathbf{x}}{\min} \Vert \mathbf{y} - \mathbf{A} \mathbf{x} \Vert_2^2 + \lambda \phi(\mathbf{x})
$$</p>
<p>where $\mathbf{x}$ is the signal to be recovered.</p>
<p>$\mathbf{e}$ is the measurement noise.</p>
<p>$\mathbf{A}$ is the Sensing Matrix.</p>
<p>$\mathbf{y}$ is the measurements.</p>
<p>$\phi(\mathbf{x})$ is the signal regularizer (i.e., image prior).</p>
<h4 id="denoising">Denoising</h4>
<p>Denoising is the process of removing noise from a signal.</p>
<p>Given,</p>
<p>$$
\mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{e}
$$</p>
<p>$\mathbf{A} = \mathbf{I}$ (identity matrix).</p>
<p>Thus, we only have the noisy measurements $\mathbf{y} = \mathbf{x} + \mathbf{e}$.</p>
<p>This is the simplest and most “boring” low-level vision problem.</p>
<h5 id="denoising-by-residual-learning-dncnn">Denoising by Residual Learning (DnCNN)</h5>
<p>The DnCNN is a deep learning-based denoising method.</p>
<p>It utilizes residual learning to learn the noise from the noisy image.
MSE is the perfect loss in this setting and it is easy to optimize.</p>
<h5 id="bias-free-cnn-for-denoising">Bias-Free CNN for Denoising</h5>
<p>The key technical feature here is that we remove all the bias term from the DnCNN.</p>
<p>This makes it interpretable via linear algebra tools and generalizable to noise levels beyond the training range.</p>
<h4 id="deblurring">Deblurring</h4>
<p>Deblurring is the process of removing blur from a signal.</p>
<p>Given,</p>
<p>$$
\mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{e}
$$</p>
<p>$\mathbf{A}$ acts as a blurring operator (i.e., convolution with a blur kernel).</p>
<h5 id="deblurring-by-estimating-blurring-kernel">Deblurring by Estimating Blurring Kernel</h5>
<p>The key idea here is to estimate the blur kernel from the blurred image.
Here, we can also use the MSE as loss function.</p>
<h4 id="super-resolution">Super-Resolution</h4>
<p>Super-resolution is the process of enhancing the resolution of an image.</p>
<p>Given,</p>
<p>$$
\mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{e} = \mathbf{D} \mathbf{H} \mathbf{x} + \mathbf{e}
$$</p>
<p>$\mathbf{H}$ is a blurring operator and $\mathbf{D}$ is a down-sampling operator.</p>
<h5 id="super-resolution-by-srcnn">Super-Resolution by SRCNN</h5>
<p>SRCNN maps low-resolution features nonlineraly to high-resolution representations.
This ensures we have good conceptual connections to previous methods.</p>
<p>Again, we use the MSE loss here as well.</p>
<h4 id="compression">Compression</h4>
<p>Image compression minimizes its size (in bytes) without degrading the image quality below an acceptable threshold.</p>
<h5 id="end-to-end-optimized-image-compression">End-to-end Optimized Image Compression</h5>
<p>We can jointly optimize a weighted sum of the rate and distortion.</p>
<ul>
<li>Rate ($R$) as the discrete entropy and distortion ($D$) as the MSE.</li>
<li>Loss function: $R + \lambda D$ (where $\lambda$ can be fixed or learnable).</li>
<li>Backpropagate through the non-differentiable quantizer by adding uniform noise.</li>
</ul>
<h5 id="quantizer">Quantizer</h5>
<p>The quantizer is a non-differentiable operation.</p>
<p>$$
\hat{y_i} = \text{round}(y_i) \text{ and } p_{\hat{y_i}}(n) = \int_{n - \frac{1}{2}}^{n + \frac{1}{2}} p_{y_i}(t) dt, \text{ for all } n \in \mathcal{Z}
$$</p>
<p>$$
\tilde{y_i} = y_i + \Delta y_i, \text{ for } \Delta y_i \sim \mathcal{U}(-\frac{1}{2}, \frac{1}{2})
$$</p>
<p>The density function of $\tilde{y_i}$ is a continuous relaxtion of the probability mass function of $\hat{y_i}$, identical at integer values.</p>
<p>Independent uniform noise is frequently used as a model of quantization error in signal processing.</p>
<h5 id="network-architecture">Network Architecture</h5>
<p>Generalized Divisive Normalization (GDN) is used to normalize the input.</p>
<p>Let $\mathbf{x} \in \mathbb{R}^N$ be the input vector to GDN, and the output response $\mathbf{z} \in \mathbb{R}^N$ can be computed by,
$$
z_i = \frac{x_i}{\left(\beta_i + \sum_{j=1}^N \gamma_{ji} x_j^2 \right)^\frac{1}{2}}
$$</p>
<p>where the weight matrix $\gamma \in \mathbb{R}^{N \times N}$ and the bias vector $\beta \in \mathbb{R}^N$ are parameters in GDN to be optimized.</p>
<h4 id="colorization">Colorization</h4>
<p>Grey-level images are obtained by dropping the color representation of color images.</p>
<h5 id="image-colorization-by-cnns">Image Colorization by CNNs</h5>
<p>Predict a probability distribution of discretized ab values.</p>
<ul>
<li>CIE LAB is a more perceptually uniform color space than RGB.</li>
<li>Cast a regression task into a multiclass classification task top cope with the inherent ambiguity and multimodal nature of colorization.</li>
</ul>
<h3 id="multi-exposure-image-fusion-mef">Multi-Exposure Image Fusion (MEF)</h3>
<p>MEF takes an image sequence with different exposure levels as input and produces a high-quality image with richer details.</p>
<p>This is mainly accomplished by a weighted summation framework $\mathbf{y} = \sum_{k=1}^K \mathbf{w_k} \odot \mathbf{x_k}$.</p>
<h4 id="multi-exposure-image-fusion-by-mef-net">Multi-Exposure Image Fusion by MEF-Net</h4>
<p>Predict downsampled weight maps, followed by guided filtering for upsampling.</p>
<p>MSE is not applicable here, MEF structural similarity (MEF-SSIM) index as the objective function instead.
It is however, perceptually optimized.</p>
<h3 id="loss-functions">Loss Functions</h3>
<p>We have seen that the MSE loss is the most common loss function in low-level vision tasks.
However, one can use many.</p>
<ul>
<li>Mean Squared Error (MSE)</li>
<li>Structural Similarity (SSIM)</li>
<li>learned Perceptual Image Patch Similarity (LPIPS)</li>
<li>Deep Image Structure and Texture Similarity (DISTS)</li>
<li>$\vdots$</li>
</ul>
<h4 id="mean-squared-error-mse">Mean Squared Error (MSE)</h4>
<p>$$
\text{MSE} = \frac{1}{N} \sum_{i=1}^N (\mathbf{x_i} - \hat{\mathbf{x_i}})^2
$$</p>
<p>where $\mathbf{x_i}$ is the ground-truth and $\hat{\mathbf{x_i}}$ is the prediction.</p>
<h4 id="why-do-we-love-mse">Why Do We Love MSE?</h4>
<p>It is simple, parameter-free, cheap to compute and also memory-less!</p>
<p>It is per definition a valid <em>distance</em> metric (i.e., satisfying non-negativity, identiy, symmetry, and triangular inequality).</p>
<p>It also has a clear physical meaning as energy.
It is simply an excellent metric in the context of optimization (convex, differentiable, often admits closed-form analytical solutions).</p>
<h4 id="what-is-wrong-with-mse">What is Wrong with MSE?</h4>
<p>In the context of computer vision, MSE does not care about pixel ordering, which in many tasks is important.</p>
<p>It cares about the pixel <em>difference</em>, but not about the underlying signal, which again, is important in computer vision tasks.
The same goes for the sign of the pixel difference.</p>
<p>Further, MSE implicitly assumes that errors are statistically independent.
Which is true <em>if</em> spatial dependencies are eliminated prior to computation.
But no easy task as natural images are highly structured (i.e., spatially correlated).</p>
<p>We can however find a solution for the flaws in MSE, we can learn a “perceptual transform”, denote this $\mathbf{f}$.</p>
<p>$$
D(\mathbf{x_i}, \hat{\mathbf{x_i}}) = \frac{1}{N} \sum_{i=1}^N (\mathbf{f}(\mathbf{x})_i - \mathbf{f}(\hat{\mathbf{x}})_i)^2
$$</p>
<p>What are some desirable properties of $\mathbf{f}$?</p>
<ul>
<li>It should be differentiable.</li>
<li>It should be invariant to small changes.</li>
<li>It should be sensitive to large changes.</li>
<li>It should be able to capture the perceptual similarity between images.</li>
</ul>
<h4 id="structural-similarity-ssim">Structural Similarity (SSIM)</h4>
<p>The SSIM index is a perceptual metric that quantifies the image quality degradation that is caused by processing such as data compression or by losses in data transmission.</p>
<p>$$
\text{SSIM}(\mathbf{x}, \hat{\mathbf{x}}) = l(\mathbf{x}, \hat{\mathbf{x}}) \cdot s(\mathbf{x}, \hat{\mathbf{x}}) = \left(\frac{2 \mu_{\mathbf{x}} \mu_{\hat{\mathbf{x}}} + c_1}{\mu_{\mathbf{x}}^2 + \mu_{\hat{\mathbf{x}}} + c_1}\right) \cdot \left( \frac{2 \sigma_{\mathbf{x} \hat{\mathbf{x}}} + c_2}{\sigma_{\mathbf{x}}^2 + \sigma_{\hat{\mathbf{x}}}^2 + c_2} \right)
$$</p>
<h5 id="what-is-wrong-with-ssim">What is Wrong with SSIM?</h5>
<p>Normalization is sensitive to low intensities.</p>
<p>It does not consider the chrominance (color) information.</p>
<p>Relies on patch-by-patch comparison, which is not ideal for global image quality assessment.</p>
<h4 id="learned-perceptual-image-patch-similarity-lpips">Learned Perceptual Image Patch Similarity (LPIPS)</h4>
<p>LPIPS is a learned metric that measures perceptual similarity between two images.</p>
<p>$$
\text{LPIPS}(\mathbf{x}, \hat{\mathbf{x}}) = \sum_{i=1}^S \sum_{j=1}^{N_i} w_{ij} \text{ MSE}(\mathbf{f}(\mathbf{x})_j^{(i)}, \mathbf{f}(\hat{\mathbf{x}})_j^{(i)})
$$</p>
<h4 id="deep-image-structure-and-texture-similarity-dists">Deep Image Structure and Texture Similarity (DISTS)</h4>
<p>DISTS is a learned metric that measures perceptual similarity between two images.</p>
<p>$$
\text{DISTS}(\mathbf{x}, \hat{\mathbf{x}}) = 1 - \sum_{i=0}^S \sum_{j=1}^{N_i} \left( \alpha_{ij} l\left( \mathbf{x_j}^{(i)}, \hat{\mathbf{x_j}}^{(i)} \right) + \beta_{ij} s\left( \mathbf{x_j}^{(i)}, \hat{\mathbf{x_j}}^{(i)} \right) \right)
$$</p>
<h3 id="summary">Summary</h3>
<p>Advances of deep learning has been driven by the ImageNet competition.</p>
<p>As depth increases, need to have a smart architecture design to make training more effective.</p>
<p>Deep learning also beigns to dominate low-level vision, e.g., image denoising, deblurring, super-resolution, compression etc.</p> <div class="mt-24"> <div class="grid grid-cols-2 gap-1.5 sm:gap-3"> <a href="/cityu/cs4487/cs4487_10" class="group relative flex flex-nowrap rounded-lg border border-black/15 px-4 py-3 pl-10 no-underline transition-colors duration-300 ease-in-out hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:border-white/20 dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="absolute left-2 top-1/2 size-5 -translate-y-1/2 fill-none stroke-current stroke-2"> <line x1="5" y1="12" x2="19" y2="12" class="translate-x-3 scale-x-0 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-hover:scale-x-100 group-focus-visible:translate-x-0 group-focus-visible:scale-x-100"></line> <polyline points="12 5 5 12 12 19" class="translate-x-1 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-focus-visible:translate-x-0"></polyline> </svg> <div class="flex items-center text-sm"> Part 10 - Neural Networks and Deep Learning Part 2 </div> </a> <div class="invisible"></div> </div> </div> <div class="mt-24"> <div class="giscus"></div> <script data-astro-rerun src="https://giscus.app/client.js" data-repo="rezaarezvan/rezvan.xyz" data-repo-id="R_kgDOHvQr3w" data-category="General" data-category-id="DIC_kwDOHvQr384CiWVC" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="en" data-loading="lazy" crossorigin="anonymous" async></script> </div> </article> </div>  </main> <footer class="animate"> <div class="mx-auto max-w-screen-sm px-3"> <div class="relative"> <div class="absolute -top-12 right-0"> <button id="back-to-top" class="group relative flex w-fit flex-nowrap rounded border border-black/15 py-1.5 pl-8 pr-3 transition-colors duration-300 ease-in-out hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:border-white/20 dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="absolute left-2 top-1/2 size-4 -translate-y-1/2 rotate-90 fill-none stroke-current stroke-2"> <line x1="5" y1="12" x2="19" y2="12" class="translate-x-2 scale-x-0 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-hover:scale-x-100 group-focus-visible:translate-x-0 group-focus-visible:scale-x-100"></line> <polyline points="12 5 5 12 12 19" class="translate-x-1 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-focus-visible:translate-x-0"></polyline> </svg> <div class="text-sm">Back to top</div> </button> </div> </div> <div class="flex items-center justify-between"> <div>&copy; 2025 • rezarezvan.com</div> <div class="flex flex-wrap items-center gap-1.5"></div> </div> </div> </footer> <aside data-pagefind-ignore> <div id="backdrop" class="bg-[rgba(0, 0, 0, 0.5] invisible fixed left-0 top-0 z-50 flex h-screen w-full justify-center p-6 backdrop-blur-sm" data-astro-transition-persist="astro-3snakcvo-2"> <div id="pagefind-container" class="m-0 flex h-fit max-h-[80%] w-full max-w-screen-sm flex-col overflow-auto rounded border border-black/15 bg-neutral-100 p-2 px-4 py-3 shadow-lg dark:border-white/20 dark:bg-neutral-900"> <div id="search" class="pagefind-ui pagefind-init" data-pagefind-ui data-bundle-path="/pagefind/" data-ui-options="{&#34;showImages&#34;:false,&#34;excerptLength&#34;:15,&#34;resetStyles&#34;:false}"></div> <script type="module" src="/_astro/Search.astro_astro_type_script_index_0_lang.C4tRTXsn.js"></script> <div class="mr-2 pb-1 pt-4 text-right text-xs dark:prose-invert">
Press <span class="prose text-xs dark:prose-invert"><kbd class="">Esc</kbd></span> or click anywhere to close
</div> </div> </div> </aside> <script>
  const magnifyingGlass = document.getElementById("magnifying-glass");
  const backdrop = document.getElementById("backdrop");

  function openPagefind() {
    const searchDiv = document.getElementById("search");
    const search = searchDiv.querySelector("input");
    setTimeout(() => {
      search.focus();
    }, 0);
    backdrop?.classList.remove("invisible");
    backdrop?.classList.add("visible");
  }

  function closePagefind() {
    const search = document.getElementById("search");
    search.value = "";
    backdrop?.classList.remove("visible");
    backdrop?.classList.add("invisible");
  }

  // open pagefind
  magnifyingGlass?.addEventListener("click", () => {
    openPagefind();
  });

  document.addEventListener("keydown", (e) => {
    if (e.key === "/") {
      e.preventDefault();
      openPagefind();
    } else if ((e.metaKey || e.ctrlKey) && e.key === "k") {
      e.preventDefault();
      openPagefind();
    }
  });

  // close pagefind
  document.addEventListener("keydown", (e) => {
    if (e.key === "Escape" || e.keyCode === 27) {
      closePagefind();
    }
  });

  // close pagefind when searched result(link) clicked
  document.addEventListener("click", (event) => {
    if (event.target.classList.contains("pagefind-ui__result-link")) {
      closePagefind();
    }
  });

  backdrop?.addEventListener("click", (event) => {
    if (!event.target.closest("#pagefind-container")) {
      closePagefind();
    }
  });

  // prevent form submission
  const form = document.getElementById("form");
  form?.addEventListener("submit", (event) => {
    event.preventDefault();
  });
</script>  </body></html>