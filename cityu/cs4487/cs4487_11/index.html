<!DOCTYPE html><html class="bg-background text-foreground" lang="en"> <head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"><meta name="generator" content="Astro v5.8.0"><meta name="robots" content="index, follow"><meta name="HandheldFriendly" content="True"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="format-detection" content="telephone=no,date=no,address=no,email=no,url=no"><meta name="theme-color" content="#121212" media="(prefers-color-scheme: dark)"><meta name="theme-color" content="#121212" media="(prefers-color-scheme: light)"><link rel="sitemap" href="/sitemap-index.xml"><link rel="manifest" href="/site.webmanifest"><link rel="alternate" type="application/rss+xml" title="rezarezvan.com" href="https://rezarezvan.com/rss.xml"><!-- PageFind --><link href="/pagefind/pagefind-ui.css" rel="stylesheet"><script src="/pagefind/pagefind-ui.js"></script><!-- KaTeX support --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><!-- inline KaTeX --><script>
    function renderKaTeX() {
      if (typeof renderMathInElement !== 'undefined') {
        renderMathInElement(document.body, {
          delimiters: [
            { left: '$$', right: '$$', display: true },
            { left: '$', right: '$', display: false },
          ],
        })
      }
    }

    document.addEventListener('DOMContentLoaded', renderKaTeX)
    document.addEventListener('astro:after-swap', renderKaTeX)
  </script><link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><link rel="shortcut icon" href="/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><meta name="apple-mobile-web-app-title" content="rezvan-blog"><link rel="manifest" href="/site.webmanifest"><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><script type="module" src="/_astro/ClientRouter.astro_astro_type_script_index_0_lang.CtSceO8m.js"></script><script>
    function init() {
      setGiscusTheme()
    }

    const setGiscusTheme = () => {
      const giscus = document.querySelector('.giscus-frame')

      const isDark = document.documentElement.classList.contains('dark')

      if (giscus) {
        const url = new URL(giscus.src)
        url.searchParams.set('theme', isDark ? 'dark' : 'light')
        giscus.src = url.toString()
      }
    }

    document.addEventListener('DOMContentLoaded', () => init())
    document.addEventListener('astro:after-swap', () => init())
  </script><title>Part 11 - High-Level and Low-Level Vision Applications | rezarezvan.com</title><meta name="title" content="Part 11 - High-Level and Low-Level Vision Applications | rezarezvan.com"><meta name="description" content="Personal website and course notes repository"><link rel="canonical" href="https://rezarezvan.com"><meta name="robots" content="noindex"><meta property="og:title" content="Part 11 - High-Level and Low-Level Vision Applications"><meta property="og:description" content="Personal website and course notes repository"><meta property="og:image" content="https://rezarezvan.com/static/1200x630.png"><meta property="og:image:alt" content="Part 11 - High-Level and Low-Level Vision Applications"><meta property="og:type" content="website"><meta property="og:locale" content="en"><meta property="og:site_name" content="rezarezvan.com"><meta property="og:url" content="https://rezarezvan.com/cityu/cs4487/cs4487_11/"><meta name="twitter:title" content="Part 11 - High-Level and Low-Level Vision Applications"><meta name="twitter:description" content="Personal website and course notes repository"><meta property="twitter:image" content="https://rezarezvan.com/static/1200x630.png"><meta name="twitter:image:alt" content="Part 11 - High-Level and Low-Level Vision Applications"><meta name="twitter:card" content="summary_large_image"><link rel="stylesheet" href="/_astro/index.B_asM7xH.css"></head><body class="overflow-x-hidden"> <div class="flex h-fit min-h-screen flex-col gap-y-6 font-sans"> <div class="bg-background/50 sticky top-0 z-50 divide-y backdrop-blur-sm xl:divide-none"> <header data-astro-transition-persist="astro-l7r54iwe-1"> <div class="mx-auto flex max-w-3xl items-center justify-between gap-4 px-4 py-3"> <a href="/" target="_self" class="transition-colors duration-300 ease-in-out flex shrink-0 items-center justify-center gap-3">  <span class="hidden h-full text-lg font-medium min-[300px]:block">rezarezvan.com</span>  </a> <div class="flex items-center sm:gap-4"> <nav class="hidden items-center gap-4 text-sm sm:flex sm:gap-6"> <a href="/blog" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> blog<span>/</span>  </a><a href="/chalmers" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> chalmers<span>/</span>  </a><a href="/cityu" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> cityu<span>/</span>  </a><a href="/about" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> about<span>/</span>  </a><a href="/research" target="_self" class="inline-block duration-300 ease-in-out hover:text-foreground/30 transition-colors"> research<span>/</span>  </a> </nav> <button id="magnifying-glass" aria-label="Search" class="flex items-center px-2 text-sm transition-colors duration-300 ease-in-out hover:rounded hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:border-white/20 dark:bg-neutral-900 dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg height="16" stroke-linejoin="round" viewBox="0 0 16 16" width="16" style="color: currentcolor;"> <path fill-rule="evenodd" clip-rule="evenodd" d="M3.5 7C3.5 5.067 5.067 3.5 7 3.5C8.933 3.5 10.5 5.067 10.5 7C10.5 7.88461 10.1718 8.69256 9.63058 9.30876L9.30876 9.63058C8.69256 10.1718 7.88461 10.5 7 10.5C5.067 10.5 3.5 8.933 3.5 7ZM9.96544 11.0261C9.13578 11.6382 8.11014 12 7 12C4.23858 12 2 9.76142 2 7C2 4.23858 4.23858 2 7 2C9.76142 2 12 4.23858 12 7C12 8.11014 11.6382 9.13578 11.0261 9.96544L14.0303 12.9697L14.5607 13.5L13.5 14.5607L12.9697 14.0303L9.96544 11.0261Z" fill="currentColor"></path> </svg>
&nbsp;Search
</button> <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).load=e;window.dispatchEvent(new Event("astro:load"));})();;(()=>{var A=Object.defineProperty;var g=(i,o,a)=>o in i?A(i,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):i[o]=a;var d=(i,o,a)=>g(i,typeof o!="symbol"?o+"":o,a);{let i={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t),11:t=>1/0*t},o=t=>{let[l,e]=t;return l in i?i[l](e):void 0},a=t=>t.map(o),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([l,e])=>[l,o(e)]));class y extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var b;if(!this.hydrator||!this.isConnected)return;let e=(b=this.parentElement)==null?void 0:b.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let c=this.querySelectorAll("astro-slot"),n={},h=this.querySelectorAll("template[data-astro-template]");for(let r of h){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(let r of c){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("name")||"default"]=r.innerHTML)}let p;try{p=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(r){let s=this.getAttribute("component-url")||"<unknown>",v=this.getAttribute("component-export");throw v&&(s+=` (export ${v})`),console.error(`[hydrate] Error parsing props for component ${s}`,this.getAttribute("props"),r),r}let u;await this.hydrator(this)(this.Component,p,n,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),c.disconnect(),this.childrenConnectedCallback()},c=new MutationObserver(()=>{var n;((n=this.lastChild)==null?void 0:n.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});c.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),c=this.getAttribute("client");if(Astro[c]===void 0){window.addEventListener(`astro:${c}`,()=>this.start(),{once:!0});return}try{await Astro[c](async()=>{let n=this.getAttribute("renderer-url"),[h,{default:p}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),u=this.getAttribute("component-export")||"default";if(!u.includes("."))this.Component=h[u];else{this.Component=h;for(let f of u.split("."))this.Component=this.Component[f]}return this.hydrator=p,this.hydrate},e,this)}catch(n){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,n)}}attributeChangedCallback(){this.hydrate()}}d(y,"observedAttributes",["props"]),customElements.get("astro-island")||customElements.define("astro-island",y)}})();</script><astro-island uid="1f72gE" prefix="r18" component-url="/_astro/mobile-menu.B8l2bHjq.js" component-export="default" renderer-url="/_astro/client.Bg5MhsdL.js" props="{&quot;data-astro-transition-persist&quot;:[0,&quot;astro-iq5tym4z-2&quot;]}" ssr client="load" opts="{&quot;name&quot;:&quot;MobileMenu&quot;,&quot;value&quot;:true}" data-astro-transition-persist="astro-iq5tym4z-2" await-children><button data-slot="dropdown-menu-trigger" class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 size-9 md:hidden" title="Menu" type="button" id="radix-:r18R0:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu h-5 w-5"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg><span class="sr-only">Toggle menu</span></button><!--astro:end--></astro-island> <button data-slot="button" class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 size-9" id="theme-toggle" title="Toggle theme"> <svg width="1em" height="1em" class="size-4 scale-100 rotate-0 transition-all dark:scale-0 dark:-rotate-90" data-icon="lucide:sun">   <symbol id="ai:lucide:sun" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><circle cx="12" cy="12" r="4"/><path d="M12 2v2m0 16v2M4.93 4.93l1.41 1.41m11.32 11.32l1.41 1.41M2 12h2m16 0h2M6.34 17.66l-1.41 1.41M19.07 4.93l-1.41 1.41"/></g></symbol><use href="#ai:lucide:sun"></use>  </svg> <svg width="1em" height="1em" class="absolute size-4 scale-0 rotate-90 transition-all dark:scale-100 dark:rotate-0" data-icon="lucide:moon">   <symbol id="ai:lucide:moon" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3a6 6 0 0 0 9 9a9 9 0 1 1-9-9"/></symbol><use href="#ai:lucide:moon"></use>  </svg> <span class="sr-only">Toggle theme</span> </button> <script data-astro-rerun>
  const theme = (() => {
    const localStorageTheme = localStorage?.getItem('theme') ?? ''
    if (['dark', 'light'].includes(localStorageTheme)) {
      return localStorageTheme
    }
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
      return 'dark'
    }
    return 'light'
  })()

  document.documentElement.setAttribute('data-theme', theme)
  document.documentElement.classList.add(
    theme === 'dark' ? 'scheme-dark' : 'scheme-light',
  )
  window.localStorage.setItem('theme', theme)
</script> <script type="module">function a(){const e=document.documentElement,n=e.getAttribute("data-theme")==="dark"?"light":"dark";e.classList.add("[&_*]:transition-none"),e.setAttribute("data-theme",n),e.classList.remove("scheme-dark","scheme-light"),e.classList.add(n==="dark"?"scheme-dark":"scheme-light"),window.getComputedStyle(e).getPropertyValue("opacity"),requestAnimationFrame(()=>{e.classList.remove("[&_*]:transition-none")}),localStorage.setItem("theme",n)}function s(){const e=document.getElementById("theme-toggle");e&&e.addEventListener("click",a)}s();document.addEventListener("astro:after-swap",()=>{const e=localStorage.getItem("theme")||"light",t=document.documentElement;t.classList.add("[&_*]:transition-none"),window.getComputedStyle(t).getPropertyValue("opacity"),t.setAttribute("data-theme",e),t.classList.remove("scheme-dark","scheme-light"),t.classList.add(e==="dark"?"scheme-dark":"scheme-light"),requestAnimationFrame(()=>{t.classList.remove("[&_*]:transition-none")}),s()});</script> </div> </div> </header>  <div id="mobile-toc-container" class="w-full xl:hidden"><details class="group"><summary class="flex w-full cursor-pointer items-center justify-between"><div class="mx-auto flex w-full max-w-3xl items-center px-4 py-3"><div class="relative mr-2 size-4"><svg class="h-4 w-4" viewBox="0 0 24 24"><circle class="text-primary/20" cx="12" cy="12" r="10" fill="none" stroke="currentColor" stroke-width="2"></circle><circle id="mobile-toc-progress-circle" class="text-primary" cx="12" cy="12" r="10" fill="none" stroke="currentColor" stroke-width="2" stroke-dasharray="62.83" stroke-dashoffset="62.83" transform="rotate(-90 12 12)"></circle></svg></div><span id="mobile-toc-current-section" class="text-muted-foreground flex-grow truncate text-sm">
Overview
</span><span class="text-muted-foreground ml-2"><svg width="1em" height="1em" class="h-4 w-4 transition-transform duration-200 group-open:rotate-180" data-icon="lucide:chevron-down">   <symbol id="ai:lucide:chevron-down" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m6 9l6 6l6-6"/></symbol><use href="#ai:lucide:chevron-down"></use>  </svg></span></div></summary><div class="mx-auto max-h-[30vh] max-w-3xl overflow-y-auto"><ul class="flex list-none flex-col gap-y-2 px-4 pb-4" id="mobile-table-of-contents"><li class="text-foreground/60 px-4 text-sm ml-4"><a href="#image-classification" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="image-classification">Image Classification</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#imagenet" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="imagenet">ImageNet</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#performance-of-deep-learning" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="performance-of-deep-learning">Performance of Deep Learning</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#lenet-5-1998" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="lenet-5-1998">LeNet-5 (1998)</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#alexnet" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="alexnet">AlexNet</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#vggnet" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="vggnet">VGGNet</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#inception-module" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="inception-module">Inception Module</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#inceptionnet-v1" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="inceptionnet-v1">InceptionNet (V1)</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#residual-learning" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="residual-learning">Residual Learning</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#residual-network-resnet" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="residual-network-resnet">Residual Network (ResNet)</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#resnext" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="resnext">ResNeXt</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#squueze-and-excitation-networks-senets" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="squueze-and-excitation-networks-senets">Squueze-and-Excitation Networks (SENets)</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#vison-transformers-vits" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="vison-transformers-vits">Vison Transformers (ViTs)</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#contrastive-language-image-pretraining-clip" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="contrastive-language-image-pretraining-clip">Contrastive Language-Image Pretraining (CLIP)</a></li><li class="text-foreground/60 px-4 text-sm ml-4"><a href="#object-detection" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="object-detection">Object Detection</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#ms-coco" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="ms-coco">MS COCO</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#region-based-cnn-r-cnn" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="region-based-cnn-r-cnn">Region-based CNN (R-CNN)</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#fast-r-cnn" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="fast-r-cnn">Fast R-CNN</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#faster-r-cnn" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="faster-r-cnn">Faster R-CNN</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#object-detection-lots-of-variables" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="object-detection-lots-of-variables">Object Detection: Lots of Variables</a></li><li class="text-foreground/60 px-4 text-sm ml-4"><a href="#semantic-segmentation" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="semantic-segmentation">Semantic Segmentation</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#pascal-voc" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="pascal-voc">PASCAL VOC</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#fully-convolutional-networks-fcn" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="fully-convolutional-networks-fcn">Fully Convolutional Networks (FCN)</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#deconv" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="deconv">DeConv</a></li><li class="text-foreground/60 px-4 text-sm ml-4"><a href="#instance-segmentation" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="instance-segmentation">Instance Segmentation</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#mask-r-cnn" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="mask-r-cnn">Mask R-CNN</a></li><li class="text-foreground/60 px-4 text-sm ml-4"><a href="#model-comparison" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="model-comparison">Model Comparison</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#maximum-discrepancy-mad-competition" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="maximum-discrepancy-mad-competition">MAximum Discrepancy (MAD) Competition</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#quantify-the-discrepancy" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="quantify-the-discrepancy">Quantify the Discrepancy</a></li><li class="text-foreground/60 px-4 text-sm ml-4"><a href="#low-level-vision" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="low-level-vision">Low-Level Vision</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#general-formulation" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="general-formulation">General Formulation</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#maximum-a-posteriori-map-estimation-for-image-enhancement" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="maximum-a-posteriori-map-estimation-for-image-enhancement">Maximum a Posteriori (MAP) Estimation for Image Enhancement</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#denoising" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="denoising">Denoising</a></li><li class="text-foreground/60 px-4 text-sm ml-12"><a href="#denoising-by-residual-learning-dncnn" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="denoising-by-residual-learning-dncnn">Denoising by Residual Learning (DnCNN)</a></li><li class="text-foreground/60 px-4 text-sm ml-12"><a href="#bias-free-cnn-for-denoising" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="bias-free-cnn-for-denoising">Bias-Free CNN for Denoising</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#deblurring" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="deblurring">Deblurring</a></li><li class="text-foreground/60 px-4 text-sm ml-12"><a href="#deblurring-by-estimating-blurring-kernel" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="deblurring-by-estimating-blurring-kernel">Deblurring by Estimating Blurring Kernel</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#super-resolution" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="super-resolution">Super-Resolution</a></li><li class="text-foreground/60 px-4 text-sm ml-12"><a href="#super-resolution-by-srcnn" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="super-resolution-by-srcnn">Super-Resolution by SRCNN</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#compression" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="compression">Compression</a></li><li class="text-foreground/60 px-4 text-sm ml-12"><a href="#end-to-end-optimized-image-compression" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="end-to-end-optimized-image-compression">End-to-end Optimized Image Compression</a></li><li class="text-foreground/60 px-4 text-sm ml-12"><a href="#quantizer" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="quantizer">Quantizer</a></li><li class="text-foreground/60 px-4 text-sm ml-12"><a href="#network-architecture" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="network-architecture">Network Architecture</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#colorization" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="colorization">Colorization</a></li><li class="text-foreground/60 px-4 text-sm ml-12"><a href="#image-colorization-by-cnns" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="image-colorization-by-cnns">Image Colorization by CNNs</a></li><li class="text-foreground/60 px-4 text-sm ml-4"><a href="#multi-exposure-image-fusion-mef" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="multi-exposure-image-fusion-mef">Multi-Exposure Image Fusion (MEF)</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#multi-exposure-image-fusion-by-mef-net" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="multi-exposure-image-fusion-by-mef-net">Multi-Exposure Image Fusion by MEF-Net</a></li><li class="text-foreground/60 px-4 text-sm ml-4"><a href="#loss-functions" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="loss-functions">Loss Functions</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#mean-squared-error-mse" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="mean-squared-error-mse">Mean Squared Error (MSE)</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#why-do-we-love-mse" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="why-do-we-love-mse">Why Do We Love MSE?</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#what-is-wrong-with-mse" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="what-is-wrong-with-mse">What is Wrong with MSE?</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#structural-similarity-ssim" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="structural-similarity-ssim">Structural Similarity (SSIM)</a></li><li class="text-foreground/60 px-4 text-sm ml-12"><a href="#what-is-wrong-with-ssim" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="what-is-wrong-with-ssim">What is Wrong with SSIM?</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#learned-perceptual-image-patch-similarity-lpips" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="learned-perceptual-image-patch-similarity-lpips">Learned Perceptual Image Patch Similarity (LPIPS)</a></li><li class="text-foreground/60 px-4 text-sm ml-8"><a href="#deep-image-structure-and-texture-similarity-dists" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="deep-image-structure-and-texture-similarity-dists">Deep Image Structure and Texture Similarity (DISTS)</a></li><li class="text-foreground/60 px-4 text-sm ml-4"><a href="#summary" class="toc-item underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-id="summary">Summary</a></li></ul></div></details></div><script type="module">const p="Overview";const g=2*Math.PI*10;let c=null,a=null,m=null,f=null,r=[],u=[],l=[];function b(){c=document.getElementById("mobile-toc-progress-circle"),a=document.getElementById("mobile-toc-current-section"),m=document.querySelector("#mobile-toc-container details"),f=document.getElementById("mobile-table-of-contents"),c&&(c.style.strokeDasharray=g.toString(),c.style.strokeDashoffset=g.toString())}function w(){if(r=Array.from(document.querySelectorAll(".prose h2, .prose h3, .prose h4, .prose h5, .prose h6")),r.length===0){u=[];return}u=r.map((e,n)=>{const i=r[n+1];return{id:e.id,start:e.offsetTop,end:i?i.offsetTop:document.body.scrollHeight}})}function E(){if(r.length===0)return[];const e=window.scrollY+138,n=window.scrollY+window.innerHeight,i=[];return r.forEach(t=>{const o=t.offsetTop,s=o+t.offsetHeight;(o>=e&&o<=n||s>=e&&s<=n||o<=e&&s>=n)&&i.push(t.id)}),u.forEach(t=>{if(t.start<=n&&t.end>=e){const o=document.getElementById(t.id);if(o){const s=o.offsetTop+o.offsetHeight;t.end>s&&(s<n&&t.end>e||e>s&&e<t.end)&&i.push(t.id)}}}),[...new Set(i)]}function h(e){if(!f||!a)return;f.querySelectorAll(".toc-item").forEach(t=>{const o=t,s=o.dataset.headingId;s&&e.includes(s)?o.classList.add("text-foreground"):o.classList.remove("text-foreground")});let n=p;const i=[];if(e.length>0)for(const t of r)e.includes(t.id)&&t.textContent&&i.push(t.textContent.trim());i.length>0&&(n=i.join(", ")),a.textContent=n}function d(){if(!c)return;const e=document.documentElement.scrollHeight-window.innerHeight,n=e>0?Math.min(Math.max(window.scrollY/e,0),1):0;c.style.strokeDashoffset=(g*(1-n)).toString()}function S(){const e=E();JSON.stringify(e)!==JSON.stringify(l)&&(l=e,h(l)),d()}function y(){f&&f.querySelectorAll(".toc-item").forEach(e=>{e.addEventListener("click",()=>{m&&(m.open=!1)})})}function v(){if(b(),w(),!!a){if(r.length===0){a.textContent=p,window.addEventListener("scroll",d,{passive:!0}),d();return}l=E(),h(l),d(),y(),window.addEventListener("scroll",S,{passive:!0}),window.addEventListener("resize",T,{passive:!0})}}function T(){w();const e=E();JSON.stringify(e)!==JSON.stringify(l)&&(l=e,h(l)),d()}function I(){window.removeEventListener("scroll",S),window.removeEventListener("scroll",d),window.removeEventListener("resize",T),l=[],r=[],u=[]}document.addEventListener("astro:page-load",v);document.addEventListener("astro:after-swap",()=>{I(),v()});document.addEventListener("astro:before-swap",I);</script> </div> <main class="grow"> <div class="mx-auto flex max-w-3xl grow flex-col gap-y-6 px-4">   <section class="relative left-[calc(50%-50vw)] grid w-screen grid-cols-[minmax(0px,1fr)_min(calc(var(--breakpoint-md)-2rem),100%)_minmax(0px,1fr)] gap-y-6 px-4"> <div class="col-start-2"> <nav aria-label="breadcrumb" data-slot="breadcrumb"> <ol data-slot="breadcrumb-list" class="text-muted-foreground flex flex-wrap items-center gap-1.5 text-sm break-words sm:gap-2.5"> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"> <a data-slot="breadcrumb-link" class="hover:text-foreground transition-colors" href="/"> <svg width="1em" height="1em" class="size-4 shrink-0" data-icon="lucide:home">   <symbol id="ai:lucide:home" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 21v-8a1 1 0 0 0-1-1h-4a1 1 0 0 0-1 1v8"/><path d="M3 10a2 2 0 0 1 .709-1.528l7-5.999a2 2 0 0 1 2.582 0l7 5.999A2 2 0 0 1 21 10v9a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"/></g></symbol><use href="#ai:lucide:home"></use>  </svg> </a> </li>  <li data-slot="breadcrumb-separator" role="presentation" aria-hidden="true" class="[&amp;&gt;svg]:size-3.5"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></li> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"><a data-slot="breadcrumb-link" class="hover:text-foreground transition-colors" href="/cityu"> <span class="flex items-center gap-x-2"> <svg width="1em" height="1em" class="size-4" data-icon="lucide:graduation-cap">   <symbol id="ai:lucide:graduation-cap" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M21.42 10.922a1 1 0 0 0-.019-1.838L12.83 5.18a2 2 0 0 0-1.66 0L2.6 9.08a1 1 0 0 0 0 1.832l8.57 3.908a2 2 0 0 0 1.66 0zM22 10v6"/><path d="M6 12.5V16a6 3 0 0 0 12 0v-3.5"/></g></symbol><use href="#ai:lucide:graduation-cap"></use>  </svg> CityU </span> </a></li>  <li data-slot="breadcrumb-separator" role="presentation" aria-hidden="true" class="[&amp;&gt;svg]:size-3.5"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></li> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"><a data-slot="breadcrumb-link" class="hover:text-foreground transition-colors" href="/cityu/cs4487"> <span class="flex items-center gap-x-2"> <svg width="1em" height="1em" class="size-4" data-icon="lucide:book-open">   <symbol id="ai:lucide:book-open" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 7v14m-9-3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4a4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3a3 3 0 0 0-3-3z"/></symbol><use href="#ai:lucide:book-open"></use>  </svg> Machine Learning </span> </a></li>  <li data-slot="breadcrumb-separator" role="presentation" aria-hidden="true" class="[&amp;&gt;svg]:size-3.5"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></li> <li data-slot="breadcrumb-item" class="inline-flex items-center gap-1.5"><span data-slot="breadcrumb-page" role="link" aria-disabled="true" aria-current="page" class="text-foreground font-normal"> <span class="flex items-center gap-x-2"> <svg width="1em" height="1em" class="size-4 shrink-0" data-icon="lucide:file-text">   <symbol id="ai:lucide:file-text" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"/><path d="M14 2v4a2 2 0 0 0 2 2h4M10 9H8m8 4H8m8 4H8"/></g></symbol><use href="#ai:lucide:file-text"></use>  </svg> <span>Part 11 - High-Level and Low-Level Vision Applications</span> </span> </span></li> </ol> </nav> </div>  <section class="col-start-2 flex flex-col gap-y-6 text-center"> <div class="flex flex-col"> <h1 class="mb-2 scroll-mt-31 text-4xl leading-tight font-medium text-pretty" id="post-title"> Part 11 - High-Level and Low-Level Vision Applications </h1> <div class="text-muted-foreground mb-4 flex flex-wrap items-center justify-center gap-2 text-sm"> <div class="flex items-center gap-2"> <span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground">CS4487</span> <div data-orientation="vertical" role="none" data-slot="separator-root" class="bg-border red shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px h-4!"></div> <span>November 20, 2024</span> <div data-orientation="vertical" role="none" data-slot="separator-root" class="bg-border red shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px h-4!"></div>  <div data-orientation="vertical" role="none" data-slot="separator-root" class="bg-border red shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px h-4!"></div> <span>16 min read</span> </div> </div> </div> <nav class="col-start-2 grid grid-cols-1 gap-4 sm:grid-cols-2"> <a href="/cityu/cs4487/cs4487_10#post-title" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-xl group flex items-center justify-start size-full" aria-disabled="false">  <svg width="1em" height="1em" class="mr-2 size-4 transition-transform group-hover:-translate-x-1" data-icon="lucide:arrow-left">   <symbol id="ai:lucide:arrow-left" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m12 19l-7-7l7-7m7 7H5"/></symbol><use href="#ai:lucide:arrow-left"></use>  </svg> <div class="flex flex-col items-start overflow-hidden text-wrap"> <span class="text-muted-foreground text-left text-xs"> Previous Post </span> <span class="w-full text-left text-sm text-balance text-ellipsis"> Part 10 - Neural Networks and Deep Learning Part 2 </span> </div>  </a>  <a href="#" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-xl group flex items-center justify-end size-full pointer-events-none opacity-50 cursor-not-allowed" aria-disabled="true">  <div class="flex flex-col items-end overflow-hidden text-wrap"> <span class="text-muted-foreground text-right text-xs"> Next Post </span> <span class="w-full text-right text-sm text-balance text-ellipsis"> You&#39;re at the newest post! </span> </div> <svg width="1em" height="1em" class="ml-2 size-4 transition-transform group-hover:translate-x-1" data-icon="lucide:arrow-right">   <symbol id="ai:lucide:arrow-right" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 12h14m-7-7l7 7l-7 7"/></symbol><use href="#ai:lucide:arrow-right"></use>  </svg>  </a> </nav> </section> <div class="sticky top-20 col-start-1 row-span-1 mr-8 ml-auto hidden h-[calc(100vh-5rem)] max-w-fit xl:block"> <astro-island uid="Z1NvhTb" prefix="r23" component-url="/_astro/scroll-area.D9cGnicD.js" component-export="ScrollArea" renderer-url="/_astro/client.Bg5MhsdL.js" props="{&quot;className&quot;:[0,&quot;flex max-h-[calc(100vh-8rem)] flex-col overflow-y-auto&quot;],&quot;type&quot;:[0,&quot;always&quot;]}" ssr client="load" opts="{&quot;name&quot;:&quot;ScrollArea&quot;,&quot;value&quot;:true}" await-children><div dir="ltr" data-slot="scroll-area" class="relative flex max-h-[calc(100vh-8rem)] flex-col overflow-y-auto" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px"><style>[data-radix-scroll-area-viewport]{scrollbar-width:none;-ms-overflow-style:none;-webkit-overflow-scrolling:touch;}[data-radix-scroll-area-viewport]::-webkit-scrollbar{display:none}</style><div data-radix-scroll-area-viewport="" data-slot="scroll-area-viewport" class="ring-ring/10 dark:ring-ring/20 dark:outline-ring/40 outline-ring/50 size-full rounded-[inherit] transition-[color,box-shadow] focus-visible:ring-4 focus-visible:outline-1" style="overflow-x:hidden;overflow-y:hidden"><div style="min-width:100%;display:table"><astro-slot> <ul class="mr-8 flex list-none flex-col gap-y-2 px-4" id="table-of-contents"> <li class="text-lg font-medium">Table of Contents</li> <li class="text-foreground/60 text-sm ml-4"> <a href="#image-classification" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="image-classification"> Image Classification </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#imagenet" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="imagenet"> ImageNet </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#performance-of-deep-learning" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="performance-of-deep-learning"> Performance of Deep Learning </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#lenet-5-1998" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="lenet-5-1998"> LeNet-5 (1998) </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#alexnet" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="alexnet"> AlexNet </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#vggnet" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="vggnet"> VGGNet </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#inception-module" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="inception-module"> Inception Module </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#inceptionnet-v1" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="inceptionnet-v1"> InceptionNet (V1) </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#residual-learning" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="residual-learning"> Residual Learning </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#residual-network-resnet" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="residual-network-resnet"> Residual Network (ResNet) </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#resnext" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="resnext"> ResNeXt </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#squueze-and-excitation-networks-senets" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="squueze-and-excitation-networks-senets"> Squueze-and-Excitation Networks (SENets) </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#vison-transformers-vits" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="vison-transformers-vits"> Vison Transformers (ViTs) </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#contrastive-language-image-pretraining-clip" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="contrastive-language-image-pretraining-clip"> Contrastive Language-Image Pretraining (CLIP) </a> </li><li class="text-foreground/60 text-sm ml-4"> <a href="#object-detection" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="object-detection"> Object Detection </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#ms-coco" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="ms-coco"> MS COCO </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#region-based-cnn-r-cnn" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="region-based-cnn-r-cnn"> Region-based CNN (R-CNN) </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#fast-r-cnn" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="fast-r-cnn"> Fast R-CNN </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#faster-r-cnn" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="faster-r-cnn"> Faster R-CNN </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#object-detection-lots-of-variables" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="object-detection-lots-of-variables"> Object Detection: Lots of Variables </a> </li><li class="text-foreground/60 text-sm ml-4"> <a href="#semantic-segmentation" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="semantic-segmentation"> Semantic Segmentation </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#pascal-voc" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="pascal-voc"> PASCAL VOC </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#fully-convolutional-networks-fcn" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="fully-convolutional-networks-fcn"> Fully Convolutional Networks (FCN) </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#deconv" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="deconv"> DeConv </a> </li><li class="text-foreground/60 text-sm ml-4"> <a href="#instance-segmentation" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="instance-segmentation"> Instance Segmentation </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#mask-r-cnn" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="mask-r-cnn"> Mask R-CNN </a> </li><li class="text-foreground/60 text-sm ml-4"> <a href="#model-comparison" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="model-comparison"> Model Comparison </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#maximum-discrepancy-mad-competition" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="maximum-discrepancy-mad-competition"> MAximum Discrepancy (MAD) Competition </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#quantify-the-discrepancy" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="quantify-the-discrepancy"> Quantify the Discrepancy </a> </li><li class="text-foreground/60 text-sm ml-4"> <a href="#low-level-vision" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="low-level-vision"> Low-Level Vision </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#general-formulation" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="general-formulation"> General Formulation </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#maximum-a-posteriori-map-estimation-for-image-enhancement" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="maximum-a-posteriori-map-estimation-for-image-enhancement"> Maximum a Posteriori (MAP) Estimation for Image Enhancement </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#denoising" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="denoising"> Denoising </a> </li><li class="text-foreground/60 text-sm ml-12"> <a href="#denoising-by-residual-learning-dncnn" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="denoising-by-residual-learning-dncnn"> Denoising by Residual Learning (DnCNN) </a> </li><li class="text-foreground/60 text-sm ml-12"> <a href="#bias-free-cnn-for-denoising" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="bias-free-cnn-for-denoising"> Bias-Free CNN for Denoising </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#deblurring" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="deblurring"> Deblurring </a> </li><li class="text-foreground/60 text-sm ml-12"> <a href="#deblurring-by-estimating-blurring-kernel" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="deblurring-by-estimating-blurring-kernel"> Deblurring by Estimating Blurring Kernel </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#super-resolution" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="super-resolution"> Super-Resolution </a> </li><li class="text-foreground/60 text-sm ml-12"> <a href="#super-resolution-by-srcnn" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="super-resolution-by-srcnn"> Super-Resolution by SRCNN </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#compression" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="compression"> Compression </a> </li><li class="text-foreground/60 text-sm ml-12"> <a href="#end-to-end-optimized-image-compression" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="end-to-end-optimized-image-compression"> End-to-end Optimized Image Compression </a> </li><li class="text-foreground/60 text-sm ml-12"> <a href="#quantizer" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="quantizer"> Quantizer </a> </li><li class="text-foreground/60 text-sm ml-12"> <a href="#network-architecture" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="network-architecture"> Network Architecture </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#colorization" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="colorization"> Colorization </a> </li><li class="text-foreground/60 text-sm ml-12"> <a href="#image-colorization-by-cnns" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="image-colorization-by-cnns"> Image Colorization by CNNs </a> </li><li class="text-foreground/60 text-sm ml-4"> <a href="#multi-exposure-image-fusion-mef" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="multi-exposure-image-fusion-mef"> Multi-Exposure Image Fusion (MEF) </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#multi-exposure-image-fusion-by-mef-net" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="multi-exposure-image-fusion-by-mef-net"> Multi-Exposure Image Fusion by MEF-Net </a> </li><li class="text-foreground/60 text-sm ml-4"> <a href="#loss-functions" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="loss-functions"> Loss Functions </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#mean-squared-error-mse" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="mean-squared-error-mse"> Mean Squared Error (MSE) </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#why-do-we-love-mse" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="why-do-we-love-mse"> Why Do We Love MSE? </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#what-is-wrong-with-mse" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="what-is-wrong-with-mse"> What is Wrong with MSE? </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#structural-similarity-ssim" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="structural-similarity-ssim"> Structural Similarity (SSIM) </a> </li><li class="text-foreground/60 text-sm ml-12"> <a href="#what-is-wrong-with-ssim" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="what-is-wrong-with-ssim"> What is Wrong with SSIM? </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#learned-perceptual-image-patch-similarity-lpips" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="learned-perceptual-image-patch-similarity-lpips"> Learned Perceptual Image Patch Similarity (LPIPS) </a> </li><li class="text-foreground/60 text-sm ml-8"> <a href="#deep-image-structure-and-texture-similarity-dists" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="deep-image-structure-and-texture-similarity-dists"> Deep Image Structure and Texture Similarity (DISTS) </a> </li><li class="text-foreground/60 text-sm ml-4"> <a href="#summary" class="marker:text-foreground/30 list-none underline decoration-transparent underline-offset-[3px] transition-colors duration-200 hover:decoration-inherit" data-heading-link="summary"> Summary </a> </li> </ul> </astro-slot></div></div><div data-orientation="vertical" data-slot="scroll-area-scrollbar" class="flex touch-none p-px transition-colors select-none h-full w-2.5 border-l border-l-transparent" style="position:absolute;top:0;right:0;bottom:var(--radix-scroll-area-corner-height);--radix-scroll-area-thumb-height:18px"></div></div><!--astro:end--></astro-island> </div> <script type="module">let l,r=[],s=[],a=[];function v(){l=document.querySelectorAll("[data-heading-link]"),r=[]}function p(){if(s=Array.from(document.querySelectorAll(".prose h2, .prose h3, .prose h4, .prose h5, .prose h6")),s.length===0){a=[];return}a=s.map((e,t)=>{const o=s[t+1];return{id:e.id,start:e.offsetTop,end:o?o.offsetTop:document.body.scrollHeight}})}function f(e){l.forEach(t=>{t.classList.remove("text-foreground")}),e.forEach(t=>{if(t){const o=document.querySelector(`[data-heading-link="${t}"]`);o&&o.classList.add("text-foreground")}})}function u(){if(s.length===0)return[];const e=window.scrollY+80,t=window.scrollY+window.innerHeight,o=[];return s.forEach(n=>{const i=n.offsetTop,d=i+n.offsetHeight;(i>=e&&i<=t||d>=e&&d<=t||i<=e&&d>=t)&&o.push(n.id)}),a.forEach(n=>{if(n.start<=t&&n.end>=e){const i=document.getElementById(n.id);if(i){const d=i.offsetTop+i.offsetHeight;n.end>d&&(d<t&&n.end>e||e>d&&e<n.end)&&o.push(n.id)}}}),[...new Set(o)]}function c(){const e=u();JSON.stringify(e)!==JSON.stringify(r)&&(r=e,f(r))}function g(){p();const e=u();JSON.stringify(e)!==JSON.stringify(r)&&(r=e,f(r))}function h(){if(v(),p(),s.length===0){f([]);return}c(),window.addEventListener("scroll",c,{passive:!0}),window.addEventListener("resize",g,{passive:!0})}function w(){window.removeEventListener("scroll",c),window.removeEventListener("resize",g),r=[],s=[],a=[]}document.addEventListener("astro:page-load",h);document.addEventListener("astro:after-swap",()=>{w(),h()});document.addEventListener("astro:before-swap",w);</script> <article class="prose col-start-2 max-w-none"> <!doctype html><html lang="en"><head></head><body>


<meta charset="utf-8">
<title>CS4487_11</title>
<meta content="width=device-width, initial-scale=1" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" rel="stylesheet">

<svg xmlns="http://www.w3.org/2000/svg" style="display:none"><defs>
        <symbol id="info" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path>
        </symbol>
        <symbol id="lightbulb" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M9.663 17h4.673M12 3v1m6.364 1.636l-.707.707M21 12h-1M4 12H3m3.343-5.657l-.707-.707m2.828 9.9a5 5 0 117.072 0l-.548.547A3.374 3.374 0 0014 18.469V19a2 2 0 11-4 0v-.531c0-.895-.356-1.754-.988-2.386l-.548-.547z"></path>
        </symbol>
        <symbol id="alert-triangle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-3L13.732 4c-.77-1.333-2.694-1.333-3.464 0L3.34 16c-.77 1.333.192 3 1.732 3z"></path>
        </symbol>
        <symbol id="shield-alert" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M20.618 5.984A11.955 11.955 0 0112 2.944a11.955 11.955 0 01-8.618 3.04A12.02 12.02 0 003 9c0 5.591 3.824 10.29 9 11.622 5.176-1.332 9-6.03 9-11.622 0-1.042-.133-2.052-.382-3.016zM12 9v2m0 4h.01"></path>
        </symbol>
        <symbol id="message-square-warning" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M21 15a2 2 0 01-2 2H7l-4 4V5a2 2 0 012-2h14a2 2 0 012 2zM12 8v4m0 4h.01"></path>
        </symbol>
        <symbol id="book-open" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.247 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path>
        </symbol>
        <symbol id="anchor" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M8 12h.01M12 12h.01M16 12h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path>
        </symbol>
        <symbol id="pen-tool" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 19l7-7 3 3-7 7-3-3z"></path>
        </symbol>
        <symbol id="check-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path>
        </symbol>
        <symbol id="puzzle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M11 4a2 2 0 114 0v1a1 1 0 001 1h3a1 1 0 011 1v3a1 1 0 01-1 1h-1a2 2 0 100 4h1a1 1 0 011 1v3a1 1 0 01-1 1h-3a1 1 0 01-1-1v-1a2 2 0 10-4 0v1a1 1 0 01-1 1H7a1 1 0 01-1-1v-3a1 1 0 00-1-1H4a2 2 0 110-4h1a1 1 0 001-1V7a1 1 0 011-1h3a1 1 0 001-1V4z"></path>
        </symbol>
        <symbol id="git-branch" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M3 3v12h12M8 8l8 8"></path>
        </symbol>
        <symbol id="file-text" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path>
        </symbol>
        <symbol id="help-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M18.364 5.636l-3.536 3.536m0 5.656l3.536 3.536M9.172 9.172L5.636 5.636m3.536 9.192l-3.536 3.536M21 12a9 9 0 11-18 0 9 9 0 0118 0zm-5 0a4 4 0 11-8 0 4 4 0 018 0z"></path>
        </symbol>
        <symbol id="check-square" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M9 5H7a2 2 0 00-2 2v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2M9 5a2 2 0 002 2h2a2 2 0 002-2M9 5a2 2 0 012-2h2a2 2 0 012 2m-6 9l2 2 4-4"></path>
        </symbol>
        <symbol id="message-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863 0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z"></path>
        </symbol>
        <symbol id="rotate-ccw" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M3 2v6h6M3 8a9 9 0 015.168 1.637c.773.516 2.21 1.756 3.005 2.318.942.665 2.253.788 3.295.16.9-.545 1.553-1.723 2.592-2.605A9 9 0 018.709 21.5L7.5 20.295"></path>
        </symbol>
        <symbol id="code" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4"></path>
        </symbol>
        <symbol id="dumbbell" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M6.5 6.5h11m-11 11h11M5 20a2 2 0 100-4 2 2 0 000 4zM19 20a2 2 0 100-4 2 2 0 000 4zM5 10a2 2 0 100-4 2 2 0 000 4zM19 10a2 2 0 100-4 2 2 0 000 4z"></path>
        </symbol>
        <symbol id="alert-circle" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 8v4m0 4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path>
        </symbol>
        <symbol id="check" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M5 13l4 4L19 7"></path>
        </symbol>
        <symbol id="check-circle-2" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path>
        </symbol>
        <symbol id="list" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M9 5H7a2 2 0 00-2 2v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2M9 5a2 2 0 002 2h2a2 2 0 002-2M9 5a2 2 0 012-2h2a2 2 0 012 2m-3 7h3m-3 4h3m-6-4h.01M9 16h.01"></path>
        </symbol>
        <symbol id="chevron-down" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <path d="M19 9l-7 7-7-7"></path>
        </symbol></defs></svg>
<h3 id="image-classification">Image Classification</h3>
<p>Goal: Given a photographic image, predict the object class (a.k.a., object recognition).
Typically, we only have one main object present, but we can have more (multi-label classification).</p>
<p>If enough classes are considered, then it is a generic high-level vision task.</p>
<figure><img alt="Image Classification" width="888" height="502" loading="lazy" decoding="async" src="/_astro/IC.C2iCy4l5_Z2wJi9J.webp" ><figcaption>Image Classification</figcaption></figure>
<h4 id="imagenet">ImageNet</h4>
<p>ImageNet is a large-scale dataset for image classification. It has 1.2 million images and 1000 classes. It is used for the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).</p>
<figure><img alt="ImageNet" width="1150" height="458" loading="lazy" decoding="async" src="/_astro/IN.BzydZEzC_ZdTCgA.webp" ><figcaption>ImageNet</figcaption></figure>
<h4 id="performance-of-deep-learning">Performance of Deep Learning</h4>
<p>The introduction of ILSVRC coincided with the emergence of deep learning.</p>
<p>The top-5 error rates decreased as deeper neural networks were developed (note, not just deeper, but the architecture designs were smarter).</p>
<figure><img alt="History of ILSVRC" width="1150" height="516" loading="lazy" decoding="async" src="/_astro/ILSVRC.C3MYTJ-d_LCXDN.webp" ><figcaption>History of ILSVRC</figcaption></figure>
<h4 id="lenet-5-1998">LeNet-5 (1998)</h4>
<p>LeNet-5 was one of the first convolutional neural networks (CNNs) for image classification. It was developed by Yann LeCun.</p>
<p>It has a total of 7 layers, it includes convolutions &#x26; pooling and a final fully-connected layer.
The architecture was designed for hand-written digit recongition (MNIST).</p>
<figure><img alt="LeNet-5 Architecture" width="1192" height="392" loading="lazy" decoding="async" src="/_astro/LN.Bsuu_9Tu_Z1vzKLI.webp" ><figcaption>LeNet-5 Architecture</figcaption></figure>
<h4 id="alexnet">AlexNet</h4>
<p>AlexNet was the first deep CNN to win the ILSVRC in 2012. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.</p>
<p>It has a similiar architecture to LeNet, but deeper (14 layers). It has $11 \times 11$, $5 \times 5$, and $3 \times 3$ convolutions.</p>
<p>But the architecture is not the only new thing, they used some clever new tricks as well.
For example, ReLU is used, they use so called local response normalization, dropout is utilized, as well as max pooling, some data augmentation techniques and SGD with momentum.</p>
<p>Many consider AlexNet as the start of the deep learning revolution.
Partly because of the deep(er) architecture and the tricks used along with it, but also since it was one of the first networks to be trained on GPUs.</p>
<figure><img alt="AlexNet Architecture" width="1192" height="392" loading="lazy" decoding="async" src="/_astro/AN.BXUnV40Z_Z2sIz8j.webp" ><figcaption>AlexNet Architecture</figcaption></figure>
<p>We can see that in Figure 5, it is split in two pipelines because it was trained on 2 GPUs simultaneously.</p>
<h4 id="vggnet">VGGNet</h4>
<p>VGGNet was the runner-up in ILSVRC 2014. It was developed by the Visual Geometry Group at the University of Oxford.</p>
<p>It has the same style as LeNet and AlexNet, but made some other design choices.</p>
<p>The VGGNet only uses $3 \times 3$ convolutional filters (since it is fewer parameters).</p>
<p>They stack several convolutional layers on top of each other before pooling.</p>
<p>But the main difference is that the number of feature channels doubles after each stage.
This ensures they can capture more higher-level features, VGG features are very effective in modeling aspects of human perception because of this.</p>
<figure><img alt="VGGNet Architecture" width="608" height="364" loading="lazy" decoding="async" src="/_astro/VGG.wdnw2n6V_Zc8xuH.webp" ><figcaption>VGGNet Architecture</figcaption></figure>
<h4 id="inception-module">Inception Module</h4>
<p>The Inception Module was introduced in the GoogLeNet architecture, which won ILSVRC 2014.</p>
<p>It is called a Network-in-Network architecture, because it has several convolutional layers in parallel.
The idea is that they extract features at different scales ($1 \times 1$, $3 \times 3$, $5 \times 5$).</p>
<p>Then they pool these features with a $3 \times 3$ max pooling layer.
Features are also concatenated and passed to the next block (which we will become more important later).</p>
<h4 id="inceptionnet-v1">InceptionNet (V1)</h4>
<p>The InceptionNet (V1) architecture is a combination of several Inception Modules.</p>
<p>It has 9 inception modules, 22 layers and 50 convolutional blocks.</p>
<p>The InceptionNet were made for auxiliary classification tasks.
It uses features in the middle of the network to perform classification.</p>
<h4 id="residual-learning">Residual Learning</h4>
<p>The network is learning a function (image to class).
We can think of this as we are building the function block-by-block.</p>
<p>We can allow blocks to learn a <em>residual</em>, which is added to the previous block.
This allows us to keep all the previous information and make small changes with the residual.</p>
<p>A novel intuition here is that this behaves like ensembles of relatively shallow networks.</p>
<h4 id="residual-network-resnet">Residual Network (ResNet)</h4>
<p>The ResNet architecture was introduced in 2015 and won ILSVRC 2015.</p>
<p>There are different sizes of ResNet, but the most famous one is the ResNet-50 (50 layers).
ResNet uses $3 \times 3$ filters and have residual connections every two layers.</p>
<h4 id="resnext">ResNeXt</h4>
<p>ResNeXt combines the split-transform-aggregate strategy in the Inception network and the residual learning in ResNet.</p>
<p>The number of paths inside the ResNeXt block is defined as <strong>cardinality</strong>.
All the paths contain the same topology.</p>
<p>Instead of having high depth and width, having high cardinality helps in decreasing validation error.</p>
<h4 id="squueze-and-excitation-networks-senets">Squueze-and-Excitation Networks (SENets)</h4>
<p>A block for CNNs that improves channel interdependencies.
Adds a parameter to each channel of a convolutional block so that the network adaptively adjusts the weighting of each feature map.</p>
<h4 id="vison-transformers-vits">Vison Transformers (ViTs)</h4>
<p>Vison Transformers (ViTs) emerge as a competetive alternative to CNNs that are currently the state-of-the-art (SOTA) in computer vision and widely used for various image recognition tasks.</p>
<p>Three major processing elements in transformer encoder: Layer normalization, multi-head attention, and multi-layer perceptron (MLP).</p>
<figure><img alt="Vision Transformer" width="882" height="460" loading="lazy" decoding="async" src="/_astro/VIT.DoOZhLR3_Z23yTwU.webp" ><figcaption>Vision Transformer</figcaption></figure>
<h4 id="contrastive-language-image-pretraining-clip">Contrastive Language-Image Pretraining (CLIP)</h4>
<p>CLIP is an open-source, multi-modal, zero-shot model.</p>
<p>It uses both a text encoder (transformers) and a image encoder (ViT or ResNet architectures).</p>
<p>These encoders are trained to maximize the similarity of a dataset of 400 million (image, text) pairs.</p>
<h3 id="object-detection">Object Detection</h3>
<p>Goal: Identify and locate objects within an image or video.
Not only does this involve recognizing the object categories within an image, but also accurately detemining their locations in it.</p>
<figure><img alt="Object Detection" width="578" height="532" loading="lazy" decoding="async" src="/_astro/OD.CfUxXPrN_Z2gwNdL.webp" ><figcaption>Object Detection</figcaption></figure>
<p>Typically, represented as bounding boxes, as seen in Figure 8.</p>
<h4 id="ms-coco">MS COCO</h4>
<p>Microsoft Common Objects in Context (MS COCO) is a large-scale dataset for object detection, segmentation, and captioning.</p>
<p>It consists of over a million images, encompassing 80 different object categories.</p>
<h4 id="region-based-cnn-r-cnn">Region-based CNN (R-CNN)</h4>
<p>R-CNN was the first deep learning model for object detection.</p>
<p>It consists of four steps:</p>
<ol>
<li>Region Proposal</li>
<li>Feature Extraction</li>
<li>Classification</li>
<li>Bounding Box Regression</li>
</ol>
<p>But R-CNN has some limitations.
We can get bad candidate region proposals, since we are using a selective search algorithm.</p>
<p>It is also very time-consuming, 2000 forward propagations for each image.</p>
<h4 id="fast-r-cnn">Fast R-CNN</h4>
<p>Fast R-CNN was introduced to address the limitations of R-CNN.</p>
<p>The four steps are instead:</p>
<ol>
<li>Input and Convolutional Feature Extraction</li>
<li>Region Proposal</li>
<li>Region of Interest (RoI) Pooling</li>
<li>Classification and Bounding Box Regression</li>
</ol>
<p>The RoI pooling layer is used to extract a fixed-size feature map from the feature map of the CNN.</p>
<p>However, Fast R-CNN still has some limitations.
Since the RoI are fixed in size, this will reduce accuracy for objects of different scales or aspect ratios.</p>
<p>Also the region proposal is a non-learnable step.</p>
<h4 id="faster-r-cnn">Faster R-CNN</h4>
<p>This is the third iteration of the R-CNN family.</p>
<ol>
<li>Input and Convolutional Feature Extraction</li>
<li>Region Proposal Network</li>
<li>Region of Interest (RoI) Pooling</li>
<li>Classification and Bounding Box Regression</li>
</ol>
<p>The Region Proposal Network (RPN) is a learnable network that predicts the region proposals.</p>
<h4 id="object-detection-lots-of-variables">Object Detection: Lots of Variables</h4>
<p>One-stage, two-stage, and Transformer series are three main frameworks in object detection.</p>
<ul>
<li>One-stage focus on speed and may sacrifice some accuracy.</li>
<li>Two-stage has high accuracy, but the speed is relatively slow.</li>
<li>Transformer series is based on Vision Transformers.</li>
</ul>
<p>Which framework to choose depends on the application and performance requirements.</p>






























<table><thead><tr><th><strong>One-Stage</strong></th><th><strong>Two-Stage</strong></th><th><strong>Transformer Series</strong></th></tr></thead><tbody><tr><td>YOLOv1</td><td>R-CNN</td><td>DETR</td></tr><tr><td>YOLOv2</td><td>Fast R-CNN</td><td>DN-DETR</td></tr><tr><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td></tr><tr><td>YOLOv8</td><td>Faster R-CNN</td><td>Focus-DETR</td></tr></tbody></table>
<h3 id="semantic-segmentation">Semantic Segmentation</h3>
<p>Goal: Assign a semantic class label to each pixel in an image.</p>
<figure><img alt="Semantic Segmentation" width="1150" height="542" loading="lazy" decoding="async" src="/_astro/SS.C-98Lskj_ZtVXIC.webp" ><figcaption>Semantic Segmentation</figcaption></figure>
<h4 id="pascal-voc">PASCAL VOC</h4>
<p>PASCAL visual object classes (VOC) is a dataset for object detection, segmentation, and classification.</p>
<p>It consists of 20 object classes, 9.9k (VOC2007) and 23k (VOC2012) images, and 24.6k (VOC2007) and 54.9k (VOC2012) objects.</p>
<h4 id="fully-convolutional-networks-fcn">Fully Convolutional Networks (FCN)</h4>
<p>Fully Convolutional Networks (FCN) is a framework for image semantic segmentation.
The core idea is:</p>
<ul>
<li>A fully convolutional network without fully connected layers, capable of adapting to inputs of arbitrary sizes.</li>
<li>A skip architecture that combines results from different depth layers while ensuring both robustness and precision.</li>
</ul>
<figure><img alt="Fully Convolutional Network" width="852" height="414" loading="lazy" decoding="async" src="/_astro/FCN.ChNO3OoW_1tJ5N2.webp" ><figcaption>Fully Convolutional Network</figcaption></figure>
<p>However, the limitation of FCNs are that it directly upsamples the compact features.</p>
<h4 id="deconv">DeConv</h4>
<p>DeConv proposes a deconvolution network with unpooling operation to predict the segmentation map.</p>
<p>First, the convolution network downsamples the feature representations.
Then, the deconvolution network upsamples the compact features maps and refine the dense predictions.</p>
<h3 id="instance-segmentation">Instance Segmentation</h3>
<p>Goal: Detect objects in an image and label each pixel at the same time.</p>
<figure><img alt="Instance Segmentation" width="450" height="320" loading="lazy" decoding="async" src="/_astro/IS.DGcd6R2v_1py3D6.webp" ><figcaption>Instance Segmentation</figcaption></figure>
<p>Compared with object detection, it outputs a mask instead of a bounding box.
Compared with semantic segmentation, it distinguishes between different instances in the same class.</p>
<h4 id="mask-r-cnn">Mask R-CNN</h4>
<p>Mask R-CNN is a framework for instance segmentation.</p>
<ol>
<li>Input and Convolutional Feature Extraction</li>
<li>Region Proposal Network (RPN)</li>
<li>Region of Interest (RoI) Align</li>
<li>Classification, Bounding Box Regression, and Mask Prediction</li>
</ol>
<p>The RoI Align layer is used to extract a fixed-size feature map from the feature map of the CNN.</p>
<h3 id="model-comparison">Model Comparison</h3>
<p>The conventional model comparison methodology is,</p>
<ul>
<li>Pre-select a number of images from the space of all possible natural images (i.e., natural images manifold) to form the test set.</li>
<li>Collect the human label for each image in the test set to identify its ground-truth category.</li>
<li>Rank the competing classifiers according to their goodness of fit (e.g., accuracy) on the test set.</li>
</ul>
<p>However, as always we will have limitations.</p>
<ul>
<li>The test sets are small, fixed, and extensively reused.</li>
<li>More fundamentally, the underlying philosophy is to <strong>prove</strong> a classifier to be correct, which is impossible to achieve.</li>
</ul>
<h4 id="maximum-discrepancy-mad-competition">MAximum Discrepancy (MAD) Competition</h4>
<p>MAximum Discrepancy (MAD) attempts to <strong>falsify</strong> two classifiers by maximizing their prediction discrepancy.</p>
<p>A classifier that is harder to be falsified in MAD is considered to be better.</p>
<h4 id="quantify-the-discrepancy">Quantify the Discrepancy</h4>
<p>We can not use the zero-one loss to quantify the discrepancy between two classifiers.</p>
<ul>
<li>The zero-one loss is not continuous and not differentiable.</li>
<li>Lack of sensitivity to the prediction confidence.</li>
<li>Instability and lack of robustness.</li>
</ul>
<h3 id="low-level-vision">Low-Level Vision</h3>
<p>Low-level vision tasks are the fundamental building blocks of high-level vision tasks.
For example, image denoising, image deblurring, image super-resolution, and image inpainting.</p>
<h4 id="general-formulation">General Formulation</h4>
<p>$$
\mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{e}.
$$</p>
<p>where $\mathbf{x}$ is the signal to be recovered.</p>
<p>$\mathbf{e}$ is the measurement noise.</p>
<p>$\mathbf{A}$ is the Sensing Matrix.</p>
<p>$\mathbf{y}$ is the measurements.</p>
<p><em>Fidelity</em> is the closeness between the recovered signal and the original signal, i.e.,</p>
<p>$$
\underset{\mathbf{x}}{\min} \Vert \mathbf{y} - \mathbf{A} \mathbf{x} \Vert_2^2
$$</p>
<p>However, we cant just rely on fidelity, we can obtain the same fidelity with different solutions.
Where one solution is better than the other. Thus, prior knowledge is needed.</p>
<h4 id="maximum-a-posteriori-map-estimation-for-image-enhancement">Maximum a Posteriori (MAP) Estimation for Image Enhancement</h4>
<p>$$
\underset{\mathbf{x}}{\min} \Vert \mathbf{y} - \mathbf{A} \mathbf{x} \Vert_2^2 + \lambda \phi(\mathbf{x})
$$</p>
<p>where $\mathbf{x}$ is the signal to be recovered.</p>
<p>$\mathbf{e}$ is the measurement noise.</p>
<p>$\mathbf{A}$ is the Sensing Matrix.</p>
<p>$\mathbf{y}$ is the measurements.</p>
<p>$\phi(\mathbf{x})$ is the signal regularizer (i.e., image prior).</p>
<h4 id="denoising">Denoising</h4>
<p>Denoising is the process of removing noise from a signal.</p>
<p>Given,</p>
<p>$$
\mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{e}
$$</p>
<p>$\mathbf{A} = \mathbf{I}$ (identity matrix).</p>
<p>Thus, we only have the noisy measurements $\mathbf{y} = \mathbf{x} + \mathbf{e}$.</p>
<p>This is the simplest and most boring low-level vision problem.</p>
<h5 id="denoising-by-residual-learning-dncnn">Denoising by Residual Learning (DnCNN)</h5>
<p>The DnCNN is a deep learning-based denoising method.</p>
<p>It utilizes residual learning to learn the noise from the noisy image.
MSE is the perfect loss in this setting and it is easy to optimize.</p>
<h5 id="bias-free-cnn-for-denoising">Bias-Free CNN for Denoising</h5>
<p>The key technical feature here is that we remove all the bias term from the DnCNN.</p>
<p>This makes it interpretable via linear algebra tools and generalizable to noise levels beyond the training range.</p>
<h4 id="deblurring">Deblurring</h4>
<p>Deblurring is the process of removing blur from a signal.</p>
<p>Given,</p>
<p>$$
\mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{e}
$$</p>
<p>$\mathbf{A}$ acts as a blurring operator (i.e., convolution with a blur kernel).</p>
<h5 id="deblurring-by-estimating-blurring-kernel">Deblurring by Estimating Blurring Kernel</h5>
<p>The key idea here is to estimate the blur kernel from the blurred image.
Here, we can also use the MSE as loss function.</p>
<h4 id="super-resolution">Super-Resolution</h4>
<p>Super-resolution is the process of enhancing the resolution of an image.</p>
<p>Given,</p>
<p>$$
\mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{e} = \mathbf{D} \mathbf{H} \mathbf{x} + \mathbf{e}
$$</p>
<p>$\mathbf{H}$ is a blurring operator and $\mathbf{D}$ is a down-sampling operator.</p>
<h5 id="super-resolution-by-srcnn">Super-Resolution by SRCNN</h5>
<p>SRCNN maps low-resolution features nonlineraly to high-resolution representations.
This ensures we have good conceptual connections to previous methods.</p>
<p>Again, we use the MSE loss here as well.</p>
<h4 id="compression">Compression</h4>
<p>Image compression minimizes its size (in bytes) without degrading the image quality below an acceptable threshold.</p>
<h5 id="end-to-end-optimized-image-compression">End-to-end Optimized Image Compression</h5>
<p>We can jointly optimize a weighted sum of the rate and distortion.</p>
<ul>
<li>Rate ($R$) as the discrete entropy and distortion ($D$) as the MSE.</li>
<li>Loss function: $R + \lambda D$ (where $\lambda$ can be fixed or learnable).</li>
<li>Backpropagate through the non-differentiable quantizer by adding uniform noise.</li>
</ul>
<h5 id="quantizer">Quantizer</h5>
<p>The quantizer is a non-differentiable operation.</p>
<p>$$
\hat{y_i} = \text{round}(y_i) \text{ and } p_{\hat{y_i}}(n) = \int_{n - \frac{1}{2}}^{n + \frac{1}{2}} p_{y_i}(t) dt, \text{ for all } n \in \mathcal{Z}
$$</p>
<p>$$
\tilde{y_i} = y_i + \Delta y_i, \text{ for } \Delta y_i \sim \mathcal{U}(-\frac{1}{2}, \frac{1}{2})
$$</p>
<p>The density function of $\tilde{y_i}$ is a continuous relaxtion of the probability mass function of $\hat{y_i}$, identical at integer values.</p>
<p>Independent uniform noise is frequently used as a model of quantization error in signal processing.</p>
<h5 id="network-architecture">Network Architecture</h5>
<p>Generalized Divisive Normalization (GDN) is used to normalize the input.</p>
<p>Let $\mathbf{x} \in \mathbb{R}^N$ be the input vector to GDN, and the output response $\mathbf{z} \in \mathbb{R}^N$ can be computed by,
$$
z_i = \frac{x_i}{\left(\beta_i + \sum_{j=1}^N \gamma_{ji} x_j^2 \right)^\frac{1}{2}}
$$</p>
<p>where the weight matrix $\gamma \in \mathbb{R}^{N \times N}$ and the bias vector $\beta \in \mathbb{R}^N$ are parameters in GDN to be optimized.</p>
<h4 id="colorization">Colorization</h4>
<p>Grey-level images are obtained by dropping the color representation of color images.</p>
<h5 id="image-colorization-by-cnns">Image Colorization by CNNs</h5>
<p>Predict a probability distribution of discretized ab values.</p>
<ul>
<li>CIE LAB is a more perceptually uniform color space than RGB.</li>
<li>Cast a regression task into a multiclass classification task top cope with the inherent ambiguity and multimodal nature of colorization.</li>
</ul>
<h3 id="multi-exposure-image-fusion-mef">Multi-Exposure Image Fusion (MEF)</h3>
<p>MEF takes an image sequence with different exposure levels as input and produces a high-quality image with richer details.</p>
<p>This is mainly accomplished by a weighted summation framework $\mathbf{y} = \sum_{k=1}^K \mathbf{w_k} \odot \mathbf{x_k}$.</p>
<h4 id="multi-exposure-image-fusion-by-mef-net">Multi-Exposure Image Fusion by MEF-Net</h4>
<p>Predict downsampled weight maps, followed by guided filtering for upsampling.</p>
<p>MSE is not applicable here, MEF structural similarity (MEF-SSIM) index as the objective function instead.
It is however, perceptually optimized.</p>
<h3 id="loss-functions">Loss Functions</h3>
<p>We have seen that the MSE loss is the most common loss function in low-level vision tasks.
However, one can use many.</p>
<ul>
<li>Mean Squared Error (MSE)</li>
<li>Structural Similarity (SSIM)</li>
<li>learned Perceptual Image Patch Similarity (LPIPS)</li>
<li>Deep Image Structure and Texture Similarity (DISTS)</li>
<li>$\vdots$</li>
</ul>
<h4 id="mean-squared-error-mse">Mean Squared Error (MSE)</h4>
<p>$$
\text{MSE} = \frac{1}{N} \sum_{i=1}^N (\mathbf{x_i} - \hat{\mathbf{x_i}})^2
$$</p>
<p>where $\mathbf{x_i}$ is the ground-truth and $\hat{\mathbf{x_i}}$ is the prediction.</p>
<h4 id="why-do-we-love-mse">Why Do We Love MSE?</h4>
<p>It is simple, parameter-free, cheap to compute and also memory-less!</p>
<p>It is per definition a valid <em>distance</em> metric (i.e., satisfying non-negativity, identiy, symmetry, and triangular inequality).</p>
<p>It also has a clear physical meaning as energy.
It is simply an excellent metric in the context of optimization (convex, differentiable, often admits closed-form analytical solutions).</p>
<h4 id="what-is-wrong-with-mse">What is Wrong with MSE?</h4>
<p>In the context of computer vision, MSE does not care about pixel ordering, which in many tasks is important.</p>
<p>It cares about the pixel <em>difference</em>, but not about the underlying signal, which again, is important in computer vision tasks.
The same goes for the sign of the pixel difference.</p>
<p>Further, MSE implicitly assumes that errors are statistically independent.
Which is true <em>if</em> spatial dependencies are eliminated prior to computation.
But no easy task as natural images are highly structured (i.e., spatially correlated).</p>
<p>We can however find a solution for the flaws in MSE, we can learn a perceptual transform, denote this $\mathbf{f}$.</p>
<p>$$
D(\mathbf{x_i}, \hat{\mathbf{x_i}}) = \frac{1}{N} \sum_{i=1}^N (\mathbf{f}(\mathbf{x})_i - \mathbf{f}(\hat{\mathbf{x}})_i)^2
$$</p>
<p>What are some desirable properties of $\mathbf{f}$?</p>
<ul>
<li>It should be differentiable.</li>
<li>It should be invariant to small changes.</li>
<li>It should be sensitive to large changes.</li>
<li>It should be able to capture the perceptual similarity between images.</li>
</ul>
<h4 id="structural-similarity-ssim">Structural Similarity (SSIM)</h4>
<p>The SSIM index is a perceptual metric that quantifies the image quality degradation that is caused by processing such as data compression or by losses in data transmission.</p>
<p>$$
\text{SSIM}(\mathbf{x}, \hat{\mathbf{x}}) = l(\mathbf{x}, \hat{\mathbf{x}}) \cdot s(\mathbf{x}, \hat{\mathbf{x}}) = \left(\frac{2 \mu_{\mathbf{x}} \mu_{\hat{\mathbf{x}}} + c_1}{\mu_{\mathbf{x}}^2 + \mu_{\hat{\mathbf{x}}} + c_1}\right) \cdot \left( \frac{2 \sigma_{\mathbf{x} \hat{\mathbf{x}}} + c_2}{\sigma_{\mathbf{x}}^2 + \sigma_{\hat{\mathbf{x}}}^2 + c_2} \right)
$$</p>
<h5 id="what-is-wrong-with-ssim">What is Wrong with SSIM?</h5>
<p>Normalization is sensitive to low intensities.</p>
<p>It does not consider the chrominance (color) information.</p>
<p>Relies on patch-by-patch comparison, which is not ideal for global image quality assessment.</p>
<h4 id="learned-perceptual-image-patch-similarity-lpips">Learned Perceptual Image Patch Similarity (LPIPS)</h4>
<p>LPIPS is a learned metric that measures perceptual similarity between two images.</p>
<p>$$
\text{LPIPS}(\mathbf{x}, \hat{\mathbf{x}}) = \sum_{i=1}^S \sum_{j=1}^{N_i} w_{ij} \text{ MSE}(\mathbf{f}(\mathbf{x})_j^{(i)}, \mathbf{f}(\hat{\mathbf{x}})_j^{(i)})
$$</p>
<h4 id="deep-image-structure-and-texture-similarity-dists">Deep Image Structure and Texture Similarity (DISTS)</h4>
<p>DISTS is a learned metric that measures perceptual similarity between two images.</p>
<p>$$
\text{DISTS}(\mathbf{x}, \hat{\mathbf{x}}) = 1 - \sum_{i=0}^S \sum_{j=1}^{N_i} \left( \alpha_{ij} l\left( \mathbf{x_j}^{(i)}, \hat{\mathbf{x_j}}^{(i)} \right) + \beta_{ij} s\left( \mathbf{x_j}^{(i)}, \hat{\mathbf{x_j}}^{(i)} \right) \right)
$$</p>
<h3 id="summary">Summary</h3>
<p>Advances of deep learning has been driven by the ImageNet competition.</p>
<p>As depth increases, need to have a smart architecture design to make training more effective.</p>
<p>Deep learning also beigns to dominate low-level vision, e.g., image denoising, deblurring, super-resolution, compression etc.</p>


</body></html> </article> <nav class="col-start-2 grid grid-cols-1 gap-4 sm:grid-cols-2"> <a href="/cityu/cs4487/cs4487_10#post-title" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-xl group flex items-center justify-start size-full" aria-disabled="false">  <svg width="1em" height="1em" viewBox="0 0 24 24" class="mr-2 size-4 transition-transform group-hover:-translate-x-1" data-icon="lucide:arrow-left">   <use href="#ai:lucide:arrow-left"></use>  </svg> <div class="flex flex-col items-start overflow-hidden text-wrap"> <span class="text-muted-foreground text-left text-xs"> Previous Post </span> <span class="w-full text-left text-sm text-balance text-ellipsis"> Part 10 - Neural Networks and Deep Learning Part 2 </span> </div>  </a>  <a href="#" target="_self" class="duration-300 ease-in-out gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&#38;_svg]:pointer-events-none [&#38;_svg:not([class*='size-'])]:size-4 shrink-0 [&#38;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 rounded-xl group flex items-center justify-end size-full pointer-events-none opacity-50 cursor-not-allowed" aria-disabled="true">  <div class="flex flex-col items-end overflow-hidden text-wrap"> <span class="text-muted-foreground text-right text-xs"> Next Post </span> <span class="w-full text-right text-sm text-balance text-ellipsis"> You&#39;re at the newest post! </span> </div> <svg width="1em" height="1em" viewBox="0 0 24 24" class="ml-2 size-4 transition-transform group-hover:translate-x-1" data-icon="lucide:arrow-right">   <use href="#ai:lucide:arrow-right"></use>  </svg>  </a> </nav> <div class="col-start-2"> <section class="mx-auto mt-12"> <script data-astro-rerun src="https://giscus.app/client.js" data-repo="rezaarezvan/rezarezvan.com" data-repo-id="R_kgDOHvQr3w" data-category="General" data-category-id="DIC_kwDOHvQr384CiWVC" data-mapping="og:title" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="en" data-loading="lazy" crossorigin="anonymous" async></script> </section> <script>
  function updateGiscusTheme() {
    const element = document.documentElement
    const theme = element.getAttribute('data-theme')
    const iframe = document.querySelector('iframe.giscus-frame')
    if (!iframe) return
    iframe.contentWindow.postMessage(
      { giscus: { setConfig: { theme } } },
      'https://giscus.app',
    )
  }

  const observer = new MutationObserver(updateGiscusTheme)
  observer.observe(document.documentElement, {
    attributes: true,
    attributeFilter: ['class'],
  })

  window.onload = () => {
    updateGiscusTheme()
  }
</script> </div> </section> <button data-slot="button" class="items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 size-9 group fixed right-8 bottom-8 z-50 hidden" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top"> <svg width="1em" height="1em" class="mx-auto size-4 transition-all group-hover:-translate-y-0.5" data-icon="lucide:arrow-up">   <symbol id="ai:lucide:arrow-up" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 12l7-7l7 7m-7 7V5"/></symbol><use href="#ai:lucide:arrow-up"></use>  </svg> </button> <script type="module">document.addEventListener("astro:page-load",()=>{const o=document.getElementById("scroll-to-top"),t=document.querySelector("footer");o&&t&&(o.addEventListener("click",()=>{window.scrollTo({top:0,behavior:"smooth"})}),window.addEventListener("scroll",()=>{const e=t.getBoundingClientRect().top<=window.innerHeight;o.classList.toggle("hidden",window.scrollY<=300||e)}))});</script>  </div> </main> <footer class="py-4"> <div class="mx-auto flex max-w-3xl flex-col items-center justify-center gap-y-2 px-4 sm:flex-row sm:justify-between"> <div class="flex flex-wrap items-center justify-center gap-x-2 text-center"> <span class="text-muted-foreground text-sm">
&copy; 2025  rezarezvan.com </span> </div> </div> </footer> <div id="backdrop" class="invisible fixed top-0 left-0 z-50 flex h-screen w-full justify-center bg-[rgba(0,0,0,0.5)] p-6 backdrop-blur-sm" data-astro-transition-persist="astro-t6dxx5el-4"> <div id="pagefind-container" class="m-0 flex h-fit max-h-[80%] w-full max-w-screen-sm flex-col overflow-auto rounded border border-black/15 bg-neutral-100 p-2 px-4 py-3 shadow-lg dark:border-white/20 dark:bg-neutral-900"> <div id="search" class="pagefind-ui pagefind-init" data-pagefind-ui data-bundle-path="/pagefind/" data-ui-options="{&#34;showImages&#34;:false,&#34;excerptLength&#34;:15,&#34;resetStyles&#34;:false}"></div> <script type="module" src="/_astro/Search.astro_astro_type_script_index_0_lang.mBpmxV9R.js"></script> <div class="dark:prose-invert mr-2 pt-4 pb-1 text-right text-xs">
Press <span class="prose dark:prose-invert text-xs"><kbd class="">Esc</kbd></span> or click anywhere to close
</div> </div> </div> <script>
  document.addEventListener('DOMContentLoaded', () => {
    const magnifyingGlass = document.getElementById('magnifying-glass')
    const backdrop = document.getElementById('backdrop')

    function openPagefind() {
      const searchDiv = document.getElementById('search')
      const search = searchDiv.querySelector('input')
      setTimeout(() => {
        search.focus()
      }, 0)
      backdrop?.classList.remove('invisible')
      backdrop?.classList.add('visible')
    }

    function closePagefind() {
      const searchDiv = document.getElementById('search')
      const search = searchDiv.querySelector('input')
      if (search) {
        search.value = ''
      }
      backdrop?.classList.remove('visible')
      backdrop?.classList.add('invisible')
    }

    // open pagefind
    magnifyingGlass?.addEventListener('click', () => {
      openPagefind()
    })

    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape') {
        closePagefind()
      }
    })

    // close pagefind when searched result(link) clicked
    document.addEventListener('click', (event) => {
      if (event.target.classList.contains('pagefind-ui__result-link')) {
        closePagefind()
      }
    })

    backdrop?.addEventListener('click', (event) => {
      if (!event.target.closest('#pagefind-container')) {
        closePagefind()
      }
    })

    // prevent form submission
    const form = document.getElementById('form')
    form?.addEventListener('submit', (event) => {
      event.preventDefault()
    })
  })
</script>  </div> </body></html>