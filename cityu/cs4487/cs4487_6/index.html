<!DOCTYPE html><html lang="en"> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/icon" href="/favicon.ico"><meta name="generator" content="Astro v4.11.5"><!-- Canonical URL --><link rel="canonical" href="https://rezvan.xyz/cityu/cs4487/cs4487_6/"><!-- Primary Meta Tags --><title>Part 5 - Robust, Non-Linear Regression and Clustering | machine learning | rezvan.xyz</title><meta name="title" content="Part 5 - Robust, Non-Linear Regression and Clustering | machine learning | rezvan.xyz"><meta name="description"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://rezvan.xyz/cityu/cs4487/cs4487_6/"><meta property="og:title" content="Part 5 - Robust, Non-Linear Regression and Clustering | machine learning | rezvan.xyz"><meta property="og:description"><meta property="og:image" content="https://rezvan.xyz/favicon.ico"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://rezvan.xyz/cityu/cs4487/cs4487_6/"><meta property="twitter:title" content="Part 5 - Robust, Non-Linear Regression and Clustering | machine learning | rezvan.xyz"><meta property="twitter:description"><meta property="twitter:image" content="https://rezvan.xyz/favicon.ico"><!-- PageFind --><link href="/pagefind/pagefind-ui.css" rel="stylesheet"><script src="/pagefind/pagefind-ui.js"></script><!-- KaTeX support --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script><!-- inline KaTeX --><script>
    function renderKaTeX() {
        if (typeof renderMathInElement !== "undefined") {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false },
                ],
            });
        }
    }

    document.addEventListener("DOMContentLoaded", renderKaTeX);
    document.addEventListener("astro:after-swap", renderKaTeX);
</script><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><script>
    function init() {
        preloadTheme();
        onScroll();
        animate();
        updateThemeButtons();
        addCopyCodeButtons();
        setGiscusTheme();

        const backToTop = document.getElementById("back-to-top");
        backToTop?.addEventListener("click", (event) => scrollToTop(event));

        const backToPrev = document.getElementById("back-to-prev");
        backToPrev?.addEventListener("click", () => window.history.back());

        const lightThemeButton = document.getElementById("light-theme-button");
        lightThemeButton?.addEventListener("click", () => {
            localStorage.setItem("theme", "light");
            toggleTheme(false);
            updateThemeButtons();
        });

        const darkThemeButton = document.getElementById("dark-theme-button");
        darkThemeButton?.addEventListener("click", () => {
            localStorage.setItem("theme", "dark");
            toggleTheme(true);
            updateThemeButtons();
        });

        const systemThemeButton = document.getElementById(
            "system-theme-button",
        );
        systemThemeButton?.addEventListener("click", () => {
            localStorage.setItem("theme", "system");
            toggleTheme(
                window.matchMedia("(prefers-color-scheme: dark)").matches,
            );
            updateThemeButtons();
        });

        window
            .matchMedia("(prefers-color-scheme: dark)")
            .addEventListener("change", (event) => {
                if (localStorage.theme === "system") {
                    toggleTheme(event.matches);
                }
            });

        document.addEventListener("scroll", onScroll);
    }

    function updateThemeButtons() {
        const theme = localStorage.getItem("theme");
        const lightThemeButton = document.getElementById("light-theme-button");
        const darkThemeButton = document.getElementById("dark-theme-button");
        const systemThemeButton = document.getElementById(
            "system-theme-button",
        );

        function removeActiveButtonTheme(button) {
            button?.classList.remove("bg-black/5");
            button?.classList.remove("dark:bg-white/5");
        }

        function addActiveButtonTheme(button) {
            button?.classList.add("bg-black/5");
            button?.classList.add("dark:bg-white/5");
        }

        removeActiveButtonTheme(lightThemeButton);
        removeActiveButtonTheme(darkThemeButton);
        removeActiveButtonTheme(systemThemeButton);

        if (theme === "light") {
            addActiveButtonTheme(lightThemeButton);
        } else if (theme === "dark") {
            addActiveButtonTheme(darkThemeButton);
        } else {
            addActiveButtonTheme(systemThemeButton);
        }
    }

    function animate() {
        const animateElements = document.querySelectorAll(".animate");

        animateElements.forEach((element, index) => {
            setTimeout(() => {
                element.classList.add("show");
            }, index * 100);
        });
    }

    function onScroll() {
        if (window.scrollY > 0) {
            document.documentElement.classList.add("scrolled");
        } else {
            document.documentElement.classList.remove("scrolled");
        }
    }

    function scrollToTop(event) {
        event.preventDefault();
        window.scrollTo({
            top: 0,
            behavior: "smooth",
        });
    }

    function toggleTheme(dark) {
        const css = document.createElement("style");

        css.appendChild(
            document.createTextNode(
                `* {
             -webkit-transition: none !important;
             -moz-transition: none !important;
             -o-transition: none !important;
             -ms-transition: none !important;
             transition: none !important;
          }
        `,
            ),
        );

        document.head.appendChild(css);

        if (dark) {
            document.documentElement.classList.add("dark");
        } else {
            document.documentElement.classList.remove("dark");
        }

        window.getComputedStyle(css).opacity;
        document.head.removeChild(css);

        setGiscusTheme();
    }

    function preloadTheme() {
        const userTheme = localStorage.theme;

        if (userTheme === "light" || userTheme === "dark") {
            toggleTheme(userTheme === "dark");
        } else {
            toggleTheme(
                window.matchMedia("(prefers-color-scheme: dark)").matches,
            );
        }
    }

    function addCopyCodeButtons() {
        let copyButtonLabel = "📋";
        let codeBlocks = Array.from(document.querySelectorAll("pre"));

        async function copyCode(codeBlock, copyButton) {
            const codeText = codeBlock.innerText;
            const buttonText = copyButton.innerText;
            const textToCopy = codeText.replace(buttonText, "");

            await navigator.clipboard.writeText(textToCopy);
            copyButton.innerText = "✅";

            setTimeout(() => {
                copyButton.innerText = copyButtonLabel;
            }, 2000);
        }

        for (let codeBlock of codeBlocks) {
            const wrapper = document.createElement("div");
            wrapper.style.position = "relative";

            const copyButton = document.createElement("button");
            copyButton.innerText = copyButtonLabel;
            copyButton.classList = "copy-code";

            codeBlock.setAttribute("tabindex", "0");
            codeBlock.appendChild(copyButton);

            codeBlock.parentNode.insertBefore(wrapper, codeBlock);
            wrapper.appendChild(codeBlock);

            copyButton?.addEventListener("click", async () => {
                await copyCode(codeBlock, copyButton);
            });
        }
    }

    const setGiscusTheme = () => {
        const giscus = document.querySelector(".giscus-frame");

        const isDark = document.documentElement.classList.contains("dark");

        if (giscus) {
            const url = new URL(giscus.src);
            url.searchParams.set("theme", isDark ? "dark" : "light");
            giscus.src = url.toString();
        }
    };

    document.addEventListener("DOMContentLoaded", () => init());
    document.addEventListener("astro:after-swap", () => init());
    preloadTheme();
</script><link rel="stylesheet" href="/_astro/_subject_.DPh3UX5U.css">
<style>summary[data-astro-cid-xvrfupwn]{cursor:pointer;border-top-left-radius:.5rem;border-top-right-radius:.5rem;padding:.375rem .75rem;font-weight:500;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}summary[data-astro-cid-xvrfupwn]:hover{background-color:#0000000d}summary[data-astro-cid-xvrfupwn]:hover:is(.dark *){background-color:#ffffff0d}details[data-astro-cid-xvrfupwn][open] summary[data-astro-cid-xvrfupwn]{background-color:#0000000d}details[data-astro-cid-xvrfupwn][open] summary[data-astro-cid-xvrfupwn]:is(.dark *){background-color:#ffffff0d}
</style><script type="module" src="/_astro/hoisted.DzxSAGjc.js"></script></head> <body> <header data-astro-transition-persist="astro-l7r54iwe-1"> <div class="mx-auto max-w-screen-sm px-3"> <div class="flex flex-wrap justify-between gap-y-2"> <a href="/" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out">  <div class="font-semibold"> rezvan.xyz </div>  </a> <nav class="flex items-center gap-1 text-sm"> <a href="/posts" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> posts </a> <span> / </span> <a href="/chalmers" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> chalmers </a> <span> / </span> <a href="/cityu" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> cityu </a> <span> / </span> <a href="/pdf/cv/cv.pdf" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> cv </a> <span> / </span> <button id="magnifying-glass" aria-label="Search" class="flex items-center rounded border border-black/15 bg-neutral-100 px-2 py-1 text-xs transition-colors duration-300 ease-in-out hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:border-white/20 dark:bg-neutral-900 dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg height="16" stroke-linejoin="round" viewBox="0 0 16 16" width="16" style="color: currentcolor;"><path fill-rule="evenodd" clip-rule="evenodd" d="M3.5 7C3.5 5.067 5.067 3.5 7 3.5C8.933 3.5 10.5 5.067 10.5 7C10.5 7.88461 10.1718 8.69256 9.63058 9.30876L9.30876 9.63058C8.69256 10.1718 7.88461 10.5 7 10.5C5.067 10.5 3.5 8.933 3.5 7ZM9.96544 11.0261C9.13578 11.6382 8.11014 12 7 12C4.23858 12 2 9.76142 2 7C2 4.23858 4.23858 2 7 2C9.76142 2 12 4.23858 12 7C12 8.11014 11.6382 9.13578 11.0261 9.96544L14.0303 12.9697L14.5607 13.5L13.5 14.5607L12.9697 14.0303L9.96544 11.0261Z" fill="currentColor"></path></svg>
&nbsp;Search
</button> </nav> </div> </div> </header> <main>  <div class="mx-auto max-w-screen-sm px-3"> <div class="animate"> <a href="/cityu/cs4487" class="not-prose group relative flex w-fit flex-nowrap rounded border border-black/15 py-1.5 pl-7 pr-3 transition-colors duration-300 ease-in-out hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:border-white/20 dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="absolute left-2 top-1/2 size-4 -translate-y-1/2 fill-none stroke-current stroke-2"> <line x1="5" y1="12" x2="19" y2="12" class="translate-x-2 scale-x-0 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-hover:scale-x-100 group-focus-visible:translate-x-0 group-focus-visible:scale-x-100"></line> <polyline points="12 5 5 12 12 19" class="translate-x-1 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-focus-visible:translate-x-0"></polyline> </svg> <div class="text-sm"> Back to machine learning </div> </a> </div> <div class="my-10 space-y-1"> <div class="animate flex items-center gap-1.5"> <div class="font-base text-sm"> CS4487 </div>
&bull;
<div class="font-base text-sm"> <time datetime="2024-10-16T00:00:00.000Z"> October 16, 2024 </time> </div> 
&bull;
<div class="font-base text-sm">
Last modified:  <time datetime="2024-10-16T15:02:10.000Z"> October 16, 2024 </time> </div> 
&bull;
<div class="font-base text-sm"> 16 min read </div> </div> <h1 class="animate text-3xl font-semibold text-black dark:text-white"> Part 5 - Robust, Non-Linear Regression and Clustering </h1> </div> <details open class="animate rounded-lg border border-black/15 dark:border-white/20" data-astro-cid-xvrfupwn> <summary data-astro-cid-xvrfupwn>Table of Contents</summary> <nav class="" data-astro-cid-xvrfupwn> <ul class="py-3" data-astro-cid-xvrfupwn> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#outliers" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Outliers </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#ransac" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> RANSAC </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#random-sampling" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Random Sampling </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#how-to-choose-parameters" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> How to Choose Parameters? </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#non-linear-regression" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Non-Linear Regression </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#polynomial-regression" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Polynomial Regression </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#select-degree-using-cross-validation" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Select Degree Using Cross-Validation </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#kernel-trick-for-machine-learning-methods" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Kernel Trick for Machine Learning Methods </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#kernel-ridge-regression" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Kernel Ridge Regression </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#support-vector-regression" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Support Vector Regression </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#svr-primal-form-soft-margin" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> SVR (Primal Form. Soft-Margin) </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#kernel-svr" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Kernel SVR </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#regression-summary" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Regression Summary </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#supervised-learning-vs-unsupervised-learning" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Supervised Learning VS. Unsupervised Learning </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#density-estimation" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Density Estimation </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#clustering" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Clustering </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#dimensionality-reduction" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Dimensionality Reduction </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#manifold-embedding" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Manifold Embedding </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#defining-a-clustering" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Defining a Clustering </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#exhastive-clustering" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Exhastive Clustering </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#number-of-clusterings" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Number of Clusterings </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#k-means-clustering" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> K-Means Clustering </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#k-means-clustering-problem" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> K-Means Clustering Problem </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#chicken-and-egg-problem" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Chicken and Egg Problem </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#k-means-algorithm" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> K-Means Algorithm </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#the-k-means-objective" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> The K-means Objective </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#solution-to-initialization" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Solution to Initialization </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#choosing-k" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Choosing $K$ </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#circular-clusters" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Circular Clusters </a>  </li> </ul> </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#gaussian-mixture-models-gmm" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Gaussian Mixture Models (GMM) </a> <ul class="translate-x-3"> <li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#clustering-with-gmm" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Clustering with GMM </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#expectation-maximization-em-for-gmm" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Expectation-Maximization (EM) for GMM </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#gmm-a-special-case" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> GMM: A Special Case </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#covariance-matrix" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Covariance Matrix </a>  </li><li class="list-inside list-disc px-6 py-1.5 text-sm"> <a href="#trade-offs" target="_self" class="inline-block decoration-black/30 dark:decoration-white/30 hover:decoration-black/50 focus-visible:decoration-black/50 dark:hover:decoration-white/50 dark:focus-visible:decoration-white/50 text-current hover:text-black focus-visible:text-black dark:hover:text-white dark:focus-visible:text-white transition-colors duration-300 ease-in-out underline underline-offset-[3px]"> Trade-Offs </a>  </li> </ul> </li> </ul> </nav> </details> <article class="animate"> <h3 id="outliers">Outliers</h3>
<p>Outliers in the data can affect the squared-error term.</p>
<p>The regression function will try to reduce large prediction errors for outliers, at the expense of worsening predictions for other points.</p>
<h3 id="ransac">RANSAC</h3>
<p><strong>RAN</strong>dom <strong>SA</strong>mple <strong>C</strong>onsensus attempts to robustly fit a regression model in the presence of corruped data (outliers).</p>
<p>This works with any regression model, the idea is to split the data into inliers (good data) and outliers (bad data).
Therefore, we will learn the model only from the inliers.</p>
<h4 id="random-sampling">Random Sampling</h4>
<p>As the name suggests, we repeat this process many times.</p>
<ol>
<li>Randomly sample a subset of points from the data. Typically, just enough to learn the regression model.</li>
<li>Fit a model to the subset.</li>
<li>Determine all data as inlier or outlier by calculating the residuals (prediction errors) and comparing to a threshhold. The set of inliers is called the consensus set.</li>
<li>Save the model with the highest number of inliers.</li>
</ol>
<p>Finally, we use the largest consensus set to learn the final model.</p>
<h4 id="how-to-choose-parameters">How to Choose Parameters?</h4>
<p>With more iterations, we increase the chance to find the correct function.
Ergo, we have higher probability to select a subset of points that contains all the inliers.</p>
<p>Let $T$ be the number of iterations (what we would like to determine).</p>
<p>Let $s$ be the number of points (just enough) to fit the model.</p>
<p>Let $e$ be the proportion of outliers (therefore, $1-e$ is the proportion of inliers).</p>
<p>Therefore,</p>
<p>$p(\text{training subset with all inliers}) = (1-e)^s$.</p>
<p>$p(\text{training subset with at least one outlier}) = 1 - (1-e)^s$.</p>
<p>$p(\text{all } T \text{ training repeats have outliers}) = (1 - (1-e)^s)^T$.</p>
<p>Let $\delta$ be the probability of success (so probability of failure is $1-\delta$).</p>
<p>Finally, we want,</p>
<p>$$
(1 -(1 -e)^s)^T &#x3C; 1 - \delta \newline
T > \frac{\log(1 - \delta)}{\log(1 - (1 - e)^s)}
$$</p>
<p>Threshold typically set as the <em>median</em> absolute deviation of $y$.</p>
<h3 id="non-linear-regression">Non-Linear Regression</h3>
<p>So far we have only considered linear regression in the form $f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b$.</p>
<p>Similar to classification, we can do non-linear regression by performing feature mapping $\phi(\mathbf{x})$, followed by linear regression on the new feature vector.</p>
<h4 id="polynomial-regression">Polynomial Regression</h4>
<p>Let’s consider the $p$-th order polynomial regression.</p>
<p>$$
f(x) = w_0 + w_1 x + w_2 x^2 + \ldots + w_p x^p
$$</p>
<p>We can collect the terms into a vector,</p>
<p>$$
f(x) =
\begin{bmatrix}
w_0 &#x26; w_1 &#x26; w_2 &#x26; \ldots &#x26; w_p
\end{bmatrix}
\begin{bmatrix}
1 \newline
x \newline
x^2 \newline
\vdots \newline
x^p
\end{bmatrix}
= \mathbf{w}^T \phi(x)
$$</p>
<p>Now it is a linear function, so we can use the same linear regression.</p>
<h5 id="select-degree-using-cross-validation">Select Degree Using Cross-Validation</h5>
<p>Minimizing the MSE on the training set will overfit, more complex function always has lower MSE on training set.</p>
<p>Due to this, we need to use cross-validation to select the proper model, the parameter to adjust is in feature transformation step.</p>
<h4 id="kernel-trick-for-machine-learning-methods">Kernel Trick for Machine Learning Methods</h4>
<p>Recall the kernel trick,</p>
<p>$$
\mathcal{K}(\mathbf{x}, \mathbf{z}) = \phi(\mathbf{x})^T \phi(\mathbf{x^{\prime}})
$$</p>
<p>Many learning methods depend on computing inner products between features (e.g., linear regression).</p>
<p>For those methods, we can use a kernel function in place of inner product (i.e., “kernelizing” the methods), and thus introduce non-linearity.</p>
<p>Instead of first transforming the original features into the new feature space and then computing the inner product, we can compute the inner product in the new feature space directly through the kernel function.</p>
<h4 id="kernel-ridge-regression">Kernel Ridge Regression</h4>
<p>Firstly, recall ridge regression,</p>
<p>$$
\underset{\mathbf{w}}{\min} \frac{1}{2} \Vert \mathbf{X} \mathbf{w} - \mathbf{y} \Vert_2^2 + \frac{\alpha}{2} \Vert \mathbf{w} \Vert_2^2
$$</p>
<p>$$
\mathbf{w}^{\star} = (\mathbf{X}^T \mathbf{X} + \alpha \mathbf{I}_{N+1})^{-1} \mathbf{X}^T \mathbf{y} = \mathbf{X}^T (\mathbf{X} \mathbf{X}^T + \alpha \mathbf{I}_M)^{-1} \mathbf{y},
$$</p>
<p>This is achieved by making use of the matrix identity,</p>
<p>$$
(\mathbf{P}^{-1} + \mathbf{B}^T \mathbf{R}^{-1} \mathbf{B})^{-1} \mathbf{B}^T \mathbf{R}^{-1} = \mathbf{P} \mathbf{B}^T (\mathbf{B} \mathbf{P} \mathbf{B}^T + \mathbf{R})^{-1}
$$</p>
<p>In our case, $\mathbf{P} = \frac{1}{\alpha} \mathbf{I}_{N+1}$, $\mathbf{B} = \mathbf{X}$, and $\mathbf{R} = \mathbf{I}_M$.</p>
<p>This equation can be rewritten as,</p>
<p>$$
\mathbf{w}^{\star} = \sum_{i=1}^M \lambda_i \mathbf{x}^{(i)}
$$</p>
<p>with $\lambda = (\mathbf{X} \mathbf{X}^T + \alpha \mathbf{I}_M)^{-1} \mathbf{y}$.</p>
<p>I.e., the solution $\mathbf{w}^{\star}$ must lie in the span of the data cases.</p>
<p>We can arrive at the same solution using Lagrange duality, similar as SVM.</p>
<p>For any new point $\mathbf{x}^{\star}$, we make prediction by,</p>
<p>$$
y^{\star} = (\mathbf{x}^{\star})^T \mathbf{w} = (\mathbf{x}^{\star})^T \mathbf{X}^T (\mathbf{X} \mathbf{X}^T + \alpha \mathbf{I}_M)^{-1} \mathbf{y} = (\mathbf{X} \mathbf{x}^{\star})^T (\mathbf{X} \mathbf{X}^T + \alpha \mathbf{I}_M)^{-1} \mathbf{y}
$$</p>
<p>Kernelizing the method, we have,</p>
<p>$$
y^{\star} = (\mathbf{x}^{\star})^T \mathbf{w} = (\mathbf{k}^{\star})^T (\mathbf{K} + \alpha \mathbf{I}_M)^{-1} \mathbf{y}
$$</p>
<p>$K_{ij} = \mathcal{K}(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}) = \phi(\mathbf{x}^{(i)})^T \phi(\mathbf{x}^{(j)})$, the kernel matrix $(M \times M)$.</p>
<p>$k^{\star}_i = \mathcal{K}(\mathbf{x}^{(i)}, \mathbf{x}^{\star})$, vector containing the kernel values between $\mathbf{x}^{\star}$ and all training points $\mathbf{x}^{(i)}$.</p>
<p>No extra computational burden is incurred, but it now becomes a (much more powerful) non-linear regression model.</p>
<h4 id="support-vector-regression">Support Vector Regression</h4>
<p>Borrow ideas from classification, suppose we form a “tube” around the function.</p>
<p>If a point is inside, then it is “correctly” predicted.
If a point is outside, then it is “incorrectly” predicted.</p>
<p>We can allow some points to be outside the “tube”.
Penalty of point outside tube can be controlled by the hyperparameter C.</p>
<h4 id="svr-primal-form-soft-margin">SVR (Primal Form. Soft-Margin)</h4>
<p>$$
\underset{\mathbf{w}, b}{\min} \frac{1}{2} \Vert \mathbf{w} \Vert_2^2 + C \sum_{i=1}^M \vert y^{(i)} - (\mathbf{w}^T \mathbf{x}^{(i)} + b) \vert_{\epsilon}
$$</p>
<p>Epsilon-insensitive error,
$$
\vert z \vert_{\epsilon} =
\begin{cases}
0, &#x26; |z| \leq \epsilon \newline
|z| - \epsilon, &#x26; |z| > \epsilon
\end{cases}
$$</p>
<p>Equivalently, we have the SVR objective function,</p>
<p>$$
\begin{aligned}
\underset{\mathbf{w}, b, \mathbf{\xi}, \mathbf{\xi}^{\star}}{\min} &#x26; \quad \frac{1}{2} \Vert \mathbf{w} \Vert_2^2 + C \sum_{i=1}^M (\xi_i + \xi_i^{\star}) \newline
\text{subject to} &#x26; \quad y^{(i)} - \mathbf{w}^T \mathbf{x}^{(i)} - b \leq \epsilon + \xi_i, \quad i = 1, \ldots, M \newline
&#x26; \quad \mathbf{w}^T \mathbf{x}^{(i)} + b - y^{(i)} \leq \epsilon + \xi_i^{\star}, \quad i = 1, \ldots, M \newline
&#x26; \quad \xi_i, \xi_i^{\star} \geq 0, \quad i = 1, \ldots, M
\end{aligned}
$$</p>
<h4 id="kernel-svr">Kernel SVR</h4>
<p>SVR can also be kernelized through Lagrange duality. Thus, turning the linear regression to non-linear regression.</p>
<p>SVR (dual form),</p>
<p>$$
\begin{aligned}
\underset{\alpha, \alpha^{\star}}{\max} &#x26; \quad -\frac{1}{2} \sum_{i,j=1}^M (\alpha_i - \alpha_i^{\star})(\alpha_j - \alpha_j^{\star})(\mathbf{x}^{(i)})^T \mathbf{x}^{(j)} \newline
&#x26; \quad - \epsilon \sum_{i=1}^M (\alpha_i + \alpha_i^{\star}) + \sum_{i=1}^M y^{(i)}(\alpha_i - \alpha_i^{\star}) \newline
\text{subject to} &#x26; \quad \sum_{i=1}^M (\alpha_i - \alpha_i^{\star}) = 0 \newline
&#x26; \quad 0 \leq \alpha_i, \alpha_i^{\star} \leq C, \quad i = 1, \ldots, M
\end{aligned}
$$</p>
<p>where $\alpha_i$ and $\alpha_i^{\star}$ are dual variables.</p>
<h3 id="regression-summary">Regression Summary</h3>
<ul>
<li>
<p><strong>Regression Task</strong></p>
<ul>
<li>Observation $\mathbf{x}$, typically a real vector of feature values, $\mathbf{x} \in \mathbb{R}^N$.</li>
<li>$y \in \mathbb{R}$, a real-valued target variable.</li>
<li><strong>Goal</strong>: given an observation $\mathbf{x}$, predict the corresponding target variable $y$.</li>
</ul>
</li>
<li>
<p><strong>Ordinary Least Squares</strong></p>
<ul>
<li><strong>Function</strong>: Linear.</li>
<li><strong>Training</strong>: Minimize Squared Error.</li>
<li><strong>Pros</strong>: Closed-form solution.</li>
<li><strong>Cons</strong>: Sensitive to outliers and overfitting.</li>
</ul>
</li>
<li>
<p><strong>Ridge Regression</strong></p>
<ul>
<li><strong>Function</strong>: Linear.</li>
<li><strong>Training</strong>: Minimize Squared Error with $\Vert \mathbf{w} \Vert_2^2$ regularization term.</li>
<li><strong>Pros</strong>: Closed-form solution, shrinkage to prevent overfitting.</li>
<li><strong>Cons</strong>: Sensitive to outliers.</li>
</ul>
</li>
<li>
<p><strong>LASSO</strong></p>
<ul>
<li><strong>Function</strong>: Linear.</li>
<li><strong>Training</strong>: Minimize Squared Error with $\Vert \mathbf{w} \Vert_1$ regularization term.</li>
<li><strong>Pros</strong>: Feature selection (by forcing weights to zero).</li>
<li><strong>Cons</strong>: Sensitive to outliers.</li>
</ul>
</li>
<li>
<p><strong>RANSAC</strong></p>
<ul>
<li><strong>Function</strong>: Same as the base model.</li>
<li><strong>Training</strong>: Randomly sample subset of training data and fit model, keep model with most inliers.</li>
<li><strong>Pros</strong>: Ignore outliers.</li>
<li><strong>Cons</strong>: Requires enough iterations to find good consensus set.</li>
</ul>
</li>
<li>
<p><strong>Kernel Ridge Regression</strong></p>
<ul>
<li><strong>Function</strong>: Non-linear (kernel function).</li>
<li><strong>Training</strong>: Apply “kernel trick” to ridge regression.</li>
<li><strong>Pros</strong>: Non-linear regression, closed-form solution.</li>
<li><strong>Cons</strong>: Require calculating kernel matrix $(\mathcal{O}(M^2))$. Cross-validation to select hyperparameters.</li>
</ul>
</li>
<li>
<p><strong>Kernel Support Vector Regression</strong></p>
<ul>
<li><strong>Function</strong>: Non-linear (kernel function).</li>
<li><strong>Training</strong>: Minimize epsilon-error.</li>
<li><strong>Pros</strong>: Non-linear regression, faster predictions than kernel ridge regression.</li>
<li><strong>Cons</strong>: Require calculating kernel matrix $(\mathcal{O}(M^2))$. Iterative (slow) and cross-validation to select hyperparameters.</li>
</ul>
</li>
<li>
<p><strong>Feature Normalization</strong></p>
<ul>
<li>Feature normalization is typically required for regression methods with regularization.</li>
<li>Makes ordering of weights more interpretable (LASSO, RR).</li>
</ul>
</li>
<li>
<p><strong>Output transformations</strong></p>
<ul>
<li>Sometimes the output values $y$ have a large dynamic range (e.g., $10^{-1}$ to $10^5$)
<ul>
<li>Large output values will have large error, which will dominate the traning error.</li>
</ul>
</li>
<li>In this case, it is better to transform the output values using the logarithm function
<ul>
<li>$\hat{y} = \log_{10}(y)$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="supervised-learning-vs-unsupervised-learning">Supervised Learning VS. Unsupervised Learning</h3>
<ul>
<li>Supervised learning considers input-output pairs $(\mathbf{x}, y)$.
<ul>
<li>Learn a mapping $f$ from input to output.</li>
<li>Classification: output $y \in \{-1, 1\}$ (binary), or $y \in \{1, 2, \ldots, K\}$ (multi-class).</li>
<li>Regression: output $y \in \mathbb{R}$.</li>
<li>“Supervised” here means that the algorithm is learning the mapping that we want.</li>
</ul>
</li>
<li>Unsupervised learning only considers the input data $\mathbf{x}$
<ul>
<li>There is no output value</li>
<li><strong>Goal</strong>: try to discover inherent properties in the data.
<ul>
<li>Density estimation</li>
<li>Clustering</li>
<li>Dimensionality reduction</li>
<li>Manifold embedding</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="density-estimation">Density Estimation</h4>
<p>From $\mathcal{D} = \{\mathbf{x}^{(i)}\}_{i=1}^M$, estimate a probability distribution $p(\mathbf{x})$.</p>
<p>This is the mother of all unsupervised learning problems.
This is a key technique underpinning AIGC (Artificial Intelligence Generated Content).</p>
<p>This can be conditional, i.e., to estimate $p(\mathbf{x} | y)$.
(Here, we don’t learn a mapping to predict $y$ from $\mathbf{x}$, we just use $y$ as conditioning).</p>
<h4 id="clustering">Clustering</h4>
<p>Find clusters of similar items in the data.</p>
<p>Find a representative item that “summarizes” all items in the cluster.</p>
<p>A typical example, group iris flowers by their measurements (sepal width and petal length).</p>
<h4 id="dimensionality-reduction">Dimensionality Reduction</h4>
<p>Transform high-dimensional vectors into low-dimensional vectors.
Dimensions in the low-dim data may have semantic meaning.</p>
<p>Typical example is, document analysis. In high-dim, we have bag-of-words, vectors of documents.</p>
<p>In low-dim, each dimension represents similarity to a topic.</p>
<h4 id="manifold-embedding">Manifold Embedding</h4>
<p>Project high-dimensional vectors into 2- or 3-dimensional space for visualization.
Points in the low-dim space have similar pair-wise distances as in the high-dim space.</p>
<p>Typical example is, visualize a collection of handwritten digits (images).</p>
<h3 id="defining-a-clustering">Defining a Clustering</h3>
<p>Suppose we have $M$ data points $\mathcal{D} = \{\mathbf{x}^{(i)}\}_{i=1}^M$, where $\mathbf{x}^{(i)} \in \mathbb{R}^N$.</p>
<p>A clustering of the $M$ points into $K$ clusters is a partitioning of $\mathcal{D}$ into $K$ mutually disjoint groups $\mathcal{C} = \{\mathcal{C}_1, \mathcal{C}_2, \ldots, \mathcal{C}_K\}$ such that $\mathcal{C}_1 \cup \mathcal{C}_2 \cup \ldots \cup \mathcal{C}_K = \mathcal{D}$.</p>
<p>Groups in these cases are called clusters. Therefore, $K$ is the number of clusters.</p>
<p>Each data point is assigned with a cluster index $(y \in \{1, \ldots, K\})$.</p>
<h3 id="exhastive-clustering">Exhastive Clustering</h3>
<p>Suppose we have a function $f(\mathcal{C})$ that takes a clustering of $\mathcal{C}$ of the data set $\mathcal{D}$ as input, and returns a score with lower scores indicating better clustering.</p>
<p>The optimal clustering according to $f$ is simply,</p>
<p>$$
\underset{\mathcal{C}}{\arg \min} f(\mathcal{C})
$$</p>
<p>But what’s the complexity of exhaustive clustering?</p>
<h4 id="number-of-clusterings">Number of Clusterings</h4>
<p>The total number of clusterings of a data set with $M$ elements is the Bell number $B_M$. Where $B_0 = 1$ and,</p>
<p>$$
B_{M + 1} = \sum_{k=0}^M \binom{M}{k} B_k
$$</p>
<p>The first few Bell numbers are: 1, 1, 2, 5, 15, 52, 203, 877, 4140, 21147, 115975, $\ldots$.</p>
<p>The complexity of exhaustive clustering scales with $B_M$ and is thus computationally totally intractable for general scoring functions.</p>
<p>We will need either approximation algorithms or scoring functions with special properties.</p>
<h3 id="k-means-clustering">K-Means Clustering</h3>
<p>Assume we have $K$ clusters, each cluster is represented by a <strong>cluster center</strong>, $\mathbf{c}_j \in \mathbb{R}^N, j \in \{1, \ldots, K\}$.</p>
<p>Assign each data point to the closest cluster center, according to Euclidean distance $\Vert \mathbf{x} - \mathbf{c}_j \Vert_2$.</p>
<h4 id="k-means-clustering-problem">K-Means Clustering Problem</h4>
<p>But, how do we pick the cluster centers?</p>
<p>Again, assume there are $K$ clusters, we pick the cluster centers that minimize the squared distance to all its cluster member,</p>
<p>$$
\underset{\mathbf{c_1}, \ldots, \mathbf{c_K}}{\min} \sum_{i=1}^M \Vert \mathbf{x}^{(i)} - \mathbf{c}_{z^{(i)}} \Vert_2^2
$$</p>
<p>where $z^{(i)}$ is the index of the closest cluster center to $\mathbf{x}^{(i)}$,</p>
<p>$$
z^{(i)} = \underset{j=\{1, \ldots, K\}}{\arg \min} \Vert \mathbf{x}^{(i)} - \mathbf{c}_j \Vert_2^2
$$</p>
<p>Thus, this is our solution, if the assignments $\{z^{(i)}\}_{i=1}^M$ are known…</p>
<p>Let $C_j$ be the set of points assigned to cluster $j$,</p>
<p>$$
C_j = \{\mathbf{x}^{(i)} | z^{(i)} = j\}
$$</p>
<p>Cluster center is the mean of the points in that cluster,</p>
<p>$$
\mathbf{c_j} = \frac{1}{\vert C_j \vert} \sum_{\mathbf{x}^{(i)} \in C_j} \mathbf{x}^{(i)}
$$</p>
<h4 id="chicken-and-egg-problem">Chicken and Egg Problem</h4>
<p>Cluster assignment of each point depends on the cluster centers.</p>
<p>Location of cluster center depends on which points are assigned to it.</p>
<p>How do we solve this?</p>
<h4 id="k-means-algorithm">K-Means Algorithm</h4>
<p>Pick an initial cluster center (randomly or using some heuristic).</p>
<p>Then we repeat,</p>
<ol>
<li><strong>Assignment step</strong>: calculate assignment $z^{(i)}$ for each point $\mathbf{x}^{(i)}$, closest cluster center using Euclidean distance,</li>
</ol>
<p>$$
z^{(i)} = \underset{j=\{1, \ldots, K\}}{\arg \min} \Vert \mathbf{x}^{(i)} - \mathbf{c}_j \Vert_2
$$</p>
<ol start="2">
<li><strong>Update step</strong>: Calculate cluster center as average of points assigned to cluster $j$,</li>
</ol>
<p>$$
\mathbf{c_j} = \frac{\sum_{i=1}^M \mathbb{I}[z^{(i)} = j] \mathbf{x}^{(i)}}{\sum_{i=1}^M \mathbb{I}[z^{(i)} = j]}
$$</p>
<p>This procedure will eventually converge.</p>
<h4 id="the-k-means-objective">The K-means Objective</h4>
<p>$K$-means attempts to minimize the sum of within-cluster variation over all clusters (also called the within-cluster sum of squares),</p>
<p>$$
\min \ell(\mathbf{z}, \{\mathbf{c_j} \}_{j=1}^K)
$$</p>
<p>$$
\underset{\mathbf{z}, \{\mathbf{c_1}, \ldots, \mathbf{c_K\}}}{\min} \sum_{i=1}^M \Vert \mathbf{x}^{(i)} - \mathbf{c}_{z^{(i)}} \Vert_2^2
$$</p>
<p>where $\mathbf{z} = [z^{(1)}, z^{(2)}, \ldots, z^{(M)}]^T$.</p>
<p>$K$-means is exactly <strong>coordinate descent</strong> on $\ell$,
where assignment step minimizes $\ell$ with respect to $\mathbf{z}$ while holding $\{\mathbf{c_j}\}$ fixed,
and the update step minimized $\ell$ with respect to $\{\mathbf{c_j}\}$ while holding $\mathbf{z}$ fixed.</p>
<p>Thus, $\ell$ is monotonically decreasing. As $\ell$ is also lower bounded by 0, the value of $\ell$ must converge.</p>
<p>Note that $K$-means has many local optimas in general, each corresponding to a different clustering of the data. Finding the global optimum is not computionally tractable.</p>
<p>Thus, the final results can be highly sensitive to initialization, some bad initial cluster centers will yield suboptimal clustering results.</p>
<h4 id="solution-to-initialization">Solution to Initialization</h4>
<p>It is common to perform multiple random re-starts of the algorithm, and take the clustering with the best result.</p>
<p>Common initialization strategies include,</p>
<ul>
<li>Setting the initial centers to be randomly selected data points.</li>
<li>Setting the initial partition to a random partition</li>
<li>Selecting centers using a “furthest-first”-style heuristic (more formally known as $K$-means++).</li>
</ul>
<p>It often helps to initially to run with $K \log(K)$ clusters, then merge clusters to get down to $K$ and run the algorithm from that initialization.</p>
<h4 id="choosing-k">Choosing $K$</h4>
<p>Clustering results depend on the number of clusters $K$ used.
We do not typically know this information beforehand.</p>
<p>One of the most reliable and widely used methods is the <strong>elbow method</strong>.
The elbow method involves running $K$-means for a range of $K$ values and plotting the within-cluster sum of squares as a function of $K$.
Then choose the “elbow” point, where the rate of decrease slows down.</p>
<h4 id="circular-clusters">Circular Clusters</h4>
<p>One problem with $K$-means is that it assumes that each cluster has a circular shape.</p>
<p>Since it is based on Euclidean distance to each center, this means that $K$-means can not handle skewed (elliptical) clusters.</p>
<h3 id="gaussian-mixture-models-gmm">Gaussian Mixture Models (GMM)</h3>
<p>A multivariate Gaussian can model a cluster with an elliptical shape.
The ellipse shape is controlled by the covariance matrix of the Gaussian and the location of the cluster is controlled by the mean.</p>
<p>Gaussian mixture model is a weighted sum of Gaussians,</p>
<p>$$
p(\mathbf{x}) = \sum_{j=1}^K \pi_j \mathcal{N}(\mathbf{x}; \mathbf{\mu}_j, \mathbf{\Sigma}_j)
$$</p>
<p>Each Gaussian represents one elliptical cluster:</p>
<ul>
<li>$\mathbf{\mu}_j$ is mean of $j$-th cluster (the location).</li>
<li>$\mathbf{\Sigma}_j$ is the covariance matrix of $j$-th cluster (the ellipse shape).</li>
<li>$\pi_j$ is the prior weight of $j$-th cluster (how likely the cluster is).</li>
</ul>
<h4 id="clustering-with-gmm">Clustering with GMM</h4>
<p>Given a data set $\mathcal{D} = \{\mathbf{x}^{(i)}\}_{i=1}^M$, learn a GMM using maximum likelihood estimation,</p>
<p>$$
\underset{\pi, \mathbf{\mu}, \mathbf{\Sigma}}{\max} \sum_{i=1}^M \log \sum_{j=1}^K \pi_j \mathcal{N}(\mathbf{x}^{(i)}; \mathbf{\mu_j}, \mathbf{\Sigma_j})
$$</p>
<p>While we can do this directly using gradient-based optimization, it is often faster to use a special algorithm called <strong>Expectation-Maximization (EM)</strong>.</p>
<h4 id="expectation-maximization-em-for-gmm">Expectation-Maximization (EM) for GMM</h4>
<p>EM results in an algortihm similar to $K$-means.</p>
<ol>
<li><strong>E-step</strong>: Calculate cluster membership with “soft” assignment, a data point can have a fractional contribution to different clusters.
Contribution of point $i$ to cluster $j$ is defined by the posterior probability that $\mathbf{x}^{(i)}$ belongs to cluster $j$ using Bayes’ rule,</li>
</ol>
<p>$$
\begin{aligned}
z_j^{(i)} = p(z^{(i)} = j | \mathbf{x}^{(i)}) &#x26; = \frac{p(\mathbf{x}^{(i)}, z^{(i)} = j)}{p(\mathbf{x}^{(i)})} \newline
&#x26; \quad = \frac{p(\mathbf{x}^{(i)} | z^{(i)} = j) p(z^{(i)} = j)}{\sum_{k=1}^K p(\mathbf{x}^{(i)} | z^{(i)} = k) p(z^{(i)} = k)} \newline
&#x26; \quad = \frac{\pi_j \mathcal{N}(\mathbf{x}^{(i)}; \mathbf{\mu_j}, \mathbf{\Sigma_j})}{\sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x}^{(i)}; \mathbf{\mu_k}, \mathbf{\Sigma_k})}
\end{aligned}
$$</p>
<ol start="2">
<li><strong>M-step</strong>: Update each Gaussian cluster (mean, covariance, and weight) using “soft” weighting.
“Soft” count of points in cluster $j$,</li>
</ol>
<p>$$
M_j = \sum_{i=1}^M z_j^{(i)}
$$</p>
<p>Weight,</p>
<p>$$
\pi_j = \frac{M_j}{M}
$$</p>
<p>Mean,</p>
<p>$$
\mathbf{\mu_j} = \frac{1}{M_j} \sum_{i=1}^M z_j^{(i)} \mathbf{x}^{(i)}
$$</p>
<p>Covariance,</p>
<p>$$
\mathbf{\Sigma_j} = \frac{1}{M_j} \sum_{i=1}^M z_j^{(i)} (\mathbf{x}^{(i)} - \mathbf{\mu_j})(\mathbf{x}^{(i)} - \mathbf{\mu_j})^T
$$</p>
<h4 id="gmm-a-special-case">GMM: A Special Case</h4>
<p>Suppose we fix $\pi_j = \frac{1}{K}$ and $\mathbf{\Sigma}_j = \mathbf{I}$. In this case we have,</p>
<p>$$
p(\mathbf{x}^{(i)} | z^{(i)} = j) = \mathcal{N}(\mathbf{x}^{(i)}; \mathbf{\mu}_j, \mathbf{I}) = \frac{1}{(2\pi)^{N/2}} \exp \left( -\frac{1}{2} \Vert \mathbf{x}^{(i)} - \mathbf{\mu}_j \Vert_2^2 \right)
$$</p>
<p>We obtain a special case of the EM algorithm for GMM,</p>
<p>$$
z_j^{(i)} = \frac{\exp \left( -\frac{1}{2} \Vert \mathbf{x}^{(i)} - \mathbf{\mu_j} \Vert_2^2 \right)}{\sum_{k=1}^K \exp \left( -\frac{1}{2} \Vert \mathbf{x}^{(i)} - \mathbf{\mu_k} \Vert_2^2 \right)}
$$</p>
<p>$$
\mathbf{\mu_j} = \frac{1}{M_j} \sum_{i=1}^M z_j^{(i)} \mathbf{x}^{(i)}
$$</p>
<p>This is often referred to as <strong>soft</strong> $K$-means.</p>
<h4 id="covariance-matrix">Covariance Matrix</h4>
<p>The covariance matrix is an $N \times N$ matrix,</p>
<p>$$
\begin{bmatrix}
a_{11} &#x26; a_{12} &#x26; a_{13} \newline
a_{21} &#x26; a_{22} &#x26; a_{23} \newline
a_{31} &#x26; a_{32} &#x26; a_{33}
\end{bmatrix}
$$</p>
<p>For high-dimensional data, it can be very large, thus requires a lot of data to learn effectively.</p>
<p>One solution is using a <strong>diagonal</strong> covariance matrix ($N$ parameters) or <strong>spherical</strong> covariance matrix (1 parameter),</p>
<p>$$
\begin{bmatrix}
a_{11} &#x26; 0 &#x26; 0 \newline
0 &#x26; a_{22} &#x26; 0 \newline
0 &#x26; 0 &#x26; a_{33}
\end{bmatrix}
\begin{bmatrix}
a &#x26; 0 &#x26; 0 \newline
0 &#x26; a &#x26; 0 \newline
0 &#x26; 0 &#x26; a
\end{bmatrix}
$$</p>
<h4 id="trade-offs">Trade-Offs</h4>
<p>The original $K$-means algorithm performs hard assignments during clustering, and implicitly assumes all clusters will have an equal number of points assigned as well as a unit covariance matrix.</p>
<p>GMM for clustering relaxes all of these assumptions. The objective still has multiple local optima.</p>
<p>EM can also be used with any component densities/distributions to customize the model to a given data set.</p>
<p>As with $K$-means, intialization is important for GMM.</p> <div class="mt-24"> <div class="grid grid-cols-2 gap-1.5 sm:gap-3"> <a href="/cityu/cs4487/cs4487_5" class="group relative flex flex-nowrap rounded-lg border border-black/15 px-4 py-3 pl-10 no-underline transition-colors duration-300 ease-in-out hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:border-white/20 dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="absolute left-2 top-1/2 size-5 -translate-y-1/2 fill-none stroke-current stroke-2"> <line x1="5" y1="12" x2="19" y2="12" class="translate-x-3 scale-x-0 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-hover:scale-x-100 group-focus-visible:translate-x-0 group-focus-visible:scale-x-100"></line> <polyline points="12 5 5 12 12 19" class="translate-x-1 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-focus-visible:translate-x-0"></polyline> </svg> <div class="flex items-center text-sm"> Part 5 - Regression </div> </a> <div class="invisible"></div> </div> </div> <div class="mt-24"> <div class="giscus"></div> <script data-astro-rerun src="https://giscus.app/client.js" data-repo="rezaarezvan/rezvan.xyz" data-repo-id="R_kgDOHvQr3w" data-category="General" data-category-id="DIC_kwDOHvQr384CiWVC" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="en" data-loading="lazy" crossorigin="anonymous" async></script> </div> </article> </div>  </main> <footer class="animate"> <div class="mx-auto max-w-screen-sm px-3"> <div class="relative"> <div class="absolute -top-12 right-0"> <button id="back-to-top" class="group relative flex w-fit flex-nowrap rounded border border-black/15 py-1.5 pl-8 pr-3 transition-colors duration-300 ease-in-out hover:bg-black/5 hover:text-black focus-visible:bg-black/5 focus-visible:text-black dark:border-white/20 dark:hover:bg-white/5 dark:hover:text-white dark:focus-visible:bg-white/5 dark:focus-visible:text-white"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="absolute left-2 top-1/2 size-4 -translate-y-1/2 rotate-90 fill-none stroke-current stroke-2"> <line x1="5" y1="12" x2="19" y2="12" class="translate-x-2 scale-x-0 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-hover:scale-x-100 group-focus-visible:translate-x-0 group-focus-visible:scale-x-100"></line> <polyline points="12 5 5 12 12 19" class="translate-x-1 transition-transform duration-300 ease-in-out group-hover:translate-x-0 group-focus-visible:translate-x-0"></polyline> </svg> <div class="text-sm">Back to top</div> </button> </div> </div> <div class="flex items-center justify-between"> <div>&copy; 2024 • rezvan.xyz </div> <div class="flex flex-wrap items-center gap-1.5"> <button id="light-theme-button" aria-label="Light theme" class="group flex size-9 items-center justify-center rounded border border-black/15 hover:bg-black/5 focus-visible:bg-black/5 dark:border-white/20 dark:hover:bg-white/5 dark:focus-visible:bg-white/5"> <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="transition-colors duration-300 ease-in-out group-hover:animate-pulse group-hover:stroke-black group-focus-visible:animate-pulse group-focus-visible:stroke-black group-hover:dark:stroke-white dark:group-focus-visible:stroke-white"> <circle cx="12" cy="12" r="5"></circle> <line x1="12" y1="1" x2="12" y2="3"></line> <line x1="12" y1="21" x2="12" y2="23"></line> <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line> <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line> <line x1="1" y1="12" x2="3" y2="12"></line> <line x1="21" y1="12" x2="23" y2="12"></line> <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line> <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line> </svg> </button> <button id="dark-theme-button" aria-label="Dark theme" class="group flex size-9 items-center justify-center rounded border border-black/15 hover:bg-black/5 focus-visible:bg-black/5 dark:border-white/20 dark:hover:bg-white/5 dark:focus-visible:bg-white/5"> <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="transition-colors duration-300 ease-in-out group-hover:animate-pulse group-hover:stroke-black group-focus-visible:animate-pulse group-focus-visible:stroke-black group-hover:dark:stroke-white dark:group-focus-visible:stroke-white"> <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path> </svg> </button> <button id="system-theme-button" aria-label="System theme" class="group flex size-9 items-center justify-center rounded border border-black/15 hover:bg-black/5 focus-visible:bg-black/5 dark:border-white/20 dark:hover:bg-white/5 dark:focus-visible:bg-white/5"> <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="transition-colors duration-300 ease-in-out group-hover:animate-pulse group-hover:stroke-black group-focus-visible:animate-pulse group-focus-visible:stroke-black group-hover:dark:stroke-white dark:group-focus-visible:stroke-white"> <rect x="2" y="3" width="20" height="14" rx="2" ry="2"></rect> <line x1="8" y1="21" x2="16" y2="21"></line> <line x1="12" y1="17" x2="12" y2="21"></line> </svg> </button> </div> </div> </div> </footer> <aside data-pagefind-ignore> <div id="backdrop" class="bg-[rgba(0, 0, 0, 0.5] invisible fixed left-0 top-0 z-50 flex h-screen w-full justify-center p-6 backdrop-blur-sm" data-astro-transition-persist="astro-3snakcvo-2"> <div id="pagefind-container" class="m-0 flex h-fit max-h-[80%] w-full max-w-screen-sm flex-col overflow-auto rounded border border-black/15 bg-neutral-100 p-2 px-4 py-3 shadow-lg dark:border-white/20 dark:bg-neutral-900"> <div id="search" class="pagefind-ui pagefind-init" data-pagefind-ui data-bundle-path="/pagefind/" data-ui-options="{&#34;showImages&#34;:false,&#34;excerptLength&#34;:15,&#34;resetStyles&#34;:false}"></div>  <div class="mr-2 pb-1 pt-4 text-right text-xs dark:prose-invert">
Press <span class="prose text-xs dark:prose-invert"><kbd class="">Esc</kbd></span> or click anywhere to close
</div> </div> </div> </aside> <script>
  const magnifyingGlass = document.getElementById("magnifying-glass");
  const backdrop = document.getElementById("backdrop");

  function openPagefind() {
    const searchDiv = document.getElementById("search");
    const search = searchDiv.querySelector("input");
    setTimeout(() => {
      search.focus();
    }, 0);
    backdrop?.classList.remove("invisible");
    backdrop?.classList.add("visible");
  }

  function closePagefind() {
    const search = document.getElementById("search");
    search.value = "";
    backdrop?.classList.remove("visible");
    backdrop?.classList.add("invisible");
  }

  // open pagefind
  magnifyingGlass?.addEventListener("click", () => {
    openPagefind();
  });

  document.addEventListener("keydown", (e) => {
    if (e.key === "/") {
      e.preventDefault();
      openPagefind();
    } else if ((e.metaKey || e.ctrlKey) && e.key === "k") {
      e.preventDefault();
      openPagefind();
    }
  });

  // close pagefind
  document.addEventListener("keydown", (e) => {
    if (e.key === "Escape" || e.keyCode === 27) {
      closePagefind();
    }
  });

  // close pagefind when searched result(link) clicked
  document.addEventListener("click", (event) => {
    if (event.target.classList.contains("pagefind-ui__result-link")) {
      closePagefind();
    }
  });

  backdrop?.addEventListener("click", (event) => {
    if (!event.target.closest("#pagefind-container")) {
      closePagefind();
    }
  });

  // prevent form submission
  const form = document.getElementById("form");
  form?.addEventListener("submit", (event) => {
    event.preventDefault();
  });
</script>  </body></html>