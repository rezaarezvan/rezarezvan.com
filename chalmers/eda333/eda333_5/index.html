<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Astro v4.11.5"><link rel="icon" type="image" href="/favicon.ico"><title>Part 5 - Cache memory</title><!-- KaTeX support --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script><!-- inline KaTeX --><link rel="stylesheet" href="/_astro/index.CwgzIfsj.css">
<link rel="stylesheet" href="/_astro/_slug_.hCvEQTvV.css">
<style>article[data-astro-cid-v5ro3oot]{max-width:80ch;margin:0 auto}.nav-button[data-astro-cid-v5ro3oot]{display:flex;align-items:center;padding:.5rem;border-radius:.5rem;transition:background-color .3s ease;text-decoration:none;color:var(--text-color);background-color:var(--bg-color);border:1px solid var(--border-color)}.nav-button[data-astro-cid-v5ro3oot]:hover{background-color:var(--hover-color)}.nav-button[data-astro-cid-v5ro3oot] .arrow[data-astro-cid-v5ro3oot]{font-size:1.5rem;line-height:1}.nav-button[data-astro-cid-v5ro3oot] .text[data-astro-cid-v5ro3oot]{display:flex;flex-direction:column;margin:0 .5rem}.nav-button[data-astro-cid-v5ro3oot] .label[data-astro-cid-v5ro3oot]{font-size:.8rem;text-transform:uppercase;letter-spacing:.05em;color:var(--muted-color)}.nav-button[data-astro-cid-v5ro3oot] .title[data-astro-cid-v5ro3oot]{font-weight:500}.prev-button[data-astro-cid-v5ro3oot]{justify-content:flex-start}.next-button[data-astro-cid-v5ro3oot]{justify-content:flex-end;text-align:right}@media (max-width: 640px){.nav-button[data-astro-cid-v5ro3oot]{width:100%}.next-button[data-astro-cid-v5ro3oot]{justify-content:flex-start;text-align:left}.next-button[data-astro-cid-v5ro3oot] .text[data-astro-cid-v5ro3oot]{order:2;margin-left:.5rem}.next-button[data-astro-cid-v5ro3oot] .arrow[data-astro-cid-v5ro3oot]{order:1}}
</style><script type="module">document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})});
</script></head> <body> <div class="container mx-auto px-4 flex flex-col md:flex-row min-h-screen"> <aside class="w-full md:w-64 border-b md:border-r md:border-b-0 border-[var(--border-color)] border-dashed pt-8"> <header class="flex flex-col h-full"> <div class="flex items-center mb-4"> <script>
  function setTheme(mode) {
    localStorage.setItem("theme-storage", mode);
    document.documentElement.setAttribute('data-theme', mode);
  }
  function toggleTheme() {
    const currentTheme = localStorage.getItem("theme-storage") || "light";
    const newTheme = currentTheme === "light" ? "dark" : "light";
    setTheme(newTheme);
  }
  const savedTheme = localStorage.getItem("theme-storage") || "light";
  setTheme(savedTheme);
  window.toggleTheme = toggleTheme;
</script> <button id="theme-toggle" onclick="toggleTheme()" aria-label="Toggle theme" class="w-6 h-6 cursor-pointer"> <div class="w-5 h-5 border-2 border-primary rounded-full transition-colors duration-300 ease-in-out hover:bg-primary"></div> </button> <a href="/" class="text-2xl font-semibold ml-3 h-10 pr-3">rezvan.xyz</a> </div> <nav class="flex flex-wrap gap-2 md:flex-col md:gap-2"> <a href="/principles" class="transition-colors">
[principles]
</a><a href="/cv" class="transition-colors">
[cv]
</a><a href="/posts" class="transition-colors">
[posts]
</a><a href="/chalmers" class="transition-colors">
[chalmers]
</a><a href="/cityu" class="transition-colors">
[cityu]
</a> </nav> </header> </aside> <main class="flex-grow px-4 md:px-8 py-8 overflow-y-auto">  <article class="prose prose-sm sm:prose lg:prose-lg xl:prose-xl max-w-none" data-astro-cid-v5ro3oot> <h1 class="text-3xl sm:text-4xl font-bold mb-4" data-astro-cid-v5ro3oot>Part 5 - Cache memory</h1> <p class="text-sm text-muted-foreground mb-4" data-astro-cid-v5ro3oot>
Date: 4/14/2023 </p> <div class="markdown-content" data-astro-cid-v5ro3oot>  <p>In this part we will cover one of the most important solutions in computer science, the cache memory, but also memory in general.</p>
<h3 id="memory-types">Memory types</h3>
<p>Before we dive in - let’s quickly recap the different types of memory there are:</p>
<ul>
<li>Static RAM (SRAM)
<ul>
<li>0.5 ns - 2.5 ns | $2000 - $5000 per GB</li>
</ul>
</li>
<li>Dynamic RAM (DRAM)
<ul>
<li>50 ns - 70 ns | $20 - $75 per GB</li>
</ul>
</li>
<li>Magnetic storage
<ul>
<li>5 ms - 20 ms | $0.20 - $2 per GB</li>
</ul>
</li>
</ul>
<p>So, therefore, an ideal memory would be one of the speed of an SRAM but at the cost of a hard drive.</p>
<h3 id="locality-of-reference">Locality of reference</h3>
<p>The locality of reference is the tendency that programs, when accessing memory, usually access memory addresses that is the same or very close to each other.</p>
<p>This is a very crucial tendency - if we only keep a subset of the data, memory and instructions, that are probable to appear, in a small memory - we can use that instead.</p>
<p>This is exactly what we can define as a <em>cache memory</em>. Just to clearly define the two different localities:</p>
<ul>
<li>Spatial Locality
<ul>
<li>A memory address that is <em>adjacent</em></li>
</ul>
</li>
<li>Temporal Locality
<ul>
<li>The <em>exact same</em> address.</li>
</ul>
</li>
</ul>
<h3 id="cache-memory">Cache memory</h3>
<p>So, as we defined, the cache memory, is a small but fast memory that stores data you want to access often (and adjacent)</p>
<p>Let’s define some terminology:
$$
h_x = \textbf{Hit rate} = \text{Percentage of hits in memory x} \newline
(1 - h_x) = m_x = \textbf{Miss rate} = \text{Percentage of misses in memory x}
$$</p>
<p>$$
T_x = \textbf{Hit time} = \text{Acces time for memory x} \newline
(T_2 - T_1) = \textbf{Miss penalty} = \text{Miss penalty for a miss in memory 1}
$$</p>
<p>With these defined we can finally define what we will use the most, <em>Average Memory Access Time</em> (AMAT):
$$
\text{AMAT} = h_1\ T_1 + m_1\ T_2 = h_1\ T_1 + (1 - h_1)T_2
$$</p>
<h3 id="memory-hierarchy">Memory hierarchy</h3>
<p>When dealing with memory, we’ll hear the word hierarchy - what this means is the different “levels”.</p>
<p>Our cache memory is the closest and fastest, after that we have our primary memory - and, in some cases we’ll even have a “secondary” memory (hard drive).</p>
<p>You may also see the word level been thrown around. Our cache memory is the L1 memory. The primary memory can be divided into bigger L2-Lx caches.</p>
<p>We’ll cover this in more detail later. But why this is the case - is just as before, due to the locality of reference.</p>
<p>If we keep our memory in “blocks” we can easily keep track of where “groups” of memory are located.</p>
<h3 id="cache-implementation">Cache implementation</h3>
<p>So, in reality, we keep track of memory blocks in our caches.</p>
<p>Let’s say we have 128 bytes of memory, we divide our memory into block of 16 bytes, meaning 8 total blocks.</p>
<p>In our cache memory we only have space for <strong>four</strong> blocks. How can we store all 8?</p>
<p>Well, the most obvious answer is that we keep block 0 - 3 at block “place” 0 - 3. But what about 4 - 7?</p>
<p>We use modulo for that!</p>
<p>But then we need a way to differentiate block 0 with block 4 and so on?</p>
<p>We’ll keep a “tag block” to indicate what block that is currently stored!</p>
<p>A rule of thumb is that:
$$
\frac{\text{Memory size}}{\text{Cache size}} = N \newline
$$</p>
<p>$$
\text{Tag bits} = log_2(N)
$$</p>
<p>So, therefore we have to always keep track of these four questions in mind:</p>
<ul>
<li><strong>Where</strong> shall the block be place?</li>
<li><strong>How do we find</strong> a specific block</li>
<li><strong>Which block</strong> shall be written when the place is needed</li>
<li>How do we handle writes (“<strong>store</strong>”)?</li>
</ul>
<p>So to solve these problems we have to:</p>
<ul>
<li>Have some <em>index</em> bits that indicates what <em>index</em> we have to look at, along with the tag and offset bits.</li>
<li>If we miss:
<ul>
<li>A block that already is stored at that position gets <strong>thrown out</strong>. If it is different from when it was read (dirty) - we need to write it back to the next level.</li>
<li>The searched block gets read into the cache and gets stored at the address <strong>index</strong>, and the required word gets delivered to the CPU.</li>
<li>This, not surprisingly, takes a lot of time.</li>
</ul>
</li>
</ul>
<h3 id="mapping-types">Mapping types</h3>
<p>We’ve lightly covered one type of mapping for caches - the direct mapping.</p>
<p>Index in cache = (Block address) mod (Cache size)</p>
<p>But we also have <em>associative caches</em>.</p>
<ul>
<li>Each index points at a <em>set</em> of blocks spaces (setindex).</li>
<li>A block can be placed anywhere within a set.</li>
<li>More blocks with the same index can now be in the cache simultaneously.</li>
<li>Set size = <strong>associativity</strong></li>
</ul>
<p>Note: This is more costly than direct mapping!</p>
<p>So, a two-way set associative cache is equivalent to two parallel direct mapped caches.</p>
<h3 id="handle-writes-in-caches">Handle writes in caches</h3>
<p>We have two main strageties for this:</p>
<ul>
<li><strong>Write back</strong> (aka copy back)
<ul>
<li>Write only in cache</li>
<li>When a block needs to be thrown out, we write it (copy) to primary memory *only if it has been updated (this requires one extra “dirty” bit).</li>
</ul>
</li>
<li><strong>Write through</strong>
<ul>
<li>The block writes to PM as well (normally via a so-called “Store buffer”).</li>
</ul>
</li>
</ul>
<p>But what do we do when we have a so-called “store miss”?</p>
<ul>
<li>For a write-back cache:
<ul>
<li>Read in the block to the cache.</li>
</ul>
</li>
<li>For write-through cache:
<ul>
<li>Two strategies:
<ul>
<li>“Allocate on miss”: Read in the block to the cache.</li>
<li>“Write around”: Do not read in the block (Since, programs often write whole blocks before they get read, for example when initializing a program).</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="cache-misses-affect-cpi">Cache misses affect CPI</h3>
<p>Let’s look at an example:</p>
<p>Given:</p>
<ul>
<li>Instruction cache miss rate = 2%</li>
<li>Data cache miss rate = 4%</li>
<li>Miss penalty = 100 clock cycles</li>
<li>Base CPI = 2</li>
<li>Loads &#x26; stores take 36% of total instructions</li>
</ul>
<p>We can calculate:</p>
<ul>
<li>Miss cycles per instruction ($\Delta CPI_{cache}$)
<ul>
<li>I-cache misses: $\Delta CPI_{instruction} = 0.02 * 100 = 2$</li>
<li>D-cache misses: $\Delta CPI_{data} = 0.36 * 0.04 * 100 = 1.44$</li>
</ul>
</li>
</ul>
<p>Therefore, our total CPI is = 2 + 2 + 1.44 = 5.44!</p>
<h3 id="multiple-level-cache">Multiple-level cache</h3>
<ul>
<li>Primary cache (L1)
<ul>
<li>Focuses on <strong>minimal hit time</strong>.</li>
</ul>
</li>
<li>L2 Cache
<ul>
<li>Focuses on <strong>low miss rate</strong> to avoid accessing PM.</li>
</ul>
</li>
<li>Therefore:
<ul>
<li>L1 caches are often very small when dealing with multiple levels of caches, compared to having single caches.</li>
<li>L1’s block size is often smaller than L2’s.</li>
</ul>
</li>
</ul>
<h3 id="floating-point">Floating-point</h3>
<p>$$
x = (-1)^S \cdot\ (1 + Fraction) \cdot\ 2^{Exponent - Bias}
$$</p>  </div> <nav class="flex flex-col sm:flex-row justify-between mt-8 pt-4 border-t border-border" data-astro-cid-v5ro3oot>   </nav> </article>  </main> </div> </body></html> 